\documentclass[a4paper, 11pt, nofootinbib]{article}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\usepackage{amsmath}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage[utf8]{inputenc}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{makecell}
\usepackage{tabularx}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{lscape}
\usepackage{afterpage}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{chngcntr}

\counterwithin{figure}{section}

\geometry{a4paper, margin=1in}

\renewcommand{\figurename}{Abb.}
\newcommand{\code}[1]{\texttt{#1}}
\renewcommand{\contentsname}{Inhalt}
\renewcommand{\listfigurename}{Abbildungsverzeichnis}


\definecolor{system}{RGB}{141,0,76}
\definecolor{inhalt}{RGB}{2,47,99}
\definecolor{darstellung}{RGB}{116,117,117}
\definecolor{nutzung}{RGB}{207,2,127}

\begin{document}
\title{Zusammenfassung Data Warehousing FS2018}
\author{Alex Neher}
\maketitle

\tableofcontents
\newpage
\listoffigures
\newpage

\graphicspath{{./Pictures/}}

\section{Die Notwendigkeit von Data Warehouses}
\subsection{Entscheidungsunterstützung (Skript S15)}
Data Warehouses sind keine neue Erfindung. Bereits in den 1960er Jahren wurden sogenannte \textbf{Managementsinformationssysteme} entwickelt. Diese MIS dienten dazu, Entscheidungsträgern alle benötigten Informationen zeitnah, fehlerfrei, flexibel, ergonomisch, effizient, effektiv und inspirativ zur Verfügung zu stellen. Diese Systeme treffen also nicht selbst Entscheidungen, sie \textbf{unterstützen} die Entscheidungsträger lediglich bei ihrer Entscheidung.

Es gibt vier Arten der Entscheidungsunterstützung:

\begin{description}
	\item [Modellbasiert:] z.B. Lineare Optimierung - Ein Mathematischer Ansatz basierend auf einem Modell $\Longrightarrow$ Abbildung der Realität
	\item [Wissensbasiert: ] z.B. Expertensysteme - Ansätze von Künstlicher Intelligenz
	\item [Datenbasiert: ] Basierend auf grossen Datenmengen $\Longrightarrow$ Data-Warehouse, OLAP oder Data-Mining
	\item [KI: ] Basierend auf Vorschlägen von Systemen, die Entscheidungen auf Basis von Daten und/oder gelernten Inhalten ($\longrightarrow$ Machine Learning)
\end{description}


\paragraph{Ein Expertensystem}(XPS oder ES) ist ein Computerprogramm, das Menschen bei der Lösung von komplexen Problemen wie ein menschlicher Experte unterstützen kann, indem es Handlungsempfehlungen aus einer Wissensbasis ableitet.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{expertensystem.png}
	\caption{Beispiel eines Expertensystems}
	\label{fig:xps}
\end{figure}

\newpage

\subsection{Ungenügen der "gängigen" Datenhaltung (Skript S17)}

\begin{itemize}
	\item Verschiedene Datenformate
	\item Verschiedene Werkzeuge
	\item Heterogenität der Daten
		\subitem Technisch (Mainframe / DBMS / Flatfile etc)
		\subitem Logisch (Schemata / Formate / Darstellungen etc.)
		\subitem Syntaktisch (Datum / Codierung / Währung)
		\subitem Qualitativ (Fehlende / Falsche / Doppelte Werte)
		\subitem Verfügbarkeit (Permanent / Periodisch / Temporär)
		\subitem Rechtlich (Datenschutz / Zugriffsverwaltung / Archivierung)
\end{itemize}

$\longrightarrow$ Neuer Ansatz einer Datenaufbereitung muss her: \textbf{Homogenisierung}

\subsection{Ungenügen der operativen Datenbanken für Entscheide (Skript S18)}

"Reguläre" Datenbanken im Geschäftsumfeld sind zu fest mit geschäftsrelevanten Lese- und Schreiboperationen beschäftigt. Bei solchen Datenbanken spricht man von OLTP-System (Online Transactional Processing). Diese Datenbanken sind also ziemlich schlecht geeignet für eine analytische, vorausschauende Bewirtschaftung, da solche Auswertung viel Zeit und vor allem Rechen-Performance benötigen.

Ausserdem liegen Daten in OLTP-Datenbanken meist in der 3. Normalform vor. Während dies eine sehr vernetze und effiziente Art der Datenspeicherung ist, ist die 3. Normalform ein schlechtes Abbild des intuitiven Denkens eines Managers.

$\rightarrow$ Neuer Ansatz einer Datenbank muss her: \textbf{analytische Datenbanken}

\subsection{SQL-Abfragen für Management-Zwecke (Skript S18ff)}
Zusätzlich zu den vorhin genannten Gründen, sind Manager des SQL meist nicht mächtig. Sie wollen lieben "Drag and Drop" Interfaces, um sich ihre Daten "zusammenzuklicken" wie z.b. Microsoft Access.

Zudem sind Datenbank-Abfragen stets \textbf{zweidimensional} in Tabellen dargestellt. Wenn man nun aber Daten in drei Dimensionen auswerten will (z.B. Zeit, Ort und Anzahl), so ist dies zwar möglich mittels Tabellen, aber nicht sonderlich leserfreundlich.

$\rightarrow$ Neuer Ansatz der Datenabfrage muss her: \textbf{OLAP}

\newgeometry{margin=0.5in}

\begin{landscape}
\subsection{OLAP vs OLTP}
	
	\centering
	
	\begin{tabular}[htbp]{|l|r|r|}
		\hline 
		Merkmal & OLTP System  & OLAP System  \\ 
		\hline 
		Ausrichtung auf & Programm, BWL Prozess & Mensch, Analyse\\ 
		\hline 
		Zeitliche Reichweite & Taktisch & Strategisch \\ 
		\hline 
		Entscheidungsstufe & Tief & Hoch \\ 
		\hline
		Zweck & Rationalisierung \& Automatisierung & Planung \& Entscheidung \\
		\hline
		Anwenderzahl &Hoch & Tief \\
		\hline
		Entscheidung & Deduktiv & Induktiv / Explorativ \\
		\hline
		Bewirtschaftung I & Ändernd & Befragend \\
		\hline
		Bewirtschaftung II & Auf Datensatzebene & Auf Aggregatsebene \\
		\hline
		Anwendungsmuster & Voraussehbar & Variierend \\
		\hline
		Befragungsmuster & Einfach & Komplex \\
		\hline
		Bearbeitung & Repetitiv & Ad hoc / unstrukturiert \\
		\hline
		Betriebliches Wissen & Verarbeitend & Generierend \\
		\hline
		Verteilungsgrad & Dezentral & Zentral \\
		\hline
		Performance-Bedarf & Durchgehend hoch & Variierend \\
		\hline
		Mehrbenutzersynchronisation & Hoch & Tief bis keine \\
		\hline
		Optimierung & Schneller Insert \& Delete & Schnelles Lesen \\
		\hline
		Transaktionsdurchsatz & Hoch & Tief \\
		\hline
		Transaktionsdauer & Kurte Mutationen weniger Tupel & Lange Abfragen vieler Tupel \\
		\hline
		Abfragen & Häufige, einfache Abfragen & Weniger häufige, komplexe Anfragen \\
		\hline
		Antwortzeiten & (Mili)sekunden & Sekunden, Minuten, Stunden \\
		\hline
		Endbenutzerwerkzeug-Hersteller & DB-Hersteller & Markt \\
		\hline
		Zeitbezug & Aktuell & Historisch \\ 
		\hline
		Zeitdimension & Zeitpunkt & Zeitraum \\
		\hline
		Beständigkeit & Dynamisch & Statisch \\
		\hline
		Granularität & Fein & Grob \\
		\hline
		Datenbestand & Vollständig & Lückenhaft \\
		\hline
		Redundanz & Normalisiert & Denormalisiert \\
		\hline
		Datenqualität / Aussagekraft & Tief & Hoch \\
		\hline
		Aufbereitung & Anwendungsneutral & Analyseorientiert \\
		\hline
		Aktualisierung & Laufend & Periodisch \\
		\hline
		Verarbeitungseinheit & Keon & Gross \\
		\hline
		Verteilungsgrad & Dezentral & Zentral \\
		\hline
		Datenquelle & Aktuelle Unternehmensdaten & Interne \& externe Daten \\
		\hline
	\end{tabular} 
	\end{landscape}

\restoregeometry

\section{Daten vs. Informationen vs. Wissen vs. Weisheit (Skript S26)}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{DIWW.PNG}
	\caption{DIKW-Pyramid}
	\label{fig:dikw}
\end{figure}

Bei Entscheidungsfindungen muss unterschieden werden zwischen

\begin{itemize}
	\item Daten
	\item Informationen
	\item Wissen
	\item Weisheit
\end{itemize}

\paragraph{Daten}
Daten sind das, was in Datenbanken oder Excel-Tabellen gespeichert wird. \textbf{Unstrukturierte Fakten} wie z.B die Zahlenreihenfolge 
\begin{center}
\textbf{	Rot, 192.234.235.245.678.v2.0}
\end{center}

\paragraph{Informationen}
Aus Daten alleine werden wir nicht schlau. Diese Daten müssen zuerst in einen \textbf{Zusammenhang} gebracht werden:

\begin{center}
\textbf{	Das südliche Rotlicht Pitt/George St. ist soeben rot geworden }
\end{center}

Nun können wir aus dieser, vorher völlig nutzlosen Zahlenreihe eine \textbf{Information} extrahieren. Nämlich dass sie Koordinaten sind und sich das "Rot" auf ein Lichtsignal bezieht..

Die Daten stellen zwar den eigentlichen Wert der Information dar (die Koordinaten und Rotlicht-Licht), sind aber ohne Zusammenhang völlig wertlos.

\paragraph{Wissen}
Aus Informationen \textbf{Wissen} zu machen ist nun, zumindest maschinell gesehen wesentlich schwerer. Wir Menschen generieren Wissen, indem wir Informationen geistig verarbeiten. Soll heissen, wir \textbf{interpretieren und ordnen} die gegebene Information.

Das heisst in diesem Fall, wir checken wo wir sind und in welche Richtung wir uns bewegen. Je nach dem ist das Wissen dann:

\begin{center}
\textbf{	Ich fahre in eine völlig andere Richtung, diese Information interessiert mich nicht.}
\end{center}

oder aber

\begin{center}
\textbf{	Das Rotlicht, auf welches ich zufahre, ist gerade rot geworden}
\end{center}

\paragraph{Weisheit}
Weisheit wird definiert als \textbf{Anwendung von Wissen auf eine Problemlösung}. Das heisst, das erworbene Wissen wird mit einer Portion Erfahrung und gesundem Menschenverstand gemixt. In unserem Fall, angenommen wir fahren auf das Rotlicht zu, sagt uns die Erfahrung, dass man bei einem roten Rotlicht stoppen soll und der gesunde Menschenverstand wirft noch ein, dass es entweder einen Unfall geben wird oder aber sicherlich eine Busse, sollte man erwischt werden. Das Ganze resultiert in:

\begin{center}
\textbf{	Ich sollte vermutlich nächstens einmal anhalten}
\end{center}

\vspace*{20px}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{DIKW.PNG}
	\caption{Die Beziehung zwischen Daten - Informationen - Wissen - Weisheit}
	\label{fig:DIKW}
\end{figure}

Zusammengefasst kann man also sagen, Informationen sind das Verständnis von Zusammenhängen in Daten, oder auch \textbf{was die Daten bedeuten}. Wissen ist das Verständnis dieser Information (\textbf{was heisst das für mich?}) und Weisheit bestimmt, \textbf{was nun zu tun sei}. 
\newpage

\section{Das Data Warehouse}

\blockquote[Buch, S32]{In einer optimalen Welt würden Daten "perfekt" abgelegt werden, leicht zugänglich, platzsparend, sicher und für verschiedene Zwecke nützlich. Da wir aber leider nicht in einer optimalen Welt leben, ist dies nicht der Fall.}
\vspace{10px}


\begin{wrapfigure}[10]{R}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{datawarehouse.PNG}
	\caption{Aufbau eines Datawarehouses}
	\label{fig:datawarehouse}
\end{wrapfigure}

Daten sind in der Praxis meist nicht optimal abgelegt. Daten existieren meist
\begin{itemize}
	\item in unterschiedlichen Formaten (Excel, Access, DB etc)
	\item in unterschiedlichen DB-Strukturen 
	\item in unterschiedlichen IT-Architekturen und -Systemen. Meist auch uralt Legacy-Systeme (Wie z.B. Cobol)
	\item zeit-aktuell und dynamisch
	\item zu detailliert und feingranular für wirksame Management-Abfragen
	\item in einem Format, das für Änderungstransaktionen optimiert wurde (z.B. 3. Normalform)
	\item mit begrenzten Zugriffsrechten (z.B. aus Security-Gründen)
	\item in einem schlecht verfügbaren Zustand (Legacy-System, proprietäres Format, Security-Gründe)
	\item in einem Format, welches komplexe SQL-Queries verlangt, um an Informationen oder Wissen zu gelangen.
\end{itemize}

$\rightarrow$ Lösung: \textbf{Data-Warehouse}

\newpage

\subsection{Definition Data-Warehouse (Skript S 37)}
	
\blockquote[Oracle corp: Data warehousing Guide 11g (2007)]{A data warehouse is a relational database that is designed for query and analysis rather than for transaction processing. It usually contains historic data derived from transaction data, but can incude data from other sources. Data warehouses separate analysis workload from transactin workload and enable an organisation to consoldiate data from several sources.} 
 

\blockquote[IBM Corp: Enterprise Data Warehousing with DB2.9 - Redbook (2008)]{A data warehouse is a organisation's data with a corporate wide scope for use in decision support and informational applications.}
\vspace*{10px}

Zusammengefasst kann man also sagen, ein Data Warehouse ist eine Datenbank, welche nicht (ausschliesslich) zur Speicherung von Informationen genutzt wird, sondern hauptsächlich als Hilfsmittel bei Entscheidungen eingesetzt wird ($\rightarrow$ Experten-Systeme)



\subsection{Bestandteile eines Data-Warehouses (Skript S43)}

\begin{description}
	\item [SSRS: ] SQL Server Reporting Services
	\item [SSAS: ] SQL Server Analysis Services
	\item [SSIS: ] SQL Server Integration Services
\end{description}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{bestandteiledatawarehouse.PNG}
	\caption{Bestandteile eines Data-Warehouses}
	\label{fig:bstdw}
\end{figure}

\newpage

\subsection{Welche Datenbank für welche Tasks? (Buch S40ff)}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{bestandteileDW.png}
	\caption{Welche Bestandteile eines Data-Warehouses werden für was benutzt?}
	\label{fig:bestDW}
\end{figure}


Verschiedene Datenbanken können (und sollten) für verschiedene Tasks verwendet werden.

\newpage

\section{Referenzarchitekturen}
\begin{wrapfigure}[15]{L}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{refModell.png}
	\caption{Referenzmodell eines Data Warehouses nach Bauer \& Günzel}
	\label{fig:refModel}
\end{wrapfigure}


Eine \textbf{Referenzarchitektur} ist ein Referenzmodell für eine Überklasse von Architekturen, in unserem Falle von Date Warehouses.

Ein \textbf{Referenzmodell} ist ein allgemeines Modell für eine Klasse von Dingen. Ein Referenzmodell sollte folgende zwei Eigenschaften haben:

\begin{itemize}
	\item Es können bestimmte Sachverhalte oder Modelle auf dessen Basis erstellt werden.
	\item Das allgemeine Modell kann als Vergleichsobjekt dienen.
\end{itemize}

In diesem Modul wird ausschliesslich mit dem hier dargestellten Referenzmodell von Bauer \& Günzel gearbeitet.

\vspace{3cm}

\subsection{Bestandteile des Referenzmodells (Skript S45ff)}
\subsubsection{Datenquelle (Skript S46)}
Als \textbf{Datenquelle} eines Data-Warehouses werden oft beliebige\textit{ Bezugs-Datenbestände} benutzt (auch \textit{effektive Daten} oder \textit{Primärdaten} genannt)

Das können zum Beispiel:
\begin{itemize}
	\item Daten aus \textbf{Legacy-Systemen}
	\item Daten aus \textbf{Anwendungsprogrammen}
	\item Daten aus \textbf{zentralen/dezentralen Arbeitsplatz-DBs}
	\item \textbf{Textdateien} (z.B. im ASCII oder UTF-8 Format)
	\item Tabellen aus z.B Excel
\end{itemize}

sein.

\newpage

Es gibt weiterhin einige Probleme mit Datenquellen aus verschiedenen Systemen, wie zum Beispiel:
\begin{itemize}
	\item Unterschiedliche Zeichencodierung (ASCII, ANSI, UTF-8)
	\item Unterschiedliche Trennzeichen zwischen Datenfeldern (Komma, Semikolon)
	\item Unterschiedliche Zeilenwechsel (CR/LF, LF)
	\item Unterschiedlichen Sortierungen (alphanumerisch, numerisch)
\end{itemize}

Deshalb wird immer häufiger auf einheitliche XML-Inputs zurückgegriffen.

\subsubsection{Arbeitsbereich / Staging Area (Skript S46 / Buch S55)}
Der Arbeitsbereich integriert die Daten aus den verschiedenen vorhin genannten Datenquellen. Diese Integration passiert nach der Extraktion aus diesen Datenquellen.

Die Daten werden mit Hilfe des Metadaten-Repository anhand ihrer Metadaten zusammengefügt und abgelegt.

Das \textbf{Metadaten-Repository} besteht normalerweise aus verschiedenen Datenbank-Tabellen zur Verwaltung von Metadaten. Diese Metadaten stammen aus sehr unterschiedlichen Systemen und enthalten alle notwendigen Beschreibungen zu ihrem System und der Umwelt. Somit können die heterogenen Daten fast ohne Programmieraufwand zu einer homogenen Masse zusammengefügt werden.

\vspace{10px}

\noindent Die Staging Area ist ein \textbf{flüchtiges Zwischendepot} für die Daten. Hier werden die notwendigen Transformationen durchgeführt werden, welche in weiteren Schritten notwendig sind.

\subsubsection{Basis-DB / Operational Data Store (ODS) (Skript S47 / Buch S58)}
Ein ODS ist eine Datenbank, welche aktuelle/operative Daten hält, meist in kleinen Teilmengen unterteilt. Diese Datenbank ist eine \textbf{temporäre} Zwischenstation zwischen der Staging Area und dem Data Warehouse. 

\vspace{10px}

\noindent Das Datenmodell des OBS entspricht meist demjenigen der Datenquelle. Dies ist vor allem dann der Fall, wenn der OBS aus Leistungsgründen vom Quellsystem getrennt wurde und benutzt wird um gelegentliche Einzelfall-Analysen durchzuführen.

\vspace{10px}

\noindent Das Datenmodell des OBS kann andererseits auch demjenigen des Data Warehouses entsprechen. Dies ist dann der FAll, wenn der ODS als temporärer Zwischenspeicher zwischen den Quelldaten und dem Data Warehouse genutzt wird, oder aber wenn der ODS vom Data Warehouse Daten erhält.

Eine Basis-DB/ein ODS muss nach Gauer \& Günzel folgende Eigenschaften haben:
\begin{itemize}
	\item Die Daten sind \textbf{integriert} von den jeweiligen Datenquellen. Das heisst die verschiedenen Datenformate und Schematas wurden vereinheitlicht.
	\item Sie enthält nebst aktuellen Daten auch \textbf{historische Daten}, jedoch in geringerer Feingranularität wie das Data Warehouse.
	\item Sie ist \textbf{Anwendungsneutral}, d.h. sie ist nicht für eine spezielle Anwendung optimiert bzw. fokussiert.
	\item Nach einer definierten Zeitspanne werden die Daten in die \textbf{Ableitungsdatenbank} übertragen, wo sie je nach Auswertungsbedarf in einem anderen Detaillierungsgrad abgelegt werden.
	\item Die \textbf{Aktualisierung} der Daten erfolgt zu \textbf{beliebigen Zeitpunkten}. Dieser Zeitpunkt wird durch den Aktualisierungsbedarf gesteuert.
	\item Die Daten wurden bereits in der Staging Area bereinigt.
\end{itemize}

\vspace{10px}

\noindent Der ODS wird entweder laufend oder periodisch mit Daten aus der Staging Area gefüttert. Vor allem in kleineren Data Warehouse Systemen fällt der ODS sogar manchmal ganz weg und die Staging Area übernimmt auch diese Arbeiten. Im professionellen Umfeld sind jedoch sowohl der ODS wie auch die Stagin-Area ein integraler Bestandteil der Data Warehouse Architektur. 

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{ODS.png}
	\caption{Eingliederung des ODS in die Data Warehouse Architektur}
	\label{fig:ODS}
\end{figure}

\subsubsection{Ableitungsdatenbank (=(Enterprise-)Data Warehouse) (Skript S47/Buch S64)}
Wenn von einem Data Warehouse gesprochen wird, ist in der Regel ein \textit{Enterprise-Data-Warehouse} gemeint (= ein DWH mit unternehmensweitem Datenmodell und unternehmensweiten, universellen Datenbestand). 

\vspace{10px}

\noindent Die Datenbestände eines Data Warehouses werden zwar periodisch ergänzt, jedoch werden praktisch nie Daten gelöscht oder verändert, wenn sie mal im Data Warehouse sind. Aufgrund dessen beinhalten solche Systeme eine enorme Menge von Daten (auch VLDBs für Very Large DataBases).

Es ist jedoch selten nötig auf den gesamte§n Datenbestand dieser riesigen Datenbanken zuzugreifen. Aufgrund dessen werden Daten, die sehr selten benutzt werden in grossen Unternehmungen meist in Teilkopien des Data Warehouses, sogenannten \textit{Data Marts} gespeichert.

\subsubsection{Auswertungsdatenbank (Data-Mart) (Skript S50/Buch S67)}
Ein Data-Mart ist eine \textbf{Teilkopie} eines Data-Warehouses, die aber auf demselben Datenmodell basieren. Solche Data-Marts werden meist für Abteilungen einer Unternehmung wie z.B. das Marketing erstellt. Der Vorteil dieser Data-Marts ist, dass diese unabhngig sind vom zentralen DW.

Vorteile eines Data-Marts sind z.B. 

\begin{itemize}
	\item Bessere Leistung, da nicht alle Analysen/Auswertungen auf dem zentralen DW gemacht werden müssen
	\item Entlastung des DW
	\item Im Falle von lokalen Data-Marts weniger Netzwerkbelastung
\end{itemize}

Die Pflege und (Weiter)Entwicklung des Data-Marts liegt jeweils in der Verantwortung der einzelnen Abteilungen.

\vspace{10px}

\noindent Dass ein Data-Mart existieren kann, \textbf{muss} ein zentrales Data-Warehouse existieren. Deshalb werden solche Data-Marts meist als \textbf{abhängige Data-Marts} bezeichnet.

 Diese abhängigen Data-Marts stehen im Gegensatz zu den (historischen) \textbf{unabhängigen Data-Marts}. Diese Data Mars erhalten ihre Daten direkt von verschiedenen Quellsystemen (gehen also nicht über ein zentrales Data-WArehouse). Jedoch widerspricht das dem Gedanken von einem zentralen "Datenhort" des Data-Warehousing und \textit{sollten} heute nicht mehr verwendet werden (oder man sollte sie zumindest nicht mehr als Data-Marts bezeichnen...)

Ein "Problem" von Data-Marts ist, dass sie unabhängig voneinander sind. Somit werden meist dieselben Aggregationen mehrmals auf verschiedenen Data-Marts durchgeführt, was auf längere Zeit nicht sehr kosteneffizient ist. Stattdessen sollten diese Aggregationen eher auf dem zentralen DW durchgeführt werden (was aber nicht immer geht, da die verschiedenen Abteilungen nicht wissen, dass diese Aggregation bereits in einem anderen Data-Mart durchgeführt wurde)

\newpage

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{DWH_Bereiche.png}
	\caption{Bereiche eines Data-Warehouses nach Bauer \& Günzel}
	\label{fig:bereiche}
\end{figure}

\subsubsection{Integrationsbereich (Skript S54)}

Der Integrationsbereich bereitet die Daten, die aus verschiedenen Datenquellen kommen via staging area und ODS so auf, dass sie ins zentrale DW überführt werden können. 

Im Zusammenhang mit dem Integrationsbereich wird oft von \textbf{ETL} gesprochen. ETL steht für \textit{Extract - Transfer - Load} und entspricht den wesentlichen Arbeitsschritten, die durchgeführt werden müssen, bis die Daten vom Quellsystem im DW landen.



\subsubsection{Auswertungsbereich (Buch S72)}
Der Auswertungsbereich des Data Warehouses umfasst die \textbf{Ableitungsdatenbank/Data Warehouse} und die \textbf{Auswertungsdatenbank}. Genaueres über diese beiden Bestandteile des Auswertungsbereiches können in Kapitel 4.1.4 - Ableitungsdatenbank und 4.1.5 - Auswertungsdatenbank nachgelesen werden.

\vspace{10px}

\noindent Wie der Name schon vermuten lässt, werden die Daten, die von den Datenquellen über ETL-Prozesse in die Basisdatenbank geladen wurden, hier ausgewertet. 

Auswerten kann vieles sein. So können einfache Aggergationsfunktionen zur Auswertung benutzt werden, aber auch komplexe statistische Methoden, die z.B. beim Data Mining zum Einsatz kommen.

Zusammengefasst kann man sagen: Eine \textbf{Auswertung} ist die \textit{Anwendung von Auswertefunktionen auf ausgewählte Daten zur Generierung von neuer Information} .

Zudem werden die ausgewerteten Daten so aufbereitet und bereitgestellt, dass sie auch in anderen Systemen weiterverarbeitet werden können und/oder an andere Personen/Unternehmen weitergegeben werden.

Last but not least werden die Ergebnisse der Auswertung wieder in die Basisdatenbank zurückgegspeichert, so dass sie die Qualität der Datenbasis erhöht und zukünftige Auswertungen somit verbessert. 

%TODO: finish this


\subsubsection{Verwaltungsbereich (Skript S272)}
Der Verwaltungsbereich eines Data Warehouses enthält den \textbf{Data Warehouse Manager}, den \textbf{Metadaten Manager} und das \textbf{Repositorium} (Abb. \ref{fig:bereiche})

\paragraph{Data Warehouse Manager}\mbox{}\\
Wie der Name dies schon vermuten lässt, managt der Data Warehouse Manager (ein \textbf{Mensch}) das Data Warehouse, oder besser gesagt, dessen Prozesse. 

Wie in Abb. \ref{fig:bereiche} zu sehen ist, übernimmt der Data Warehouse Manager hauptsächlich die Steuerung der folgenden Prozesse:

\begin{itemize}
	\item Monitor
	\item Extraktion
	\item Transformation
	\item Laden
	\item Auswertung
\end{itemize}
\vspace{10px}

\noindent Aufgrund dessen, dass er den Monitor überwacht, der wiederum die Datenquellen überwacht, kann der Data Warehouse Manager die \textbf{Datenbeschaffung triggern}.

\paragraph{Metadaten Manager (Buch S. 43ff)}\mbox{}\\
Ähnlich wie beim Data Warehouse Manager kann beim Metadaten Manager recht einfach auf dessen Tätigkeit geschlossen werden: Es ist eine \textbf{Software}, die die Metadaten überwacht, steuert und initialisiert des Data Warehouses.

\vspace{10px}

\noindent Genauer gesagt heisst das, der Metadaten Manager verwaltet das \textbf{Metadaten Repositorium}, in welchem alle Metadaten (also Operative, Struktur-, Prozess- und Begriffsmetadaten) gespeichert werden.

Es können ausserdem Metadaten aus Entwurfs- und Modellierungswerkzeugen (wie z.B. SSMS oder MySQL Workbench) extrahiert werden

\vspace{10px}

\noindent Der Metadaten Manager übergibt ausserdem die Metadaten, die er aus dem Repositorium zieht dem Data Warehouse Manager, der diese wiederum an die metadatengesteuerten Werkzeuge, die diese zur Laufzeit lesen, interpretieren und ausführen. Diese Werkzeuge erzeugen selbst wieder Metadaten (z.B. Log- oder Reportdateien), die schlussendlich wieder im Repositorium landen und später wieder diesen Tools gefüttert werden. 

Das Ziel ist es, eine vollständig automatisierte Aktualisierung der Metadaten zu erreichen.

\paragraph{Repositorium (Buch S79)}\mbox{}\\
Das Repositorium ist der zentrale Storage für die Metadaten im Data Warehouse.

\newpage

\subsection{Referenzmodelle in die Praxis umgesetzt (Skript S63)}
\subsubsection{"Hub and Spoke"-Architektur}
\begin{wrapfigure}[14]{L}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=16\baselineskip]{HubAndSpoke.PNG}
	\caption{Hub and Spoke / Nabe und Speiche Architektur}
	\label{label}
\end{wrapfigure}

Bei der Hub and Spoke (auch Nabe und Speiche Arhictektur) werden aus mehreren Quellsystemen (Speichen/Spokes) Daten in eine Basisdatenbank geladen, dort ETL-ed und anschliessend in eine Ableitungsdatenbank überführt (Hub/Nabe). Von dort werden sie anschliessend wieder in verschiedene Auswertung-Datenbanken verteilt (Speiche/Spokes).

\vspace{10px}

\noindent Das Core-Data-Warehouse ist also eine der zentralen Naben oder Hubs und ist verantwortlich für die Integration, QA und die Verteilung von Daten an die verschiedenen Data-Marts (Auswertungs-Datenbanken). 

\vspace{80px}


\subsubsection{Star-Architektur}

\begin{wrapfigure}[16]{R}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=16\baselineskip]{starschema.jpg}
	\caption{Starschema mit einer Fakttabelle in der Mitte und fünf Dimensionstabellen}
	\label{fig:star}
\end{wrapfigure}

Die Star- oder Stern-Architektur ist eine der drei am weitesten verbreiteten Datenmodelle. Sie besteht aus einer sog. \textbf{Faktentabelle} in der Mitte, die von mehreren \textbf{Dimensionstabellen} sternförmig umgeben ist. 

Durch eine solche Anordnung sind die Datenbanken normalerweise \textit{denormalisiert}, können also redundante Daten enthalten. Jedoch wird dieser eventuelle höhere Speicherbedarf in Kauf genommen, da ein solches Sternschema eine wesentliche höhere Performance bietet im ETL-Bereich wie eine normalisierte Datenbank.

\vspace{10px}

\noindent Eine weitere Verbesserung der Performance könnte man mit dem, aus der Stern-Architektur abgeleiteten \textbf{Schneeflockenarchitektur} erzielen.

\newpage

\subsubsection{Snowflake-Architektur}
\begin{wrapfigure}[21]{L}{0.7\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{snowflake.PNG}
	\caption{Beispiel einer Schneeflocken-Architektur}
	\label{label}
\end{wrapfigure}

Die Snowflake- oder Schneeflocken-Architektur ist eng mit der Stenr-Architektur verbunden. Der einzige Unterschied zwischen der Stern- und der Schneeflocken-Architektur ist, dass die Dimensionstabellen gleichzeitig auch Faktentabellen sind, die wiederum von anderen Tabellen umgeben sind. Diese Dimensionstabellen werden über Joins verknüpft. 

\subsubsection{Galaxy-Architektur}

\begin{wrapfigure}[19]{R}{0.7\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{galaxy.PNG}
	\caption{Galaxy-Architektur}
	\label{fig:galaxy}
\end{wrapfigure}

Die Galaxy- oder Galaxie-Architektur ist ebenfalls stark mit dem Stern-Schema verwandt. Allerdings können sich hier mehrere Faktentabelle eine Dimensionstabelle teilen.

\vspace{20px}

\noindent Bei den drei oben genannten Architekturen werden Daten als \textbf{Fakten} bezeichnet und somit in der \textbf{Faktentabelle} gespeichert. Fakten werden, je nach Literatur auch \textbf{Metriken} oder \textbf{Kennzahlen} genannt. Solche Faktentabellen können über die Zeit sehr gross werden und enthalten Kennzahlen, die sich aus dem laufenden Geschäft ableiten lassen, wie z.B. Profitabilität, Kosten oder Ausgaben.

Wie in Abbildung \ref{fig:star} zu sehen ist, enthalten Faktentabellen Fremdschlüssel zu den Einträgen in den Dimensionstabellen. Das impliziert, dass es jeden Eintrag zu einer Kombination von Dimensionen nur einmal geben kann.

Im Gegensatz zu den riesigen und volatlen Faktentabellen sind Dimenstionstabellen recht statisch und vergleichsweise klein. 

\section{ETL}
\subsection{Extraktion}

Unter Extraktion versteht man die \textbf{Übertragung der Daten vom Quellsystem in die Staging Area}

\vspace{10px}

\noindent Je nach dem, welche Extraktions-Strategie gewählt wurde, werden die Daten zu unterschiedlichen Zeitpunkten extrahiert:

\begin{description}
	\item[Sofort] Der Extraktionsprozess wird bei jeder Änderung sofort gestartet
	\item[Trigger-/Ereignisbasiert] z.B. Beim Erreichen einer festgelegten Anzahl von Änderungen
	\item[Periodisch] Die Daten haben eine geforderte Mindestaktualität. Nach Ablauf dieser festgelegten Periode werden die Änderungen in eine sog. Snapshot-Datei überführt, die anschliessend in die Datenbank importiert wird.
	\item[Anfrage] Eine Anfrage auf die Datenbank erfordert aktualisierte Daten.
\end{description}

\vspace{10px}

\noindent Es gibt zwei grundsätzlich zwei \textbf{Extraktionstechniken}
\begin{description}
	\item[Replikationsbasiert] Geänderte Tupel werden in spezielle Tabellen geschrieben
	\item[Logbasiert] Jede Änderung wird in einer Logdatei protokolliert. Durch die Auswertung dieser Logdatei wird entschieden, welche Daten geändert und somit extrahiert werden müssen
\end{description}

\noindent Die Extraktion geschieht meist über SQL und es werden meist Schnittstellen zu den Datenbanken verwendet wie z.B. ODBC, OLE DB oder JDBC.

\vspace{10px}

\noindent \textbf{Open Database Connectivity (ODBC)} ist eine standardisierte Datenbankschnittstelle, mit welcher über SQL auf die Datenbank zugegriffen werden kann. Der Vorteil von ODBC ist, dass es eine API gibt, die es einem erlaubt, eine DB-Anwendung zu schreiben, die unabhängig vom Datenbankmanagement-System (DBMS) (mySQL, postgreSQL, Oracle etc) funktioniert, solange ODBC einen Treiber dafür hat.

\vspace{10px}

\noindent \textbf{Java Database Connectivity (JDBC)} ist eine universelle Java-basierte Datenbankschnittstelle für Datenbanken diverser Hersteller. JDBC ist vor allem auf relationale Datenbanken ausgelegt.


\subsection{Tranformation}

Nach der Extraktion müssen diese rohen Daten transformiert werden. Das heisst, sie müssen so aufbereitet werden, dass sie auch tatsächlich Sinn ergeben. Die Transformation lässt sich auf folgende Arbeitsschritte herunterbrechen:

\begin{description}
	\item[Data migration / Data wrangling] Daten werden \textbf{vereinheitlicht} also dass sie z.B. dieselben Masseinheiten oder Datenformate verwenden.
	\item[Data clean(s)ing] Daten werden \textbf{bereinigt}, also doppelte Daten gelöscht, falsche Daten korrigiert oder veraltete Daten aktualisiert.
\end{description}

\vspace{10px}

\noindent Dabei nimmt das Data Wrangling mit Abstand am meisten Zeit in Anspruch (bis zu 80\%)


\subsection{Load}

Die extrahierten und bereinigten Daten müssen nun noch in die \textbf{Basis- oder Ableitungsdatenbank übertragen} werden. Daür werden meist Ladewerkzeuge des Datenbankherstellers verwendet (SSIS bei SQL oder SQL*Loader bei Oracle)

\vspace{10px}

\noindent 
Weitere Möglichkeiten sind z.B. die SSIS Integration von Visual Studio, SSIS direkt oder DataWriter von IBM

\vspace{10px}

\textbf{SQL-Codebeispiele für den Load-Prozess}

\vspace{10px}

\noindent SQL-INSERT

\begin{lstlisting}[language=SQL]
INSERT INTO table_name
	VALUES (
		value1,
		value2,
		value3,
		...
	)
; 
\end{lstlisting}

\noindent SQL BULK INSERT

\begin{lstlisting}[language=SQL]
//FileType=1 (TxtFile1.txt)

"Kelly","Reynold","kelly@reynold.com" 
"John","Smith","bill@smith.com" 
"Sara","Parker", "sara@parker.com"

//FileType=2 (TxtFile2.txt) 

Kelly,Reynold,kelly@reynold.com 
John,Smith,bill@smith.com 
Sara,Parker,sara@parker.com 

BULK INSERT TmpStList FROM 'c:\TxtFile1.txt' WITH (FIELDTERMINATOR = '","') 
BULK INSERT tmpStList FROM 'c:\TxtFile2.txt' WITH (FIELDTERMINATOR = ',')
\end{lstlisting}


\vspace{10px}

\noindent Aufgrund dessen, dass während des Ladens die betroffenen Datenbanken gar nicht oder nur eingeschränkt nutzbar sind, ist die Effizienz des Ladevorgang essentiell. Aus diesem Grund werden Ladevorgänge meist in der Nacht, am frühen Morgen oder übers Wochenende durchgeführt.

Es wird dabei zwischen \textbf{Online- und Offline-Ladevorgängen} unterschieden. Beim Online-Laden kann die Datenbank weiterhin verwendet werden. Beim Offline-Ladevorgang ist die Datenbank während des Vorgangs offline, kann also nicht verwendet werden. Ob eine Datenbank einen Online-Ladevorgang unterstützt, hängt davon ab, ob die verwendete Anwendung oder Datenbank dies zur Verfügung stellt. So könnten z.B. während des Ladevorgang die Datenänderungen in ein Logfile geschrieben werden und dieses Logfile wird nach erfolgreichem Ladevorgang in die aktualisierte Datenbank übertragen.

Ebenfalls unterscheidet man zwischen dem \textbf{Initialisierungsladen} (alle Daten, DB wird zum ersten Mal gefüllt) und dem \textbf{Aktualisierungsladen} (nur geänderte Daten werden in die DB geladen)

Daten werden entweder in eine Basis- Ableitungs- oder Auswertungsdatenbank geladen. Aufgrund der grossen Datenmengen können nicht die normalen Datenmanipulationswerkzeuge eingesetzt werden. Stattdessen werden meist sog. \textbf{Bulk-Loader},die auf genau diese Tasks spezialisiert sind, verwendet. Diese Bulk-Loader verwenden aus Effizienzgründen keine Schnittstelle zur DB, sondern greifen direkt darauf zu. Deshalb können bestimmte Bulk-Loader auch nur für bestimmte Datenbanken verwendet werden. 

\subsection{Monitor}
Der Monitor spielt je nach Extraktionsstrategie eine wichtige Rolle. Er überwacht die Datenquellen und bemerkt somit Datenänderungen sofort. Je nach Extraktionsstrategie wird somit ein ETL-Prozess getriggert, ein Logeintrag oder eine Snapshot-Datei gemacht o.ä. 

Monitore haben vor allem in Data Warehouses eine sehr grosse Bedeutung. Dank ihnen müssen weniger DB-Zugriffe gemacht werden und gleichzeitig werden Redundanzen vermieden.

\vspace{10px}

\noindent Es gibt einige Sachen, die zu bedenken sind beim Einführen einer Monitor-Lösung:
\begin{itemize}
	\item Wird die gesamte Änderung festgehalten oder nur das neue/geänderte Tupel? ($\rightarrow$ Je nach dem Platzproblem)
	\item Benachricht das Quellsystem den Monitor sobald es eine Änderung gegeben hat ($\rightarrow$ Triggering) oder fragt der Monitor regelmässig beim Quellsystem nach ($\rightarrow$ Polling)? ($\rightarrow$ je nach dem, Überbelastung von Quell- oder Zielsystem)
	\item Überwacht das Quellsystem selbst, welche Änderungen erfolgen, oder wird dies von einem externen Dienst übernommen? ($\rightarrow$ Überbelastung Quellsystem vs. Kosten eines externen Dienstes)
	\item Sollen Änderungen sofort zum Data Warehouse gepusht werden oder nur zu bestimmten Zeiten (z.B. über Nacht)? ($\rightarrow$ Belastung des Data Warehouses und Quellsystems vs. Synchrone Daten.)
	
\end{itemize}

\newpage

\section{Unstrukturierte Daten}
Ein Grossteil der Daten, der uns heute vorliegt, ist \textbf{unstrukturiert}. Das heisst, es ist mit viel Aufwand verbunden, aus grossen Datenmengen Informationen zu extrahieren (man denke z.B. daran, wie viel Aufwand es benötigen würde, um 10'000 E-Mails nach Thema zu sortieren vs. wie viel Aufwand es benötigt, 10'000 Datensätze nach ID zu sortieren).

Mithilfe von \textbf{strukturierten Daten} kann man \textit{automatisiert} Informationen aus Datenquellen extrahieren und aufbereiten.

\subsection{Nutzen}

\noindent Wenn man heute unstrukturierte Daten auswerten würde, könnte man z.B. automatisch Kundenbewertungen und -beschwerden aus Foren und Blogs auslesen, Twitter-Posts analysieren u.v.m. 

Zusammengefasst kann man sagen: Unstrukturierte Daten ermöglichen \textbf{Analysen, die zu schnellen Reaktionen und Entscheidungen führen}.5

\subsection{Herausforderungen}

\begin{itemize}
	\item Kein vorgegebenes/einheitliches Schema
	\item Kein vorgegebenes/einheitliches Datenformat
	\item Semantikabhängig (z.B. JSON vs. XML)
	\item Grosse Mengen $\rightarrow$ können nicht manuell ausgewertet werden.
	\item Priorität kann nicht automatisiert gemessen werden.
	\item Datenqualität kann nicht automatisiert gemessen werden
\end{itemize}

\subsection{Analyse}

\subsubsection{NLP - Natural Language Processing (Skript S74)}
 NLP beschreibt die Techniken und Methoden zur maschinellen Verarbeitung natürlicher Sprache. Ziel ist eine direkte Kommunikation zwische nMensch und coputer auf Basis der natürlichen (menschlichen) Sprache.

NLP ist eine sehr AI-lastige Technologie und mittels welcher Text in strukturierte Informationen gebündelt werden kann. NLP setzt sich mittels der Verarbeitung der natürlichen Sprache, dem Verstehen und der Semantik von 
Wörtern und Sätzen der Klassifizierung von Texten auseinander.

\subsubsection{Data Mining}
 Data Mining ist die systematische Anwendung statistischer Methoden auf grosse Datenbestände mit dem Ziel, neue Querverbindungen und Trends zu erkennen.

\subsubsection{Text Mining}
 Text Mining ist eine Untermethode von Data Mining und konzentriert sich vor allem auf Data Mining von grossen Texten wie z.B. die Plagiaterkennung, die gesamte Doktorarbeiten durchliest und analysiert.
 
 \subsubsection{Klassifikation}
 Klassifikation ist eine Unterart des Data-Minings und versucht, den Datenbestand in vorgegebene Klassen zuzuordnen.
 
 \subsubsection{Regression}
 Die Regression ist eine Unterart des Data-Minings und zielt darauf ab, einen Ursache-Wirkung-Zusammenhang zwischen einzelnen Merkmalen der zugrunde liegenden Datenbasis zu ermitteln. 
 
 Am Beispiel von Abbildung \ref{fig:regclass} kann Klassifikation vs. Regression so unterschieden werden: Mittels \textbf{Klassifikation} kann ermittelt werden, dass ca. die Hälfte der untersuchten Gene krank sind, während mit der \textbf{Regression} gezeigt werden kann, dass, je höher die Konzentration von Gen 1 ist, desto länger leben die Patienten (Trend)
 
 \begin{figure}[htb]
 	\centering
 	\includegraphics[keepaspectratio=true,height=15\baselineskip]{regressionvsclassification.png}
 	\caption{Klassifikation (links) vs. Regression (rechts)}
 	\label{fig:regclass}
 \end{figure}

\newpage

\subsubsection{Abhängigkeitsentdeckung / Assoziationsanalyse}
Die Abhängigkeitsentdeckung oder auch Assoziationsanalyse wird verwendet, um Zusammenhänge von Merkmalen und Muster und/oder Abhängigkeiten in grossen Datenbeständen zu finden. 

Ein Beispiel davon ist die \textbf{Warenkorbanalyse}. Es soll gezeigt werden, dass Kunden, die Zahnbürsten kaufen, mit einer grossen Wahrscheinlichkeit auch Zahnpasta kaufen.
\vspace{10px}

\noindent Die Analyste geschieht auf Basis von zwei Werten. Dem \textbf{Support} und der \textbf{Confidence}. Der Support ist die Wahrscheinlichkeit, dass der Kunde sowohl X, wie auch Y kauft. Die Confidence ist die Wahrscheinlichkeit, mit welcher der Kunde noch Y kauft, nachdem er bereits X im Warenkorb hat.
\vspace{10px}

\noindent Mathematisch kann das so ausgedrückt werden:

\begin{equation}
confidence({X} \rightarrow {Y}) = \dfrac{support({X, Y})}{support({X})}
\end{equation}

Ein ebenfalls wichtiger Begriff ist der \textbf{Lift}. Er beschreibt die Abhängkeit zwischen der Zahnbürste und der Zahnpasta im Warenkorb.

\begin{description}
	\item[Lift = 1: ] X und Y werden unabhängig voneinander gekauft.
	\item[Lift $>$ 1: ] Es besteht eine positive Abhängigkeit. Wenn jemand X kauft, kauft er wahrscheinlich auch Y.
	\item[Lift $<$ 1: ] Es besteht eine negative Abhängigkeit. Wenn jemand X kauft, ist es unwahrscheinlich, dass er auch Y kauft.
\end{description}

\paragraph{Beispiel}
Es soll die Abhängigkeit zwischen dem Verkauf von \textbf{Zahnbürsten} und \textbf{Zahnpasta} überprüft werden.

\begin{itemize}
	\item 20\% aller Kunden kaufen sowohl eine Zahnbürste, wie auch Zahnpasta.
	\item 10\% aller Kunden kaufen nur eine Zahnbürste.
	\item 40\% aller Kunden kaufen nur Zahnpasta
\end{itemize}

\begin{equation}
lift(Zahnbuerste \rightarrow Zahnpasta) = \dfrac{support({Zahnbuerste \rightarrow Zahnpasta})}{support(Zahnbuerste) * support(Zahnpasta)}
\end{equation}

Mit den oben genannten Zahlen eingesetzt ergibt dies:

\begin{equation}
	lift(Zahnbuerste \rightarrow Zahnpasta) = \dfrac{0.2}{0.1 * 0.4} = 5
\end{equation}

Der Lift ist also wesentlich höher wie 1, woraus geschlossen werden kann, dass eine \textbf{starke Abhängigkeit} besteht zwischen dem Kauf von Zahnbürsten und Zahnpasta.
 
 \newpage
 
 \subsubsection{Clustering}
 
  \begin{figure}[htb]
 	\centering
 	\includegraphics[keepaspectratio=true,height=12\baselineskip]{clustering.png}
 	\caption{Partitionierendes vs. Hierarchisches Clustering}
 	\label{fig:Clustering}
 \end{figure}
 
 Clustering ist ähnlich wie die Klassifikation, allerdings werden beim Clustering-Verfahren automatisch passende Klassen erstellt. Das Clustering-Verfahren zielt darauf ab, Gruppen oder Klassen zu erstellen, innerhalb von welchen sich die Objekte sehr ähnlich sind. Gleichzeitig sollten sich die erstellten Gruppen aber möglichst stark voneinander unterscheiden.
 
 Clustering wird normalerweise nur zu Beginn ausgeführt, um Klassen zu erstellen, mit welchen anschliessend eine Klassifikation durchgeführt werden kann.
 
 \vspace{10px}
 
 \noindent Es wird generell zwischen dem \textbf{partitionierenden} Clustering und dem \textbf{hierarchischen} Clustering unterschieden (Abbildung \ref{fig:Clustering}):
 
 \paragraph{Partitionierendes Clustering} unterteilt die gegebenen Daten in eine \textbf{vorgegebene Anzahl} von Klassen. Dabei werden zuerst die vorgebene Anzahl Clusterzentren/Klassen aus den Datensätzen ausgewählt. Anschliessend werden die restlichen Datensätze diesen Clusterzentren zugeordnet. 
 
 Nach dem ersten Durchgang werden erneut Clusterzentren/Klassen gewählt und die Daten werden wiederum diesen, neuen, Clusterzentren zugeordnet. Dieses ganze Spiel wird solange wiederholt, bis die Daten 'optimal' geclustert sind (möglichst ähnlich innerhalb der Klasse/des Clusters, möglichst verschieden zwischen den Klassen/Cluster).
 
   \begin{figure}[htb]
 	\centering
 	\includegraphics[keepaspectratio=true,height=12\baselineskip]{hierarchical_clustering.png}
 	\caption{Beispiel von agglomerativem (oben) und diversiven (unten) clustering}
 	\label{fig:hierCluster}
 \end{figure}
 
 \paragraph{Hierarchisches Clusteriong} kann wiederum in \textit{agglomeratives} und \textit{diversives} Clustering unterteilt werden. (Siehe Abbildung \ref{fig:hierCluster})
 
 Das \textit{agglomerative} Clustering funktioniert so, dass jeder Datensatz eine eigene Klasse/ein eigenes Cluster ist. Anschliessend werden die \textbf{zwei ähnlichsten Klassen zu einer zusammengefasst}. Dieses Zusammenfassen zweier ähnlicher Klassen wird solange durchgeführt wird, bis alle Daten in einem einzigen Cluster zusammengeführt sind.
 
 \vspace{10px}
 
 \noindent Beim \textit{diversiven} Clustering geht es in die andere Richtung. Die Datensätze werden bei jeder Iteration so halbiert, dass die Elemente innerhalb eines Clusters sich so ähnlich wie möglich sind, die Cluster untereinander aber möglichst unterschiedlich zueinander sind ('optimale' Cluster also). Diese Unterteilung wird solange fortgesetzt, bis jeder Datensatz seinen eigenen Cluster hat. Dieses Verfahren wird in der Praxis aber eher weniger angewandt.
 
 \subsubsection{Visualisierungstechnik}
 \begin{wrapfigure}[10]{L}{0.6\textwidth}
 	\centering
 	\includegraphics[keepaspectratio=true,height=13\baselineskip]{streudiagramm.PNG}
 	\caption{Daten visuell aufbereitet mittels eines Streudiagrammes}
 	\label{fig:streu}
 \end{wrapfigure}
 Bei der Visualisierungstechnik überlässt man die tatsächliche Auswertung dem Menschen. Die Daten werden mittels \textbf{zwei bis drei Merkmalen} im \textbf{zwei- oder dreidimensionalen} Raum dargestellt (z.B. Streudiagramm, siehe Abbildung \ref{fig:streu}). Es ist nun dem Menschen überlassen, aus dieser Grafik etwas nützliches herauszulesen.
 
 \vspace{90px}

\subsubsection{Fallbasierte Systeme}

\begin{wrapfigure}[12]{R}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=12\baselineskip]{fallbasiert.png}
	\caption{Funktionsweise eines Fallbasierten Systemes}
	\label{fig:fallbasiert}
\end{wrapfigure}

Solche Systeme greifen auf Daten von ähnlichen, bereits gelösten Problemen zurück. Es gibt eine \textbf{Falldatenbank} gefüllt mit bereits gelösten, getaggeden oder anderswie codierten Problemen, auf welche zugegriffen werden kann. (Siehe Abbildung \ref{fig:fallbasiert})

Dieses System beruht auf der Annahme, dass geringe Änderungen an einer Problemstellung auch nur geringe Änderungen am Lösungsweg zur Konsequenz habenn (also \textbf{lineare Probleme}). Somit sind fallbasierte Systeme \textit{nicht} geeignet, um komplexe Probleme zu lösen.

\newpage

\subsubsection{Entscheidungsbaumverfahren (Buch S136)}
Ein Entscheidungsbaum ist die \textbf{grafische Darstellung einer Datenteilung} (z.B. mittels Clustering). Dabei stellen die nicht-Blatt Knoten des Baumes die Merkmale der Daten dar, nach denen der Datensatz geteilt wird. Über die Kanten werden diese Merkmale verbunden. Die Teildatenbestände (z.B. Cluster oder Klassen) werden schlussendlich in den Blättern dargestellt.

\vspace{10px}

\noindent Ein solcher Baum wird mittels des \textbf{Entscheidu9ngsbaumverfahrens}. Zuerst wird ein Merkmal bestimmt, nach welchem sortiert werden soll (in Abbildung \ref{fig:tree} wäre dies z.B. ob \\ 
$Schalengewicht <= 0.168$ und  $MSE = 10.273$ ist) und es wird danach sortiert. Dieser Vorgang wird nun wiederholt (dieses mal ist z.B. das Merkmal links $Schalengewicht <=0.059$ und $MSE = 4.635$ ist).

Der Vorgang wird nun solange wiederholt, bis alle Datensätze zu einem Cluster gehören, oder sich kein Merkmal mehr finden lässt, durch welches geteilt werden kann.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=13\baselineskip]{tree.png}
	\caption{Beispiel eines Entscheidungsbaumes}
	\label{fig:tree}
\end{figure}

Um Bäume zu verbessern, die eine hohe Fehlerquote aufweisen, kann \textbf{Pruning} verwendet werden. Dabei werden einzelne Knoten und Kanten des Baumes entfernt, um den Baum zu verkleinern und somit die Teilgenauigkeit des Baumes insgesamt zu erhöhen.

\vspace{10px}

\noindent Entscheidungsbäume werden zum Beispiel zur \textbf{Klassifizierung} von Daten hinzugezogen werden, um die Klassen einfacher und übersichtlicher darstellen zu können.

\newpage

\subsection{Architekturen}
\subsubsection{Keine physische Integration}
Strukturierte und unstrukturierte Daten existieren \textbf{parallel} in verschiedenen Speicherungsmöglichkeiten. Diese Daten werden nur verknüpft, falls beide für eine Auswertung benötigt werden.

\subsubsection{Pysische aber nicht logische Integration}
Strukturierte und unstrukturierte Daten werden zwar in der gleichen Speicherungsmöglichkeit abgelegt, sind jedoch nicht verknüpft.

Unstrukturierte Daten werden oft als BLOB (Binary Large Object) gespeichert. BLOBS sind prinzipiell einfach riesige Binär-Dateien, in welche alle unstrukturierten Daten (Blogposts, Bewertungen, YouTube Videos etc.) reingeschmissen werden.

In relationalen Datenbanken werden BLOBS oft extern gespeichert. Das heisst, die Datenbank legt nur eine Referenz ab, wo das BLOB gefunden werden kann, sollte es gebraucht werden.

Je nach dem welcher DB-Engine verwendet wird, muss eine BLOB-Spalte in einer DB-Tabelle als \textit{BLOB, LONGBLOB, LONGRAW} oder auch \textit{BYTEA} definiert werden.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{blob}
	\caption{Beispiel von BLOBs und strukturierten Daten in der selben DB}
	\label{fig:blob}
\end{figure}

\newpage

\subsubsection{Physische und logische Integration}
\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{unstruktdata.PNG}
	\caption{Die drei verschiedenen Methoden zur Integration von unstrukturierten Daten.}
	\label{fig:logInt}
\end{figure}

\noindent {\color{cyan}\textbf{Extraktion von unstrukturierten Daten}} geschieht meist mittles NPL und UIMA (Unstructured Information Management Architecture). Dabei muss darauf geachtet werden, dass zwar alle unnützen Formatierungen entfernt werden, so dass die Daten gesäubert und integriert werden können, jedoch die wichtigen Formatierngen beibehalten werden (wie z.B. HTML-tags für title o.ä)

\vspace{10px}

\noindent {\color{cyan}\textbf{Transformation von unstrukturierten Daten}} hat fast noch eine grössere Bedeutung als die Bereinigung von strukturierten Daten. Der Prozess besteht hauptsächlich daraus, Synonyme, Homonyme und Ähnlichkeit zu erkennen, irrelevante Daten wie Füllwörter oder Formatierungszeichen zu entfernen und/oder Daten in andere Sprachen zu übersetzen.

Für diesen Task werden Werkzeuge wie z.b. der DataWrangler, Open Refine oder DataCleaner verwendet, oder aber auch NLP-Werkzeuge wie GATE, UIMA oder Stanford's Core NLP Suite.
\vspace{10px}

\noindent {\color{cyan}\textbf{Laden von unstrukturierten Daten}}
\begin{description}
	\item[a) / Basisdatenbank] Die Daten werden bereits im Datenbeschaffungsprozess analysiert, aufbereitet und strukturiert bevor sie in der DB abgelegt werden. 
	
	Eine Verwendung der unstrukturierten Daten ist nach Ablegung nicht mehr möglich.
	\item[b) / Data Warehouse] Die Daten werden möglichst ohne Informationsverlust in der Basis-DB/ODS abgelegt und erst anschliessend im Data-Warehouse strukturiert.
	\item[c) / Hybrid-Architektur: ] Die strukturierten und unstrukturierten Daten werden parallel in der Basis-DB abgelegt.
\end{description}

\newpage

\subsubsection{Beispiel}
\textbf{Aufgabenstellung: } Folgende E-Mail soll in ein Data-Warehouse integriert werden.

\begin{lstlisting}
Subject: Beschwerde
Von: peer.haber@bluewin.ch
Gesendet: Di. 18.10.2016 15:27
An: Customer Service
------------------------------------------------
Geehrtes Support Team
leider bin ich mit dem Vertrag vom 21.10.2016 nicht zufrieden. sie haben mir 
am Telefon erklaert, dass im Vertrag 5GB Daten inbegriffen sind. 
Leider ist dem nicht so. Bitte kontaktieren Sie mich unter folgender Nummer:

+41 79 678 23 63

mit freundlichen Gruessen
Peer Haber
peer.haber@bluewin.ch

\end{lstlisting}

Die Mail soll in die Architektur 3a) integriert werden. Dies geschieht in vier Schritten:

\begin{enumerate}
	\item Extraktion der Daten aus den jeweiligen Ursprungsformaten $\rightarrow$ Extraktion
	\item Analyse der Daten $\rightarrow$ Transformation
	\item Datenbereinigung $\rightarrow$ Transformation
	\item Laden der Daten in ein Datenbanksystem des Wata-Warehosues $\rightarrow$ Laden
\end{enumerate}

\paragraph{Schritt 1 - Extraktion} \mbox{}\\
Der Text des E-Mails wird aus dem Ursprungsformat extrahiert:

\begin{lstlisting}
Geehrtes Support Team
leider bin ich mit dem Vertrag vom 21.10.2016 nicht zufrieden. sie haben mir 
am Telefon erklaert, dass im Vertrag 5GB Daten inbegriffen sind. 
Leider ist dem nicht so. Bitte kontaktieren Sie mich unter folgender Nummer:

+41 79 678 23 63

mit freundlichen Gruessen
Peer Haber
peer.haber@bluewin.ch
\end{lstlisting}

\newpage

\paragraph{Schritt 2 - Analyse der Daten} \mbox{} \\
Der extrahierte Text wird mittels Data Mining oder NLP analysiert:
\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{NLP_email.PNG}
	\caption{E-Mail-Analyse mit dem Text-Mining Tool UIMA}
	\label{fig:nlpemail}
\end{figure}

Die \textit{semantische Auswertung} der oben gezeigten E-Mail zeigt folgendes:

\begin{description}
	\item[Kategorisierung: ] Kategorie: Beschwerde
	\item[Auswertung Freitexteingabe: ] Unzufriedener Kunde.
	\item[Auswertung Dokument: ] Analyse des Vertrages, ob Kunde recht hat.
\end{description}

\paragraph{Schritt 3 - Datenbereinigung}
\begin{itemize}
	\item Erkennen von Synonymen und Homonymen
	\item Beseitigung von irrelevanten Daten (z.B. Füllwörtern)
	\item Evtl. Übersetzung in andere Sprachen.
\end{itemize}

\noindent \sout{Geehrtes} \textbf{Support Team},

\noindent \sout{leider bin ich mit dem} \textbf{Vertrag vom 21.10.2016 nicht zufrieden}. \sout{Sie
haben mir am} \textbf{Telefon}\sout{ erklärt, dass im} \textbf{ Vertrag 5 GB Data} \sout{inbegriffen
sind. Leider ist dem nicht so. Bitte kontaktieren Sie mich unter folgender Nummer:}

\noindent \textbf{+41 79 678 23 63}

\noindent \sout{mit freundlichen Grüssen}

\noindent \textbf{Peer Haber}

\noindent \textbf{peer.haber@bluewin.ch}

\paragraph{Schritt 4 - laden der Daten in ein Datenbanksystem des Data-Warehouses}\mbox{} \\

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=3.5\baselineskip]{haber_peer.PNG}
\end{figure}

\section{Informations- und Datenqualität}
\subsection{Definition}
\blockquote[DIN 55350/1995]{Qualität ist die Gesamtheit von Eigenschaften und Merkmalen eines Produktes oder einer Tätigkeit, die sich auf deren Eignung zur Erfüllung festgelegter oder vorausgesetzter Erfordernisse beziehen.}
\vspace{10px}

\noindent Eine weitere Definition nach Cordts 2009 lautet: \textit{Die Datenqualität ist der Grad der Eignung von Daten zur Erfüllung eines bestimmten Verwendungszweck.}

Solche Erfordernisse sind zum Beispiel:
\begin{itemize}
	\item Korrektheit
	\item Vollständigkeit
	\item Relevanz
	\item Konsistenz
	\item Aktualität
\end{itemize}

\subsection{Ursachen und Orte von Qualitätsmängel}
Schätzungsweise 10-20\% aller Daten in einem operativen System sind fehlerhaft. Diese Daten zu verbessern nimmt ca. 80\% des Aufwands im ETL-Prozess in Anspruch.

Schlechte Datenqualität kann folgende Ursachen haben:

\begin{description}
	\item[Schlechte Datenerfassung: ] Durch menschliches Versagen in der Dateneingabe/Datenerfassung; Falsch (oder gar nicht) geschulter Mitarbeiter.
	\item[Schlechte Prozesse: ] Durch mangelhaft definierte Abläufe und Zuständigkeiten
	\item[Schlechte Architektur: ] Durch unsaubere/überhastete Systemwechsel/Migrationen oder infolge von Sparmassnahmen am falschen Ort.
	\item[Schlechte Definitionen: ] Durch mangelhafte/fehlende Planung oder Sorgfalt. Durch mangelhafte/fehlende Doku.
\end{description}

\begin{figure}[htb!]
	\centering
	\includegraphics[keepaspectratio=true,height=13\baselineskip]{data_quality.PNG}
	\caption{Beispiel von schlechter Datenqualität}
\end{figure}

\newpage

\subsection{Messbarkeit von Datenqualität}
Momentan gibt es noch keine akzeptierte Metrik für die Beurteilung der Datenqualität. Allerdings gibt es Grafiken zur Visualisierung des "sweet spot" zwischen überhöhten Kosten durch schlechte Datenquaität und überhöhten Kosten durch zu hohe Investitionen in die Verbesserung der Datenqualität (Abb. \ref{fig:costQuality}). 

\begin{figure}[htb!]
\centering
\begin{minipage}{.45\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{messungDatenQualitaet.PNG}
	\captionof{figure}{"Messung" der Datenqualität}
	\label{fig:measureQuality}
\end{minipage}
\begin{minipage}{.45\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{kostenNutzung.PNG}
	\captionof{figure}{Kosten-Qualitäts Gegenüberstellung}
	\label{fig:costQuality}
\end{minipage}
\end{figure}

\begin{wrapfigure}[17]{R}{0.6\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{dgiq.PNG}
\end{wrapfigure}

\noindent Zudem hat die DGIQ (Deutsche Gesellschaft für Informations- und Datenqualität) 15 Dimensionen der Datenqualität definiert. Diese sind in vier Kategorien eingeteilt. Diese Einteilung basiert auf einer Studie des MITs.

\color{system}{
\begin{description}
	\item[Zugänglichkeit: ]  Informationen sind  \\ zugänglich, wenn sie anhand einfacher Verfahren auf direktem Weg für den Anwender abrufbar sind. 
	\item[Bearbeitbar: ]  Informationen sind leicht bearbeitbar, wenn sie leicht zu ändern/für unterschiedliche Zwecke zu verwenden sind. 
\end{description}
}

\color{inhalt}{
\begin{description}
	\item[Hohes Ansehen: ]  Informationen sind hoch angesehen, wenn die Informationsquelle, das Transportmedium und das verarbeitenden System im Ruf einer hohen Vertrauenswürdigkeit und Kompetenz stehen.
	\item[Fehlerfrei: ] Informationen sind fehlerfrei, wenn sie mit der Realität übereinstimmen 
	\item[Objektiv: ] Informationen sind objektiv, wenn sie streng sachlich und wertfrei sind.
	\item[Glaubwürdig: ] Informationen sind glaubwürdig, wenn Zertifikate einen hohen Qualitätsstandard ausweisen oder die Informationsgewinnung und -verbreitung mit hohem Aufwand betrieben werden.
\end{description}
}

\color{darstellung}{
\begin{description}
	\item[Eindeutig auslegbar: ]  Informationen sind eindeutig auslegbar, wenn sie in gleicher, fachlich korrekter Art und Weise begriffen werden.
	\item[Einheitlich dargestellt: ] Informationen sind einheitlich dargestellt, wenn die Informationen fortlaufend auf dieselbe Art und Weise abgebildet werden.
	\item[Übersichtlich]  Informationen sind übersichtlich, wenn genau die benötigten Informationen in einem passenden und leicht fassbaren Format dargestellt sind.
	\item[Verständlich] Informationen sind verständlich, wenn sie unmittelbar von den Anwendern verstanden und für deren Zwecke eingesetzt werden können
\end{description}
}

\color{nutzung}{
\begin{description}
	\item[Relevant: ]  Informationen sind relevant, wenn sie für den Anwender notwendige Informationen liefern.
	\item[Angemessener Umfang: ] Informationen sind von angemessenem Umfang, wenn die Menge der verfügbaren Information den gestellten Anforderungen genügt.
	\item[Vollständig: ]  Informationen sind vollständig, wenn sie nicht fehlen und zu den festgelegten Zeitpunkten in den jeweiligen Prozess-Schritten zur Verfügung stehen.
	\item[Wertschöpfend: ] Informationen sind wertschöpfend, wenn ihre Nutzung zu einer quantifizierbaren Steigerung einer monetären Zielfunktion führen kann.
	\item[Aktuell: ] Informationen sind aktuell, wenn sie die tatsächliche Eigenschaft des beschriebenen Objektes zeitnah abbilden
\end{description}
}

\newpage

\color{black}
\subsection{Verbesserung der Datenqualität}

\begin{wrapfigure}[17]{R}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{dataImprovement.PNG}
	\caption{Vorgehensmodell zur Datenqualitätsverbesserung}
\end{wrapfigure}

Die Verbesserung der Datenqualität kann in drei Phasen aufgeteilt werden, die wiederum in mehrere Tasks unterteilt sind.

\begin{enumerate}
	\item Daten verstehen
		\subitem Data Identification
		\subitem Data Profiling
	\item Daten verbessern
		\subitem Data Standardisation
		\subitem Data Enrichment
		\subitem Data Deduplication
	\item Daten Steuern
		\subitem Data Monitoring
\end{enumerate}

\subsubsection{Daten verstehen}
Was für Daten liegen vor?, Welche Daten kann man benutzen? Wo liegen diese Daten? Wer hat die Rechte auf diese Daten zuzugreifen? Wer kann die Daten für die weitere Verwendung "freigeben"? Was bedeuten diese Daten? In welchem Kontext stehen diese Daten?

\paragraph{Data Identification}\mbox{}\\
\textit{Woher kommen unsere Daten?} Durch die Auswertung der Metadaten kann einiges ermittelt werden, wie zum Beispiel die Art des Quellsystems.

\begin{itemize}
	\item Wie und von wem wurden die Daten erstellt?
	\item Welche Benutzer greifen am häufigsten zu?
	\item Für welche Zwecke werden die Daten vor allem eingesetzt?
	\item In welchem Zustand befinden sich die Daten?	
\end{itemize}

Bei der Data Identification werden die benötigten Daten identifiziert (wie das der Name schon sagt), ihr Ist-Zustand beschrieben und spezifiziert. 

Das Ziel der Data Identification ist es, schlussendlich eine Metadatensammlung vorliegen zu haben und anhand dessen erste Angaben zur Datenqualität machen zu können.

\newpage

\paragraph{Data Profiling} \mbox{}\\
Nachdem im vorherigen Schritt der Ist-Zustand der Daten beschrieben wurde, wird im Data-Profiing der Soll-Zustand beschrieben und spezifiziert. Das heisst, es werden Kriterien definiert wie zum Beispiel

\begin{itemize}
	\item Benennungen
	\item Datentypen
	\item Domänen
	\item Minima und Maxima
	\item Kardinalitäten (Beziehungen zwischen Tabellen)
	\item Muster
	\item Fehlertoleranz
	\item Geschäftsregeln
\end{itemize}

Ausserdem werden Beziehungen von Daten untereinander festgehalten. 

Jede Abweichung des Ist-Zustands vom definierten Soll-Zustand ist ein Qualitätsmangel. Somit kann genauer eingeschätzt werden, in welchem Gesamtzustand sich die Daten befinden. Dies wiederum ermöglicht es, Fehler zu ermitteln, priorisieren und anschliessend die Ursache zu beheben.

\vspace{10px}

\noindent Es gibt grundsätzlich drei verschiedene Arten des Data-Profiling:

\begin{description}
\item[deskriptiv/beschreibend: ] Analyse von Häufigkeiten, Abhängigkeitsanalysen, Ausreisser-Tests etc.
\item[kognitiv/lernend: ] Regelinduktionen, Segmentierungen oder Klassifizierungen
\item[deduktuv/ableitend: ] (Geschäfts-)Regelanalyse
\end{description}

Eine oder mehrere dieser Data-Profiling Methoden wird auf sämtliche Datenobjekte des Systems sowie deren Beziehungen untereinander angewandt. Man untersucht die Daten einer Spalte auf die oben genannten Kriterien und kann somit Rückschlüsse auf die Datenqualität ziehen ($\rightarrow$ Column Profiling)

Ebenfalls untersucht man die Abhängigkeiten von mehreren Spalten in einer Tabelle und kann somit z.B. überprüfen, in welcher Normalform sie sich befinden oder wie viele Fremdschlüssel in der Tabelle vorhanden sind ($\rightarrow$ Multi Column Profiling) 

\vspace{10px}

\noindent Das Ziel des Data-Profilings ist es, ein sogenanntes Soll-Metadatenreqpositorium vorliegen zu haben. Das heisst soviel wie, es soll eine Spezifikation vorliegen, wie die Metadaten der Daten aussehen müssen, bevor sie ins Zielsystem integriert werden können. Ausserdem liegt nach dem Data Profiling normalerweise ein detaillierteres Verständnis der vorliegenden Daten vor, da man sich eingehend mit ihnen beschäftigt hat.

\newpage

\subsubsection{Daten verbessern}
Je gründlicher das Data Profiling durchgeführt wird, desto zuverlässiger und besser ist die Bereinigung. je besser de Bereinigung, desto höher die Datenqualität.

\paragraph{Data Standardisation}\mbox{}\\
Daten müssen vereinheitlicht werden. Vereinheitlichung heisst meist auch Homogenisierung. Daten müssen in ein einheitliches Format und eine einheitliche Semantik gebracht werden.

Data Standardisation beinhaltet vor allem folgende Themen:

\begin{description}
	\item[Vervollständigen: ] z.B. fehlende Vornamen nachtragen
	\item[Synonyme entfernen: ] Mehrere Schreibweisen eines Wortes mit der gleichen Bedeutung 
		\subitem Fa. vs. Firma
		\subitem unverheiratet vs. ledig
		\subitem Herr vs. Hr. vs. Mr.
		\subitem etc.
	\item[Homonyme entfernen: ] Eine Schreibweise eines Wortes mit mehreren Bedeutungen
		\subitem Peter (Vor- oder Nachname?)
		\subitem Kim (männlich oder weiblich?)
		\subitem Lieferung (Von hier oder zu hier?)
		\subitem etc...
	\item[Felder zusammennehmen: ] Strasse und Hausnummer gehören zusammen
	\item[Felder aufteilen: ] Vor- und Nachname erhalten je ein eigenes Feld
	\item[Rauschdaten eliminieren: ] Entweder Jahrgang \textbf{oder} Alter. Beides ist überflüssig
	\item[Rechtschreibung: ]  Straße / Strasse oder Brun-Brücke / Brunbrücke 
	\item[Einheitliche Darstellung: ] +41 79 vs. 0079 / 03.02.2018 vs. 2018-02-03
	\item[Eliminatin von Stoppwörtern: ] z.B. der, die, das, und, oder, doch
	\item[Einheitliche Abwandlungsformen: ]  Konjugation und Deklination
		\subitem \textbf{Konjugation: }z.B. Welche Personenform wird genommen? (Er hat am 03.02 Geburtstag vs. Ich habe am 03.02 Geburtstag)
		\subitem \textbf{Deklination: }Welche Fälle werden verwendet? (die schönen Häuser vs. der schönen Häuser)
	\item[Einheitliche Einheiten: ] Nur metrisch oder nur imperial
\end{description}\footnote{Stoppwörter nennt man Wörter, die bei einer Volltextindizierung nicht beachtet werden, da sie sehr häufig auftreten und keine Relevanz für die Erfassung der Daten besitzen.}

Data Standardisation wird häufig mittels linguistischen und algorithmischen Verfahren durchgeführt. Dabei werden Referenzdaten gesammelt, die anschliessend gepflegt werden müssen, dass sie später wieder angewendet werden können.

Das Zeil der Data Standardisation ist es, Daten in einer strukturell und inhaltlich direkt vergleichbaren Einheit vorliegen zu haben.

\paragraph{Data Enrichment}\mbox{}\\
Nachdem in der Data Standardisation unnötige Informationen entfernt worden sind, werden im Schritt des Data Enrichment zusätzliche, nützliche Informationen hinzugefügt, die zukünftige Analysen vereinfachen sollen. Dies können z.B. sein:

\begin{description}
	\item[Demographische Daten: ] Nationalität, Herkunft, Schulbildung, Einkommen etc.
	\item[Geographische Daten: ] Land, Provinz, Bevölkerungsidchte etc. (Geodaten helfen oft bei der Deduplizierung von Daten)
	\item[Produktcode: ] EAN / EPC / ISBN etc. von Produkten
	\item[Kundeninformationen: ] Bestell- und Zahlungsverhalten, Interessen etc.
	\item[Tags: ] z.B. "Spätzahler", "Tierliebhaber", "Technologiebegeistert"
	\item[Umsatz: ] Umsatz, der der Kunde generiert.
	\item[Sumindex: ] Summe aller Bestellungen, die der Kunde im Laufe einer Zeitspanne aufgegeben hat.
\end{description}

Das Ziel des Data Enrichments ist es, Daten in so einem Format vorliegen zu haben, die die Suche nach Dubletten vereinfacht. Ebenfalls können mit diesen angereicherten Daten bessere Analysen durchgeführt werden. Data Enrichment hat überhaupt keinen Einfluss auf die Datenqualität, jedoch kann die Datenmenge erheblich ansteigen.

\paragraph{Data Deduplication}\mbox{}\\
Dubletten oder auch Duplikate sind Datensätze, die zwar syntaktisch unterschiedlich sind, semantisch jedoch dasselbe meinen. Es wird zwischen zwei Arten von Dubletten unterschieden: Den \textit{vollständig identischen} und den \textit{sich bis zu einem gewissen Grad ähnlichen} Dubletten.

Dubletten sind redundant und verstossen somit gegen die Information Quality Dimensionen. Ursachen für Dubletten können z.B. ein Fehler bei der Data Standardisation sein (Grösse sowohl in inch wie auch in cm angegeben), oder auch nicht aktuelle Daten (Kunde zweimal erfasst nachdem er umgezogen ist, einmal mit der neuen und einmal mit der alten Adresse).

Es gibt jedoch auch Fälle, da sind Dubletten gewollt. Das kann z.B. sein bei einer Mehrfachablage für eine Verlosung, oder aber auch bei Gratis Trials von Online-Diensten (ich bin einmal Peter Müller und nach dem Ablauf des Gratis Trials bin ich plötzlich Sandro Hüsler, aber eigentlich immer noch die selbe Person)

Dubletten können Anomalien und Fehlern in Analysen und Prognosen auslösen oder können in grosser Anzahl auch zu einem Ressourcenproblem führen. Deshalb sollten sie so gut wie möglich beseitigt werden. Diese Beseitigung findet in zwei Teilschritten statt:

\begin{enumerate}
	\item Dublettenerkennung Dies wird meist mittels Clustering durchgeführt. Die gefundenen möglichen Dubletten werden aufgelistet und auf Redundanz untersucht.
	\item Eliminierung und/oder Merging von Dubletten (Siehe Abb. \ref{fig:dubl})
\end{enumerate}

\begin{figure}[htb!]
	\centering
	\includegraphics[keepaspectratio=true,height=14\baselineskip]{DublettenBeseitigung.jpg}
	\caption{Prozess der Dublettenbeseitigung als iterativer Prozess}
	\label{fig:dubl}
\end{figure}

%TODO: Verschiedene Arten der Dublettenerkennung (LS06)

\newpage

\subsubsection{Daten steuern}
Da sich Daten immer wieder ändern, können Sie sich auch ungewollt verschlechtern. Ein System- und/oder Datenqualitätsowner muss da dauernd ein Auge darauf haben.

\paragraph{Data Monitoring}\mbox{}\\
Wie bei den letzten paar Themen klar geworden sein sollte, ist die Datenqualitätspflege ein iterativer Prozess. Das Monitoring begleitet diesen Prozess und dokumentiert und misst diesen.

Aufgaben des Data Monitorings sind z.B. 
\begin{itemize}
	\item Messen und Steuern der Datenqualitätskennzahlen ($\rightarrow$ Data Auditing)
	\item Alarmierung bei Regelverletzung
	\item Erkennen von (Daten-)Trends
\end{itemize}

Das Ziel des Data Monitorings ist es, ein Höchstmass von Wissen und Bewusstsein für den momentanen Stand der Datenqualität zu haben.

\newpage

\section{Historisierung}
Unter der Historisierung versteht man, dass man nicht nur aktuelle Datenbestände verwaltet, sondern auch veraltete Daten. Wenn also z.B. ein Mitarbeiter eine Lohnerhöhung erhält, so wird der alte Lohn nicht überschrieben, sondern beide Einträge existieren parallel.

Es existieren verschiedene Methoden zur Historisierung. In diesem Modul wird hauptsächlich die SCD-Versionierung behandelt. SCD steht für Slowly Changing Dimensions. Es gibt die SCD-Typen 0, 1, 2 und 3. Wir werden uns ebenfalls kurz mit der Tupelversionierung auseinandersetzen.

\subsection{SCD Typ 0 - Keine Historisierung, keine Aktualisierung}
Es wird \textbf{keine Historisierung} durchgeführt. Es wird überprüft, ob unter diesem Primärschlüssel bereits ein Datensatz existiert. Falls dem so ist, \textbf{wird nichts gemacht}. Ansonsten wird der neue Datensatz in die Datenbank eingefügt.

\vspace{10px}

\noindent Es werden also nur neue Daten hinzugefügt, bereits existierende Datensätze werden jedoch \textbf{nicht} verändert.

\subsection{SCD Typ I - Keine Historisierung, Aktualisierung}
Es wird ebenfalls\textbf{ keine Historisierung} durchgeführt. Im Gegensatz zu SCD Typ 0 sind Aktualisierungen jedoch möglich. Erhält ein Mitarbeiter also eine Lohnerhöhung von CHF 5'000 und sein Lohn steigt von CHF 10'000 auf CHF 15'000, dann wird der alte Lohn in der Datenbank mit dem neuen Lohn überschrieben. Es gibt kein Zeichen mehr davon, dass der Mitarbeiter jemals nicht CHF 15'000 verdient hat.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=4\baselineskip]{scd_type1.png}
	\caption{Beispiel einer Aktualisierung.}
	\label{fig:scd1}
\end{figure}

Es muss jedoch darauf geachtet werden, dass z.B. persistente Datenaggregate (z.B. Views), die mit dem alten Datensatz zusammenhängen nun evtl. nicht mehr korrekt sind und somit neu berechnet werden müssen.

\subsection{SCD Typ II - Vollhistorisierung}
Diese Methode findet am meisten Anwendung. Es wird \textbf{nichts überschrieben}. Stattdessen werden zwei Extra-Spalten \code{dat-von} und \code{dat-bis} unterhalten, in denen festgehalten wird, in welchem Zeitraum dieser Datensatz gültig ist.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=5\baselineskip]{scd_type2.png}
	\caption{Beispiel einer Vollhistorisierung.}
	\label{fig:scd2}
\end{figure}

\noindent Im Gegensatz zu SCD Typ 1 sind persistente Datenaggregate immer noch korrekt, da der alte Datensatz ja noch existiert. Sofern sie mit den korrekten Versionen verbunden sind zumindest.


\subsection{SCD Typ III - Historisierung mit neuem Attribut (Spalte)}
Bei jeder Aktualisierung eines Datensatzes wird der gesamten Tabelle eine neue Spalte angehängt, in welche der alte Wert geschrieben wird.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=4.5\baselineskip]{scd_type3.png}
	\caption{Beispiel einer Historisierung mit neuer Spalte.}
	\label{fig:scd3}
\end{figure}

Bei dieser Art der Historisierung wird jedoch meistens nur der letzte alte Wert gespeichert. Sollte die Acme Aupply Co nun also wieder in einen neuen Staat ziehen, so wird CA mit IL überschrieben und IL wird mit dem aktuellen Staat überschrieben. Somit ist diese Art von Historisierung eigentlich nur bei solchen Daten zu empfehlen, bei welchen eine einmalige Änderung zu erwarten ist.

\subsection{Tupelversionierung}
Bei der Tupelversionierung werden die Tupel nicht mit einem Zeitstempel, wie in SCD Typ II, sondern mit einer Versionierungsnummer versehen. Es werden keine bestehenden Tupel aktualisiert, sondern nur neue Tupel erzeugt. Das Tupel mit der höchsten Versionierungsnummer ist jeweils das aktuelle gültige.

\begin{figure}[htb!]
	\centering
	\includegraphics[keepaspectratio=true,height=17\baselineskip]{tupelversionierung.PNG}
	\caption{Tupelversionierung}
	\label{fig:tupel}
\end{figure}

\newpage

\section{Multidimensionale Datenmodellierung}
\subsection{Nutzen von MDDB (Script S153 / Buch S201)}
\begin{wrapfigure}[18]{R}{0.65\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{erd.JPG}
	\caption{Ein solches ERD ist nicht mehr übersichtlich und somit nutzlos}
	\label{fig:erd}
\end{wrapfigure}
Ab einer gewissen Komplexität sind Entity-Relationship Diagramme schlicht und einfach nicht mehr brauchbar (z.B. in Abb. \ref{fig:erd}).

\vspace{10px}

\noindent Ebenfalls benötigt man ab einer gewissen Komplexität eine unschön grosse Anzahl von Joins, um anständige Datenaggregate zu kriegen, was bald auch zu Performanceproblemen führt. 

\vspace{10px}

\noindent Die Lösung der beiden oben genannten Probleme heisst \textbf{multidimensionale Datenbanken}. Anstelle von einer komplizierten zweidimensionalen Darstellung der Daten nehmen wir einfach eine Dimension hinzu .

\vspace{40px}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=12.5\baselineskip]{mddbs.png}
	\caption{Vergleich verschiedener Datenbank-Dimensionen}
	\label{fig:mddbs}
\end{figure}

In Abbildung \ref{fig:mddbs} kann eine Gegenüberstellung der verschiedenen Datenbank-Dimensionen gesehen werden. Jede Dimension fügt ein zusätzlicher Wert hinzu. So wird in der 1-dimensionalen Datenbank nur das Kurszentrum gespeichert, während in der 2-dimensionalen Datenbank zusätzlich noch die Anzahl Kursteilnehmer pro Kurs hinterlegt wird. Die 3-dimensionale Datenbank schichtet nun mehrere 2-dimensionale Datenbank-"scheiben" aufeinander in eine Würfelform. Jede dieser "Scheiben" symbolisiert ein Monat.

Anstelle, dass man nun ein ellenlanges Join-Statement von allen möglichen Tabellen machen muss, wenn man herausfinden will, wie viele Teilnehmer der Kurs K2 im Center C4 im Februar hatte, muss man nur die korrekten Koordinaten wissen (3/3/1).

\newpage

\noindent Eine MDDB (Multi-Dimensional Data Base) hat viele Vorteile. Einige davon sind:

\begin{itemize}
	\item Intuitive Benutzeroberfläche
	\item Kurze und stabile Antwortzeiten
	\item Hohe Zugriffs- Analyse- und Daten Präsentationsflexibilität
	\item Interaktives Navigieren in den Datenbeständen
	\item Schnelle Erstellung von geeigneten Ad-hoc-Berichten und Grafiken
	\item Kein technisches Datenbank-Wissen vorausgesetzt
	\item Vereinfachte Datenmustererkennung
	\item Vereinfachte Erkennung von schlecht bereinigten Daten aufgrund von unplausiblen Analysedaten
\end{itemize}

\subsection{Kennzahlen, Fakten und Dimensionen}

\noindent \textbf{OLAP-Systeme} werden verwendet, um anhand von Kennzahlen Entscheidungen zu treffen (Siehe Kap. 1.3-1.5). Das heisst, in den Datenbanken werden \textbf{Kennzahlen} gespeichert. 

\vspace{10px}

\noindent Eine solche \textbf{Kennzahl} symbolisiert eine \textbf{Eigenschaft}. Das mag z.B. ein Umsatz, ein Preis oder auch ein monatlicher Stromverbrauch sein.

Ein anderer Begriff für Kennzahlen sind \textbf{Fakten}. Mehrere Fakten können zu einem neuen Fakt aggregiert werden (z.B. Kennzahl 330 und Kennzahl 54 können aggregiert werden zum Fakt, dass das Produkt mit der PID 54 CHF330.- kostet.)

Zusätzlich wird der \textbf{Geschäftsaspekt}, auch \textbf{Dimension} genannt hinzugenommen. Er beschreibt den \textbf{Beobachtungspunkt oder -raum} der Kennzahl/des Fakts. So ein Dimension kann z.B. eine Filiale, eine Abteilung oder auch ein Quartal sein. Dimensionen sind sogenannte \textbf{deskriptiven Werte}, das heisst sie enthalten \textbf{keine messbaren Werte}, sondern geben den vorhandenen Kennzahlen lediglich einen \textbf{Kontext}.

Dabei liefern gröbere Dimensionen eine gröbere Granularität des Faktes (Dimension HSLU vs. Dimension Student $\rightarrow$ Fakt Verwaltungskosten Universität vs. Verwaltungskosten Student).

Es ist nicht immer einfach, Fakten und Dimensionen auseinanderzuhalten. So kann ein Preis sowohl Fakt, wie auch Dimension sein. Wenn man die Entwicklung des Preises im Verlaufe der Zeit beobachtet, dann ist der Preis ein Fakt. Wenn man nun aber die Anzahl von verkauften Produkten in verschiedenen Preissegmenten beobachtet, dann ist der Preis die Dimension.

\vspace{10px}

\noindent Grundsätzlich kann man sagen \textit{Eine Analyse basiert primär auf der Interpretation der Ausprägungen eines Fakts in mehreren Dimensionen.}

\newpage

\subsection{OLAP Würfel-Paradigma}
Je mehr Dimensionen man hat, desto feiner wird die Granularität der Analyse. 

\vspace{10px}

\noindent Um zurückzukommen auf Abb. \ref{fig:mddbs}: In der 1-dimensionalen Datenbank kann eine 1-dimensionale Analyse durchgeführt werden: \\

\centerline{\textit{Wie viele Kursteilnehmer hat das Kurszentrum C3}.} 

\vspace{10px}

\noindent \textbf{Dimension: } Kurszentren \\
\noindent \textbf{Fakten: } Kursteilnehmen

\vspace{10px}

\noindent Nehmen wir eine Dimension hinzu, so kann in der 2-dimensionalen Datenbank bereits eine 2-dimensionale Analyse durchgeführt werden. Zusätzlich kommt nun noch die Kurs-Dimension hinzu: \\

\centerline{\textit{Wie viele Kursteilnehmer hat der Kurs K2 im Kurszentrum C3}}

\vspace{10px}

\noindent \textbf{Dimension: } Kurszentren, Kurs \\
\noindent \textbf{Fakten: } Kursteilnehmen

\vspace{10px}

\noindent Nehmen wir nun zusätzlich noch die dritte Dimension hinzu, so kann nun eine 3-dimensionale Analyse durchgeführt werden, die wiederum feingranularer ist wie die 2-dimensionale Analyse: \\

\centerline{\textit{Wie viele Kursteilnehmer hat der Februar-Kurs K2 im Kurszentrum C3}}

\vspace{10px}

\noindent \textbf{Dimension: } Kurszentren, Kurs, Monat \\
\noindent \textbf{Fakten: } Kursteilnehmen

\vspace{10px}

\noindent Man kann nun noch beliebig viele Dimensionen hinzufügen, bis man die gewünschte Granularität erreicht hat (Wie viele weibliche Kursteilnehmer in der Altersgruppe 35-40, die im Bereich Gesundheitspflege tätig sind, hatte der Februar-Kurs K2 2016 im Kurszentrum C3?). Man ist nicht an den 'phyischen' 3-Dimensionalen Raum gebunden, sondern kann so viele Dimensionen erstellen, wie man wünscht, denn \textbf{der Würfel ist nur ein Modell und nirgends so materialisiert.} Das einzige Problem, das auftauchen könnte ist, dass es ein bisschen schwierig ist, ein 10-dimensionaler Würfel auf Papier darzustellen.

\subsubsection{Die 10 OLAP Würfel-Paradigmen}
\begin{enumerate}
	\item Sechs bis max. acht Dimensionen bleiben analystisch überschaubar und praktikabel
	\item Die verwendeten Dimensionen müssen unabhängig voneinander sein
	\item Die verwendeten Dimensionen können durchaus einer parallelen Dimensionshierarchie entnommen sein wie z.B. Print oder Broadcast
	\item Es muss nicht an jeder Dimensionskante gleich viele Elemente haben. In Abb. \ref{fig:mddbs} hat es z.B. 5 Kurse und 6 Monate.
	\item Die Elemente einer Dimension müssen immer die gleiche Granularität besitzen. Ihre Fakten sind also unmittelbar vergleichbar. Im Beispiel von Abb. \ref{fig:mddbs} sind es Monate als Zeitangabe und nicht z.B. Monate und Quartale gemischt.
	\item Unter Kombination mehrerer Dimensionen lassen sich mit dem gleichen Zahlenmaterial unterschiedliche Würfel herstellen. Jeder Würfel ist auf ein Analysemuster hin optimiert. Jede Analyse braucht ihren eigenen Würfel.
	\item Das Produkt (Projektion) aller Elemente aller Dimensionen ergibt die Anzahl der Fakten (oder NULL falls keine Daten vorhanden sind)
	\item Eine Zeitdimension hat es fast immer. Eine solche lässt zeitbezogene Entwicklungsanalysen zu.
	\item Die Fakten eines Würfels dokumentieren immer je nur eine relevante Kenngrösse wie z.B. den Umsatz oder den Gewinn
	\item Eine Dimension bringt Fakten immer in die gleiche Klasse von Beobachtungsräumen: Pro Monat, pro Quartal, pro Jahr, pro Abteilung etc.
\end{enumerate}

\subsubsection{Grundoperationen am Würfel}

\begin{description}
	\item[Slicing: ] Ausschneiden von Scheiben aus dem Würfel. Ändert nichts an der Granularität, macht aus einer 3-D Analyse eine 2-D Analyse
	\item[Dicing: ] Durch Einschränken von einer (oder mehreren) Dimensionen wird aus dem Würfel ein kleinerer erzeugt. (Z.B. aus dem Würfel in Abb. \ref{fig:mddbs} den Würfel der Kursteilnehmer von Kurs K2/K3 im Zentrum C2/C3 während Jan/Feb ausschneiden.)
	\item[Pivoting/Rotateion: ] Drehen des Würfels, so dass min. eine andere Dimension sichtbar ist
	\item[Drill-Down: ] Detaillierung eines Datenfeldes auf einzelne Werte (z.B. von Monaten auf einzelne Tage)
	\item[Drill-Up / Roll-Up: ] Gegenteil von Drill-Down; Verallgemeinerung eines Datenfeldes (z.B. von Monaten auf Quartalssicht)
	\item[Drill-Across: ] Verschiebung des Fokus auf derselben Dimensionsstufe (z.B. andere Region oder anderes Produkt)
	\item[Drill-Through: ] Ähnlich wie Drill-Across; Auswertung der gleichen Ansicht mit anderen Würfeln
	\item[Drill-In / Merge: ] Verringerung der Granularität durch Entfernen von eizelnen Dimensionen
	\item[Split: ] Gegenteil von Merge; Erhöhung der Granularität eines einzelnen Wertes durch Hinzufügen zusätzlicher Dimensionen (z.B. den Umsatz dieses Produktes in dieser exakten Filiale berechnen)
\end{description}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=8\baselineskip]{drill_across.jpg}
	\caption{Beispiel einer Dril-Across Operation}
	\label{fig:rel}
\end{figure}


\newpage

\subsection{Dimensionstruktur, -klassifizierung und -hierarchie}

Jede Dimension sollte eine Zusammenfassung mehrerer Elemente zu einem neuen, übergeordneten Dimensionswert ermöglichen ($\rightarrow$ roll-up). Das Gegenteil von roll-up ist drill-down. Diese beiden Funktionen ergeben einen hierarchischen 1:n Klassifikationspfad, wie zu sehen in Abb. \ref{fig:dimhier}.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{dimension_hierarchy.jpg}
	\caption{Klassfikiationshierarchie}
	\label{fig:dimhier}
\end{figure}

\subsubsection{Die 9 Regeln der Klassifikationshierarchie}

\begin{enumerate}
	\item Die Klassifikationsstruktur ist strukturiert
	\item Der Klassifikationsbaum muss vollständig sein. (Gehen wir davon aus, dass der Baum nach Radio und TV noch weitergeht)
	\item Die Bestimmung der Elemente und Zuordnung der Unterelemente ist schwer aber unerlässlich für künftige Analysen (Gehören Plakat-Aktionen zu Print oder Broadcast?)
	\item Die Bestimmung der Hierarchiestufen ist schwer.
	\item Höchstens 7 Hierarchiestufen mit je max. 15 - 20 Elementen um die Übersicht zu bewahren
	\item Es ist durchaus möglich, dass die gleichen Strukturelemente bis hin zur Wurzel parallel nach einer weiteren Sachlogik zusammengefasst werden. Im abgebildeten Beispiel analysieren wir nach dem Medium. Es könnte aber auch nach Zielgruppe zerlegt werden. So entsteht eine neue, parallele Klassifikationshierarchie.
	\item Dimensionen aus parallelen Klassifiationshierarchien gelten als unabhängig und können deshalb im Würfel eigene Kanten bilden.
	\item Die Hierarchie stellt uns oft vor die Frage wonach man detaillieren soll. Straftaten nach Alter oder nach Geschlecht oder nach Nationalität? Dies wird durch die geplanten Analysen bestimmt. Die Managementfragen sind zentral, bevor man mit dem Design der Dimensionsstruktur beginnt. Es kann dann halt sein, dass in mehreren Ästen eines Baumes gleiche Elemente vorkommen.
	\item Eine zusammengefasste Dimension ist eine neue Dimension. Aber die beiden Dimensionen sind nicht unabhängig, können also keine eigenen Würfel-Kanten sein. Parallele Dimensionen sind unabhängig.
\end{enumerate}

\subsection{Dünnbesetztheit}
In der Praxis sind längst nicht alle Würfel 'voll besetzt'. Diese fehlenden Daten werden auch als 'missing Data' bezeichnet. Es wird prinzipiell zwischen drei Arten von missing Data unterschieden:

\begin{description}
	\item[unmöglich] Es ist unmöglich, solche Daten zu erheben
	\item[unbekannt:] Die Daten sind einfach nicht bekannt.
	\item[(noch) nicht eingetreten: ] Das Event für diese Daten ist (noch) nicht eingetreten.
\end{description}

\noindent Zusätzlich wird zwischen der natürlichen und der logischen Dünnbesetztheit unterschieden.

\paragraph{Natürliche Dünnbesetztheit} beschreibt die Dünnbesetztheit aufgrund von nicht eingetretenen Daten (non-event) (Im Juli werden keine Skis verkauft). Jedoch hat man nicht NULL Skis verkauft, sondern 0. Mathematisch gesehen ist der Würfel also voll besetzt und es kann problemlos gerechnet werden.

\paragraph{Logische Dünnbesetztheit} beschreibt ebenfalls die Dünnbesetztheit von nicht eingetretenen Daten, jedoch nicht non-event nicht-Daten sondern event-not-applicable nicht-Daten. Nehmen wir an, ein Produkt ist im Feb.2018 auf den Markt gekommen. Es ist recht schwer, Verkaufsdaten für dieses Produkt im Nov. 16 zu finden. Diese Verkaufszahlen wären undefined oder NULL. Im Gegensatz zur natürlichen Dünnbesetztheit kann mit der logischen Dünnbesetztheit nicht korrekt gerechnet werden, da gewisse Anfragen aufgrund einer Unmöglichkeit zurückgewiesen werden und somit zu fehlenden oder verfälschten Analysen führen können.

Deshalb müssen undefinierte Felder in einem Würfel dementsprechend gekennzeichnet werden (entweder mit NULL oder n/a (not applicable)). Welcher Wert genau in undefinierte Datenfelder geschrieben werden soll, müsste in den Metadaten hinterlegt werden.

\vspace{10px}

\noindent Bei einem logisch dünnbesetzten Würfel kann die Anzahl der undefinierten und diejenige der definierten (auch 0) Zellen verglichen werden. Daraus erhält man den Füllgrad des Würfels, d.h. wie viel Prozent des Würfels enthält tatsächlich Daten. Bei einem sehr hohen Füllgrad spricht von von einem dicht besetzt (engl. dense) Würfel im Gegensatz zu einem dünn besetzten (sparse) Würfel.

\newpage

\subsection{Modelle in multidimensionallen Datenbanken}

In den bisher bekannten Datenbanken benötigt man zwei Modelle bzw. Schemata: Das \textbf{logische Modell} und das \textbf{physiche Modell}. Bei multidimensionalen Datenbanken kommt nun zusätzlich noch das \textbf{semantische Modell} dazu.

Die folgenden Modelle sind nach absteigendem Abstraktionslevel geordnet.

\subsubsection{semantisches Modell}
Der Datenwürfel mit seinen Dimensionen, Hierarchien und Fakten ist ein gutes semantisches Modell und hilft uns bei der Vorstellung von mehrdimensionalen Analysen. Der Datenwürfel ist eine möglichst wahrheitsgetreue Abbildung der Realität.

\subsubsection{logisches Modell}
Das logische Modell wird benötigt, um die Business-Logik (Requirements, interne Organisation, Prozesse etc.) darzustellen. Das Endprodukt eines logischen Modells ist normalerweise das \textbf{ERD} (Entity Relation Diagram). Das ERD zeigt die Beziehungen zwischen verschiedenen Kategorien von Daten und ist essenziell für das Datenbank-Design. 

\subsubsection{physisches Modell}
Das physische Modell baut anschliessend auf dem vorher erstellten logischen Modell auf und zeigt das tatsächliche Layout der Datenbank.

Nun werden Tabellen und Spalten mit Primär- und Fremdschlüssel erstellt nach den Vorgaben des logischen Modells. Im Gegensatz zum logischen Model ist das physische Model abhängig von der verwendeten Datenbank-Software.


\subsection{OLAP-Modelle in multidimensionalen Datenbanken}
Wie bereits in Kap. 1.5 besprochen wurde, wird OLAP (Online Analytical Processing) für die Analyse von Daten verwendet. Es wird zwischen vier Varianten des OLAP unterschieden:

\begin{description}
	\item[MOLAP:] Datenanalysen in multidimensionalen Systemen. MOLAP speichert Zahlen in Form von \textbf{Datenpunkten} und hat deshalb einen Performance-Vorteil gegenüber anderen OLAP-Systemen, die Daten als Datensätze speichern.
	\item[ROLAP: ] Datenanalysen in relationalen Systemen. Das System zieht Daten direkt aus bestehenden Datenbanken und benötigt so wesentlich weniger Speicherplatz.
	\item[HOLAP: ] Datenanalysen in hybriden Systemen. Hybrid für sowohl relational wie auch multidimensional.
	\item[DOLAP: ] Datenanlaysen in Desktop Systemen. Bei diesen Systemen nimmt nicht die tatsächliche Analyse die meiste Zeit in Anspruch, sondern das Erstellen und Auffrischen der dargestellten Daten-Würfel auf dem Desktop.
\end{description}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=21\baselineskip]{OLAP}
	\caption{Vor- und Nachteile der verschiedenen OLAP-Systemen}
	\label{fig:OLAP}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=11\baselineskip]{comp_OLAP.jpg}
	\caption{Vergleich der verschiedenen OLAP-Systeme}
	\label{fig:comp_OLAP}
\end{figure}


\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=12\baselineskip]{performance_vs_functionality.jpg}
	\caption{Abwägung der Performance vs. Funktionalität für die verschiedenen OLAP-Systeme}
	\label{fig:performance_functionality}
\end{figure}




\end{document}
