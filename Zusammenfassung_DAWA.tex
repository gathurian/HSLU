\documentclass[a4paper, 11pt]{article}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage[utf8]{inputenc}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{makecell}
\usepackage{tabularx}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{lscape}
\usepackage{afterpage}
\usepackage{geometry}

\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\newcommand{\shl}{~~\shortstack[l]}

\begin{document}
\title{Zusammenfassung Data Warehousing FS2018}
\author{Alex Neher}
\maketitle

\tableofcontents
\newpage

\graphicspath{{./Pictures/}}

\section{Die Notwendigkeit von Data Warehouses}
\subsection{Entscheidungsunterstützung (Skript S11)}
Data Warehouses sind keine neue Erfindung. Bereits in den 1960er Jahren wurden sogenannte \textbf{Managementsinformationssysteme} entwickelt. Diese MIS dienten dazu, Entscheidungsträgern alle benötigten Informationen zeitnah, fehlerfrei, flexibel, ergonomisch, effizient, effektiv und inspirativ zur Verfügung zu stellen. Diese Systeme treffen also nicht selbst Entscheidungen, sie \textbf{unterstützen} die Entscheidungsträger lediglich bei ihrer Entscheidung.

Es gibt vier Arten der Entscheidungsunterstützung:

\begin{description}
	\item [Modellbasiert:] z.B. Lineare Optimierung - Ein Mathematischer Ansatz basierend auf einem Modell $\Longrightarrow$ Abbildung der Realität
	\item [Wissensbasiert: ] z.B. Expertensysteme - Ansätze von Künstlicher Intelligenz
	\item [Datenbasiert: ] Basierend auf grossen Datenmengen $\Longrightarrow$ Data-Warehouse, OLAP oder Data-Mining
	\item [KI: ] Basierend auf Vorschlägen von Systemen, die Entscheidungen auf Basis von Daten und/oder gelernten Inhalten ($\longrightarrow$ Machine Learning)
\end{description}

\subsubsection{Expertensystem}
Ein Expertensystem (XPS oder ES) ist ein Computerprogramm, das Menschen bei der Lösung von komplexen Problemen wie ein menschlicher Experte unterstützen kann, indem es Handlungsempfehlungen aus einer Wissensbasis ableitet.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{expertensystem.png}
	\caption{Beispiel eines Expertensystems}
	\label{fig:xps}
\end{figure}

\newpage

\subsection{Ungenügen der "gängigen" Datenhaltung (Skript S13)}

\begin{itemize}
	\item Verschiedene Datenformate
	\item Verschiedene Werkzeuge
	\item Heterogenität der Daten
		\subitem Technisch (Mainframe / DBMS / Flatfile etc)
		\subitem Logisch (Schemata / Formate / Darstellungen etc.)
		\subitem Syntaktisch (Datum / Codierung / Währung)
		\subitem Qualitativ (Fehlende / Falsche / Doppelte Werte)
		\subitem Verfügbarkeit (Permanent / Periodisch / Temporär)
		\subitem Rechtlich (Datenschutz / Zugriffsverwaltung / Archivierung)
\end{itemize}

$\longrightarrow$ Neuer Ansatz einer Datenaufbereitung muss her: \textbf{Homogenisierung}

\subsection{Ungenügen der operativen Datenbanken für Entscheide (Skript S13)}

"Reguläre" Datenbanken im Geschäftsumfeld sind zu fest mit geschäftsrelevanten Lese- und Schreiboperationen beschäftigt. Bei solchen Datenbanken spricht man von OLTP-System (Online Transactional Processing). Diese Datenbanken sind also ziemlich schlecht geeignet für eine analytische, vorausschauende Bewirtschaftung, da solche Auswertung viel Zeit und vor allem Rechen-Performance benötigen.

Ausserdem liegen Daten in OLTP-Datenbanken meist in der 3. Normalform vor. Während dies eine sehr vernetze und effiziente Art der Datenspeicherung ist, ist die 3. Normalform ein schlechtes Abbild des intuitiven Denkens eines Managers.

$\rightarrow$ Neuer Ansatz einer Datenbank muss her: \textbf{analytische Datenbanken}

\subsection{SQL-Abfragen für Management-Zwecke}
Zusätzlich zu den vorhin genannten Gründen, sind Manager des SQL meist nicht mächtig. Sie wollen lieben "Drag and Drop" Interfaces, um sich ihre Daten "zusammenzuklicken" wie z.b. Microsoft Access.

Zudem sind Datenbank-Abfragen stets \textbf{zweidimensional} in Tabellen dargestellt. Wenn man nun aber Daten in drei Dimensionen auswerten will (z.B. Zeit, Ort und Anzahl), so ist dies zwar möglich mittels Tabellen, aber nicht sonderlich leserfreundlich.

$\rightarrow$ Neuer Ansatz der Datenabfrage muss her: \textbf{OLAP}

\begin{landscape}
\subsection{OLAP vs OLTP}
	
	\begin{tabular}[htbp]{|l|r|r|}
		\hline 
		Merkmal & OLTP System  & OLAP System  \\ 
		\hline 
		Ausrichtung auf & Programm, BWL Prozess & Mensch, Analyse\\ 
		\hline 
		Zeitliche Reichweite & Taktisch & Strategisch \\ 
		\hline 
		Entscheidungsstufe & Tief & Hoch \\ 
		\hline
		Zweck & Rationalisierung \& Automatisierung & Planung \& Entscheidung \\
		\hline
		Anwenderzahl &Hoch & Tief \\
		\hline
		Entscheidung & Deduktiv & Induktiv / Explorativ \\
		\hline
		Bewirtschaftung I & Ändernd & Befragend \\
		\hline
		Bewirtschaftung II & Auf Datensatzebene & Auf Aggregatsebene \\
		\hline
		Anwendungsmuster & Voraussehbar & Variierend \\
		\hline
		Befragungsmuster & Einfach & Komplex \\
		\hline
		Bearbeitung & Repetitiv & Ad hoc / unstrukturiert \\
		\hline
		Betriebliches Wissen & Verarbeitend & Generierend \\
		\hline
		Verteilungsgrad & Dezentral & Zentral \\
		\hline
		Performance-Bedarf & Durchgehend hoch & Variierend \\
		\hline
		Mehrbenutzersynchronisation & Hoch & Tief bis keine \\
		\hline
		Optimierung & Schneller Insert \& Delete & Schnelles Lesen \\
		\hline
		Transaktionsdurchsatz & Hoch & Tief \\
		\hline
		Transaktionsdauer & Kurte Mutationen weniger Tupel & Lange Abfragen vieler Tupel \\
		\hline
		Abfragen & Häufige, einfache Abfragen & Weniger häufige, komplexe Anfragen \\
		\hline
		Antwortzeiten & (Mili)sekunden & Sekunden, Minuten, Stunden \\
		\hline
		Endbenutzerwerkzeug-Hersteller & DB-Hersteller & Markt \\
		\hline
		Zeitbezug & Aktuell & Historisch \\ 
		\hline
		Zeitdimension & Zeitpunkt & Zeitraum \\
		\hline
		Beständigkeit & Dynamisch & Statisch \\
		\hline
		Granularität & Fein & Grob \\
		\hline
		Datenbestand & Vollständig & Lückenhaft \\
		\hline
		Redundanz & Normalisiert & Denormalisiert \\
		\hline
		Datenqualität / Aussagekraft & Tief & Hoch \\
		\hline
		Aufbereitung & Anwendungsneutral & Analyseorientiert \\
		\hline
		Aktualisierung & Laufend & Periodisch \\
		\hline
		Verarbeitungseinheit & Keon & Gross \\
		\hline
		Verteilungsgrad & Dezentral & Zentral \\
		\hline
		Datenquelle & Aktuelle Unternehmensdaten & Interne \& externe Daten \\
		\hline
	\end{tabular} 
	\end{landscape}


\section{Daten vs. Informationen vs. Wissen vs. Weisheit}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{DIWW.PNG}
	\caption{DIKW-Pyramid}
	\label{fig:dikw}
\end{figure}

Bei Entscheidungsfindungen muss unterschieden werden zwischen

\begin{itemize}
	\item Daten
	\item Informationen
	\item Wissen
	\item Weisheit
\end{itemize}

\paragraph{Daten}
Daten sind das, was in Datenbanken oder Excel-Tabellen gespeichert wird. \textbf{Unstrukturierte Fakten} wie z.B die Zahlenreihenfolge 
\begin{center}
\textbf{	Rot, 192.234.235.245.678.v2.0}
\end{center}

\paragraph{Informationen}
Aus Daten alleine werden wir nicht schlau. Diese Daten müssen zuerst in einen \textbf{Zusammenhang} gebracht werden:

\begin{center}
\textbf{	Das südliche Rotlicht Pitt/George St. ist soeben rot geworden }
\end{center}

Nun können wir aus dieser, vorher völlig nutzlosen Zahlenreihe eine \textbf{Information} extrahieren. Nämlich dass sie Koordinaten sind und sich das "Rot" auf ein Lichtsignal bezieht..

Die Daten stellen zwar den eigentlichen Wert der Information dar (die Koordinaten und Rotlicht-Licht), sind aber ohne Zusammenhang völlig wertlos.

\paragraph{Wissen}
Aus Informationen \textbf{Wissen} zu machen ist nun, zumindest maschinell gesehen wesentlich schwerer. Wir Menschen generieren Wissen, indem wir Informationen geistig verarbeiten. Soll heissen, wir \textbf{interpretieren und ordnen} die gegebene Information.

Das heisst in diesem Fall, wir checken wo wir sind und in welche Richtung wir uns bewegen. Je nach dem ist das Wissen dann:

\begin{center}
\textbf{	Ich fahre in eine völlig andere Richtung, diese Information interessiert mich nicht.}
\end{center}

oder aber

\begin{center}
\textbf{	Das Rotlicht, auf welches ich zufahre, ist gerade rot geworden}
\end{center}

\paragraph{Weisheit}
Weisheit wird definiert als \textbf{Anwendung von Wissen auf eine Problemlösung}. Das heisst, das erworbene Wissen wird mit einer Portion Erfahrung und gesundem Menschenverstand gemixt. In unserem Fall, angenommen wir fahren auf das Rotlicht zu, sagt uns die Erfahrung, dass man bei einem roten Rotlicht stoppen soll und der gesunde Menschenverstand wirft noch ein, dass es entweder einen Unfall geben wird oder aber sicherlich eine Busse, sollte man erwischt werden. Das Ganze resultiert in:

\begin{center}
\textbf{	Ich sollte vermutlich nächstens einmal anhalten}
\end{center}

\vspace*{20px}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{DIKW.PNG}
	\caption{Die Beziehung zwischen Daten - Informationen - Wissen - Weisheit}
	\label{fig:DIKW}
\end{figure}

Zusammengefasst kann man also sagen, Informationen sind das Verständnis von Zusammenhängen in Daten, oder auch \textbf{was die Daten bedeuten}. Wissen ist das Verständnis dieser Information (\textbf{was heisst das für mich?}) und Weisheit bestimmt, \textbf{was nun zu tun sei}. 
\newpage

\section{Das Data Warehouse}

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{datawarehouse.PNG}
	\caption{Aufbau eines Datawarehouses}
	\label{fig:datawarehouse}
\end{wrapfigure}

In einer optimalen Welt würden Daten "perfekt" abgelegt werden, leicht zugänglich, platzsparend, sicher und für verschiedene Zwecke nützlich. Da wir aber leider nicht in einer optimalen Welt leben, ist dies nicht der Fall. (Buch S32)

Daten sind in der Praxis meist nicht optimal abgelegt. Daten existieren meist
\begin{itemize}
	\item in unterschiedlichen Formaten (Excel, Access, DB etc)
	\item in unterschiedlichen DB-Strukturen 
	\item in unterschiedlichen IT-Architekturen und -Systemen. Meist auch uralt Legacy-Systeme (Wie z.B. Cobol)
	\item zeit-aktuell und dynamisch
	\item zu detailliert und feingranular für wirksame Management-Abfragen
	\item in einem Format, das für Änderungstransaktionen optimiert wurde (z.B. 3. Normalform)
	\item mit begrenzten Zugriffsrechten (z.B. aus Security-Gründen)
	\item in einem schlecht verfügbaren Zustand (Legacy-System, proprietäres Format, Security-Gründe)
	\item in einem Format, welches komplexe SQL-Queries verlangt, um an Informationen oder Wissen zu gelangen.
\end{itemize}

$\rightarrow$ Lösung: \textbf{Data-Warehouse}

\newpage

\subsection{Definition Data-Warehouse (Skript S 33)}
	
\blockquote[Oracle corp: Data warehousing Guide 11g (2007)]{A data warehouse is a relational database that is designed for query and analysis rather than for transaction processing. It usually contains historic data derived from transaction data, but can incude data from other sources. Data warehouses separate analysis workload from transactin workload and enable an organisation to consoldiate data from several sources.} 
 

\blockquote[IBM Corp: Enterprise Data Warehousing with DB2.9 - Redbook (2008)]{A data warehouse is a organisation's data with a corporate wide scope for use in decision support and informational applications.}
\vspace*{10px}

Zusammengefasst kann man also sagen, ein Data Warehouse ist eine Datenbank, welche nicht (ausschliesslich) zur Speicherung von Informationen genutzt wird, sondern hauptsächlich als Hilfsmittel bei Entscheidungen eingesetzt wird ($\rightarrow$ Experten-Systeme)



\subsection{Bestandteile eines Data-Warehouses (Skript S39)}

\begin{description}
	\item [SSRS: ] SQL Server Reporting Services
	\item [SSAS: ] SQL Server Analysis Services
	\item [SSIS: ] SQL Server Integration Services
\end{description}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{bestandteiledatawarehouse.PNG}
	\caption{Bestandteile eines Data-Warehouses}
	\label{fig:bstdw}
\end{figure}

\newpage

\subsection{Welche Datenbank für welche Tasks? (Buch S40f)}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{bestandteileDW.png}
	\caption{Welche Bestandteile eines Data-Warehouses werden für was benutzt?}
	\label{fig:bestDW}
\end{figure}


Verschiedene Datenbanken können (und sollten) für verschiedene Tasks verwendet werden.

\newpage

\section{Referenzarchitekturen}
\begin{wrapfigure}[16]{L}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{refModell.png}
	\caption{Referenzmodell eines Data Warehouses nach Bauer \& Günzel}
	\label{fig:refModel}
\end{wrapfigure}


Eine \textbf{Referenzarchitektur} ist ein Referenzmodell für eine Überklasse von Architekturen, in unserem Falle von Date Warehouses.

Ein \textbf{Referenzmodell} ist ein allgemeines Modell für eine Klasse von Dingen. Ein Referenzmodell sollte folgende zwei Eigenschaften haben:

\begin{itemize}
	\item Es können bestimmte Sachverhalte oder Modelle auf dessen Basis erstellt werden.
	\item Das allgemeine Modell kann als Vergleichsobjekt dienen.
\end{itemize}

In diesem Modul wird ausschliesslich mit dem hier dargestellten Referenzmodell von Bauer \& Günzel gearbeitet.

\vspace{2cm}

\subsection{Bestandteile des Referenzmodells (Skript S41)}
\subsubsection{Datenquelle}
Als \textbf{Datenquelle} eines Data-Warehouses werden oft beliebige\textit{ Bezugs-Datenbestände} benutzt (auch \textit{effektive Daten} oder \textit{Primärdaten} genannt)

Das können zum Beispiel:
\begin{itemize}
	\item Daten aus \textbf{Legacy-Systemen}
	\item Daten aus \textbf{Anwendungsprogrammen}
	\item Daten aus \textbf{zentralen/dezentralen Arbeitsplatz-DBs}
	\item \textbf{Textdateien} (z.B. im ASCII oder UTF-8 Format)
	\item Tabellen aus z.B Excel
\end{itemize}

sein.

\newpage

Es gibt weiterhin einige Probleme mit Datenquellen aus verschiedenen Systemen, wie zum Beispiel:
\begin{itemize}
	\item Unterschiedliche Zeichencodierung (ASCII, ANSI, UTF-8)
	\item Unterschiedliche Trennzeichen zwischen Datenfeldern (Komma, Semikolon)
	\item Unterschiedliche Zeilenwechsel (CR/LF, LF)
	\item Unterschiedlichen Sortierungen (alphanumerisch, numerisch)
\end{itemize}

Deshalb wird immer häufiger auf einheitliche XML-Inputs zurückgegriffen.

\subsubsection{Arbeitsbereich / Staging Area}
Der Arbeitsbereich integriert die Daten aus den verschiedenen vorhin genannten Datenquellen. Diese Integration passiert nach der Extraktion aus diesen Datenquellen.

Die Daten werden mit Hilfe des Metadaten-Repository anhand ihrer Metadaten zusammengefügt und abgelegt.

Das \textbf{Metadaten-Repository} besteht normalerweise aus verschiedenen Datenbank-Tabellen zur Verwaltung von Metadaten. Diese Metadaten stammen aus sehr unterschiedlichen Systemen und enthalten alle notwendigen Beschreibungen zu ihrem System und der Umwelt. Somit können die heterogenen Daten fast ohne Programmieraufwand zu einer homogenen Masse zusammengefügt werden.

\vspace{5px}

\noindent Die Staging Area ist ein \textbf{flüchtiges Zwischendepot} für die Daten. Hier werden die notwendigen Transformationen durchgeführt werden, welche in weiteren Schritten notwendig sind.

\subsubsection{Basis-DB / Operational Data Store (ODS)}
Ein ODS ist eine Datenbank, welche aktuelle/operative Daten hält, meist in kleinen Teilmengen unterteilt. Diese Datenbank ist eine \textbf{temporäre} Zwischenstation zwischen der Staging Area und dem Data Warehouse. 

\vspace{5px}

Das Datenmodell des OBS entspricht meist demjenigen der Datenquelle. Dies ist vor allem dann der Fall, wenn der OBS aus Leistungsgründen vom Quellsystem getrennt wurde und benutzt wird um gelegentliche Einzelfall-Analysen durchzuführen.

\vspace{5px}

Das Datenmodell des OBS kann andererseits auch demjenigen des Data Warehouses entsprechen. Dies ist dann der FAll, wenn der ODS als temporärer Zwischenspeicher zwischen den Quelldaten und dem Data Warehouse genutzt wird, oder aber wenn der ODS vom Data Warehouse Daten erhält.

Eine Basis-DB/ein ODS muss nach Gauer \& Günzel folgende Eigenschaften haben:
\begin{itemize}
	\item Die Daten sind \textbf{integriert} von den jeweiligen Datenquellen. Das heisst die verschiedenen Datenformate und Schematas wurden vereinheitlicht.
	\item Sie enthält nebst aktuellen Daten auch \textbf{historische Daten}, jedoch in geringerer Feingranularität wie das Data Warehouse.
	\item Sie ist \textbf{Anwendungsneutral}, d.h. sie ist nicht für eine spezielle Anwendung optimiert bzw. fokussiert.
	\item Nach einer definierten Zeitspanne werden die Daten in die \textbf{Ableitungsdatenbank} übertragen, wo sie je nach Auswertungsbedarf in einem anderen Detaillierungsgrad abgelegt werden.
	\item Die \textbf{Aktualisierung} der Daten erfolgt zu \textbf{beliebigen Zeitpunkten}. Dieser Zeitpunkt wird durch den Aktualisierungsbedarf gesteuert.
	\item Die Daten wurden bereits in der Staging Area bereinigt.
\end{itemize}

\vspace{5px}

Der ODS wird entweder laufend oder periodisch mit Daten aus der Staging Area gefüttert. Vor allem in kleineren Data Warehouse Systemen fällt der ODS sogar manchmal ganz weg und die Staging Area übernimmt auch diese Arbeiten. Im professionellen Umfeld sind jedoch sowohl der ODS wie auch die Stagin-Area ein integraler Bestandteil der Data Warehouse Architektur. 

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{ODS.png}
	\caption{Eingliederung des ODS in die Data Warehouse Architektur}
	\label{fig:ODS}
\end{figure}

\newpage

\subsubsection{Ableitungsdatenbank (=(Enterprise-)Data Warehouse)}
Wenn von einem Data Warehouse gesprochen wird, ist in der Regel ein \textit{Enterprise-Data-Warehouse} gemeint (= ein DWH mit unternehmensweitem Datenmodell und unternehmensweiten, universellen Datenbestand). 

\vspace{5px}

Die Datenbestände eines Data Warehouses werden zwar periodisch ergänzt, jedoch werden praktisch nie Daten gelöscht oder verändert, wenn sie mal im Data Warehouse sind. Aufgrund dessen beinhalten solche Systeme eine enorme Menge von Daten (auch VLDBs für Very Large DataBases).

Es ist jedoch selten nötig auf den gesamten Datenbestand dieser riesigen Datenbanken zuzugreifen. Aufgrund dessen werden Daten, die sehr selten benutzt werden in grossen Unternehmungen meist in Teilkopien des Data Warehouses, sogenannten \textit{Data Marts} gespeichert.

\subsubsection{Auswertungsdatenbank (Data-Mart)}
Ein Data-Mart ist eine \textbf{Teilkopie} eines Data-Warehouses, die aber auf demselben Datenmodell basieren. Solche Data-Marts werden meist für Abteilungen einer Unternehmung wie z.B. das Marketing erstellt. Der Vorteil dieser Data-Marts ist, dass diese unabhngig sind vom zentralen DW.

Vorteile eines Data-Marts sind z.B. 

\begin{itemize}
	\item Bessere Leistung, da nicht alle Analysen/Auswertungen auf dem zentralen DW gemacht werden müssen
	\item Entlastung des DW
	\item Im Falle von lokalen Data-Marts weniger Netzwerkbelastung
\end{itemize}

Die Pflege und (Weiter)Entwicklung des Data-Marts liegt jeweils in der Verantwortung der einzelnen Abteilungen.

\vspace{5px}

Dass ein Data-Mart existieren kann, \textbf{muss} ein zentrales Data-Warehouse existieren. Deshalb werden solche Data-Marts meist als \textbf{abhängige Data-Marts} bezeichnet.

\vspace{5px}

Diese abhängigen Data-Marts stehen im Gegensatz zu den (historischen) \textbf{unabhängigen Data-Marts}. Diese Data Mars erhalten ihre Daten direkt von verschiedenen Quellsystemen (gehen also nicht über ein zentrales Data-WArehouse). Jedoch widerspricht das dem Gedanken von einem zentralen "Datenhort" des Data-Warehousing und \textit{sollten} heute nicht mehr verwendet werden (oder man sollte sie zumindest nicht mehr als Data-Marts bezeichnen...)

Ein "Problem" von Data-Marts ist, dass sie unabhängig voneinander sind. Somit werden meist dieselben Aggregationen mehrmals auf verschiedenen Data-Marts durchgeführt, was auf längere Zeit nicht sehr kosteneffizient ist. Stattdessen sollten diese Aggregationen eher auf dem zentralen DW durchgeführt werden (was aber nicht immer geht, da die verschiedenen Abteilungen nicht wissen, dass diese Aggregation bereits in einem anderen Data-Mart durchgeführt wurde)

\subsection{Integrationsbereich (Skript S51)}
Der Integrationsbereich bereitet die Daten, die aus verschiedenen Datenquellen kommen via staging area und ODS so auf, dass sie ins zentrale DW überführt werden können. 

Im Zusammenhang mit dem Integrationsbereich wird oft von \textbf{ETL} gesprochen. ETL steht für \textit{Extract - Transfer - Load} und entspricht den wesentlichen Arbeitsschritten, die durchgeführt werden müssen, bis die Daten vom Quellsystem im DW landen.

\subsubsection{ETL}
\paragraph{Extraktion}
Unter Extraktion versteht man die \textbf{Übertragung der Daten vom Quellsystem in die Staging Area}

Je nach dem, welche \textbf{Monitorstrategie} gewählt wurde, müssen dabei noch geänderte Datenbestände selektiert werden.

\begin{description}
	\item[Triggerbasiert] Jede Datenmanipulation löst einen Trigger aus
	\item[Replikationsbasiert] Geänderte Tupel werden in spezielle Tabellen geschrieben
	\item[Zeitstempelbasiert] Jeder Datensatz hat ein Zeitstempel, welcher bei jeder Änderung aktualisiert wird. Anhand dieser Zeitstempel kann entschieden werden, welche Daten seit der letzten Extraktion geändert wurden.
	\item[Logbasiert] Jede Änderung wird in einer Logdatei protokolliert. Durch die Auswertung dieser Logdatei wird entschieden, welche Daten geändert und somit extrahiert werden müssen
	\item[Snapshotbasiert] Der gesamte Datenbestand wird periodisch in eine Datei geschrieben. Durch den Vergleich zweier solcher Snapshot-Dateien sieht man, welche Dateien zwischen den Snapshots geändert wurden.
\end{description}

\paragraph{Tranformation}
\paragraph{Load}


\end{document}
