\documentclass[a4paper, 11pt, nofootinbib]{article}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\usepackage{amsmath}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage[utf8]{inputenc}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{makecell}
\usepackage{tabularx}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{lscape}
\usepackage{afterpage}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{chngcntr}
\usepackage{multicol}

%Tabellenheadings
\renewcommand*{\thead}[1]{\bfseries #1}

%Ermöglicht bullet-points in Tabellen
\newcommand \tabitem{\makebox[1em][r]{\textbullet~}}

%Abbildungen werden pro Kapitel nummeriert (Abb. 1.4, Abb. 4.7 etc)
\counterwithin{figure}{section}

\geometry{a4paper, margin=1in}

%Bilder haben die Unterschrift Abb. anstelle von Figure
\renewcommand{\figurename}{Abb.}

%\code{Text} formatiert Text als monospace Schrift
\newcommand{\code}[1]{\texttt{#1}}

%Table of Contents und List of Figures werden in Inhalt und Abbildungsverzeichnis umbenannt
\renewcommand{\contentsname}{Inhalt}
\renewcommand{\listfigurename}{Abbildungsverzeichnis}

%Wie sollen Code-Snippets mit \lstinline|code| aussehen?
\lstset{frame=tb,
	language=sql,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{darkgreen},
	stringstyle=\color{violet},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

%RGB-Definition von eiigen Farben
\definecolor{system}{RGB}{141,0,76}
\definecolor{inhalt}{RGB}{2,47,99}
\definecolor{darstellung}{RGB}{116,117,117}
\definecolor{nutzung}{RGB}{207,2,127}
\definecolor{darkgreen}{RGB}{39,117,1}

\begin{document}
\title{Zusammenfassung Data Warehousing FS2018}
%\author{Alex Neher}
\maketitle

\tableofcontents
\newpage
\listoffigures
\newpage

\graphicspath{{./Pictures/}}

\section{Die Notwendigkeit von Data Warehouses}
\subsection{Entscheidungsunterstützung (Skript S15)}
Data Warehouses sind keine neue Erfindung. Bereits in den 1960er Jahren wurden sogenannte \textbf{Managementsinformationssysteme} entwickelt. Diese MIS dienten dazu, Entscheidungsträgern alle benötigten Informationen zeitnah, fehlerfrei, flexibel, ergonomisch, effizient, effektiv und inspirativ zur Verfügung zu stellen. Diese Systeme treffen also nicht selbst Entscheidungen, sie \textbf{unterstützen} die Entscheidungsträger lediglich bei ihrer Entscheidung.

Es gibt vier Arten der Entscheidungsunterstützung:

\begin{description}
	\item [Modellbasiert:] z.B. Lineare Optimierung - Ein Mathematischer Ansatz basierend auf einem Modell $\Longrightarrow$ Abbildung der Realität
	\item [Wissensbasiert: ] z.B. Expertensysteme - Ansätze von Künstlicher Intelligenz
	\item [Datenbasiert: ] Basierend auf grossen Datenmengen $\Longrightarrow$ Data-Warehouse, OLAP oder Data-Mining
	\item [KI: ] Basierend auf Vorschlägen von Systemen, die Entscheidungen auf Basis von Daten und/oder gelernten Inhalten ($\longrightarrow$ Machine Learning)
\end{description}


\paragraph{Ein Expertensystem}(XPS oder ES) ist ein Computerprogramm, das Menschen bei der Lösung von komplexen Problemen wie ein menschlicher Experte unterstützen kann, indem es Handlungsempfehlungen aus einer Wissensbasis ableitet.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{expertensystem.png}
	\caption{Beispiel eines Expertensystems}
	\label{fig:xps}
\end{figure}

\newpage

\subsection{Ungenügen der "gängigen" Datenhaltung (Skript S17)}

\begin{itemize}
	\item Verschiedene Datenformate
	\item Verschiedene Werkzeuge
	\item Heterogenität der Daten
		\subitem Technisch (Mainframe / DBMS / Flatfile etc)
		\subitem Logisch (Schemata / Formate / Darstellungen etc.)
		\subitem Syntaktisch (Datum / Codierung / Währung)
		\subitem Qualitativ (Fehlende / Falsche / Doppelte Werte)
		\subitem Verfügbarkeit (Permanent / Periodisch / Temporär)
		\subitem Rechtlich (Datenschutz / Zugriffsverwaltung / Archivierung)
\end{itemize}

$\longrightarrow$ Neuer Ansatz einer Datenaufbereitung muss her: \textbf{Homogenisierung}

\subsection{Ungenügen der operativen Datenbanken für Entscheide (Skript S18)}

"Reguläre" Datenbanken im Geschäftsumfeld sind zu fest mit geschäftsrelevanten Lese- und Schreiboperationen beschäftigt. Bei solchen Datenbanken spricht man von OLTP-System (Online Transactional Processing). Diese Datenbanken sind also ziemlich schlecht geeignet für eine analytische, vorausschauende Bewirtschaftung, da solche Auswertung viel Zeit und vor allem Rechen-Performance benötigen.

Ausserdem liegen Daten in OLTP-Datenbanken meist in der 3. Normalform vor. Während dies eine sehr vernetze und effiziente Art der Datenspeicherung ist, ist die 3. Normalform ein schlechtes Abbild des intuitiven Denkens eines Managers.

$\rightarrow$ Neuer Ansatz einer Datenbank muss her: \textbf{analytische Datenbanken}

\subsection{SQL-Abfragen für Management-Zwecke (Skript S18ff)}
Zusätzlich zu den vorhin genannten Gründen, sind Manager des SQL meist nicht mächtig. Sie wollen lieben "Drag and Drop" Interfaces, um sich ihre Daten "zusammenzuklicken" wie z.b. Microsoft Access.

Zudem sind Datenbank-Abfragen stets \textbf{zweidimensional} in Tabellen dargestellt. Wenn man nun aber Daten in drei Dimensionen auswerten will (z.B. Zeit, Ort und Anzahl), so ist dies zwar möglich mittels Tabellen, aber nicht sonderlich leserfreundlich.

$\rightarrow$ Neuer Ansatz der Datenabfrage muss her: \textbf{OLAP}

\newgeometry{margin=0.5in}

\begin{landscape}
\subsection{OLAP vs OLTP (Skript S24)}
	
	\centering
	
	\begin{tabular}[htbp]{|l|r|r|}
		\hline 
		Merkmal & OLTP System  & OLAP System  \\ 
		\hline 
		Ausrichtung auf & Programm, BWL Prozess & Mensch, Analyse\\ 
		\hline 
		Zeitliche Reichweite & Taktisch & Strategisch \\ 
		\hline 
		Entscheidungsstufe & Tief & Hoch \\ 
		\hline
		Zweck & Rationalisierung \& Automatisierung & Planung \& Entscheidung \\
		\hline
		Anwenderzahl &Hoch & Tief \\
		\hline
		Entscheidung & Deduktiv & Induktiv / Explorativ \\
		\hline
		Bewirtschaftung I & Ändernd & Befragend \\
		\hline
		Bewirtschaftung II & Auf Datensatzebene & Auf Aggregatsebene \\
		\hline
		Anwendungsmuster & Voraussehbar & Variierend \\
		\hline
		Befragungsmuster & Einfach & Komplex \\
		\hline
		Bearbeitung & Repetitiv & Ad hoc / unstrukturiert \\
		\hline
		Betriebliches Wissen & Verarbeitend & Generierend \\
		\hline
		Verteilungsgrad & Dezentral & Zentral \\
		\hline
		Performance-Bedarf & Durchgehend hoch & Variierend \\
		\hline
		Mehrbenutzersynchronisation & Hoch & Tief bis keine \\
		\hline
		Optimierung & Schneller Insert \& Delete & Schnelles Lesen \\
		\hline
		Transaktionsdurchsatz & Hoch & Tief \\
		\hline
		Transaktionsdauer & Kurte Mutationen weniger Tupel & Lange Abfragen vieler Tupel \\
		\hline
		Abfragen & Häufige, einfache Abfragen & Weniger häufige, komplexe Anfragen \\
		\hline
		Antwortzeiten & (Mili)sekunden & Sekunden, Minuten, Stunden \\
		\hline
		Endbenutzerwerkzeug-Hersteller & DB-Hersteller & Markt \\
		\hline
		Zeitbezug & Aktuell & Historisch \\ 
		\hline
		Zeitdimension & Zeitpunkt & Zeitraum \\
		\hline
		Beständigkeit & Dynamisch & Statisch \\
		\hline
		Granularität & Fein & Grob \\
		\hline
		Datenbestand & Vollständig & Lückenhaft \\
		\hline
		Redundanz & Normalisiert & Denormalisiert \\
		\hline
		Datenqualität / Aussagekraft & Tief & Hoch \\
		\hline
		Aufbereitung & Anwendungsneutral & Analyseorientiert \\
		\hline
		Aktualisierung & Laufend & Periodisch \\
		\hline
		Verarbeitungseinheit & Keon & Gross \\
		\hline
		Verteilungsgrad & Dezentral & Zentral \\
		\hline
		Datenquelle & Aktuelle Unternehmensdaten & Interne \& externe Daten \\
		\hline
	\end{tabular} 
	\end{landscape}

\restoregeometry

\section{Daten vs. Informationen vs. Wissen vs. Weisheit (Skript S26)}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{DIWW.PNG}
	\caption{DIKW-Pyramid}
	\label{fig:dikw}
\end{figure}

Bei Entscheidungsfindungen muss unterschieden werden zwischen

\begin{itemize}
	\item Daten
	\item Informationen
	\item Wissen
	\item Weisheit
\end{itemize}

\paragraph{Daten}
Daten sind das, was in Datenbanken oder Excel-Tabellen gespeichert wird. \textbf{Unstrukturierte Fakten} wie z.B die Zahlenreihenfolge 
\begin{center}
\textbf{	Rot, 192.234.235.245.678.v2.0}
\end{center}

\paragraph{Informationen}
Aus Daten alleine werden wir nicht schlau. Diese Daten müssen zuerst in einen \textbf{Zusammenhang} gebracht werden:

\begin{center}
\textbf{	Das südliche Rotlicht Pitt/George St. ist soeben rot geworden }
\end{center}

Nun können wir aus dieser, vorher völlig nutzlosen Zahlenreihe eine \textbf{Information} extrahieren. Nämlich dass sie Koordinaten sind und sich das "Rot" auf ein Lichtsignal bezieht..

Die Daten stellen zwar den eigentlichen Wert der Information dar (die Koordinaten und Rotlicht-Licht), sind aber ohne Zusammenhang völlig wertlos.

\paragraph{Wissen}
Aus Informationen \textbf{Wissen} zu machen ist nun, zumindest maschinell gesehen wesentlich schwerer. Wir Menschen generieren Wissen, indem wir Informationen geistig verarbeiten. Soll heissen, wir \textbf{interpretieren und ordnen} die gegebene Information.

Das heisst in diesem Fall, wir checken wo wir sind und in welche Richtung wir uns bewegen. Je nach dem ist das Wissen dann:

\begin{center}
\textbf{	Ich fahre in eine völlig andere Richtung, diese Information interessiert mich nicht.}
\end{center}

oder aber

\begin{center}
\textbf{	Das Rotlicht, auf welches ich zufahre, ist gerade rot geworden}
\end{center}

\paragraph{Weisheit}
Weisheit wird definiert als \textbf{Anwendung von Wissen auf eine Problemlösung}. Das heisst, das erworbene Wissen wird mit einer Portion Erfahrung und gesundem Menschenverstand gemixt. In unserem Fall, angenommen wir fahren auf das Rotlicht zu, sagt uns die Erfahrung, dass man bei einem roten Rotlicht stoppen soll und der gesunde Menschenverstand wirft noch ein, dass es entweder einen Unfall geben wird oder aber sicherlich eine Busse, sollte man erwischt werden. Das Ganze resultiert in:

\begin{center}
\textbf{	Ich sollte vermutlich nächstens einmal anhalten}
\end{center}

\vspace*{20px}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{DIKW.PNG}
	\caption{Die Beziehung zwischen Daten - Informationen - Wissen - Weisheit}
	\label{fig:DIKW}
\end{figure}

Zusammengefasst kann man also sagen, Informationen sind das Verständnis von Zusammenhängen in Daten, oder auch \textbf{was die Daten bedeuten}. Wissen ist das Verständnis dieser Information (\textbf{was heisst das für mich?}) und Weisheit bestimmt, \textbf{was nun zu tun sei}. 
\newpage

\section{Das Data Warehouse}

\blockquote[Buch, S32]{In einer optimalen Welt würden Daten "perfekt" abgelegt werden, leicht zugänglich, platzsparend, sicher und für verschiedene Zwecke nützlich. Da wir aber leider nicht in einer optimalen Welt leben, ist dies nicht der Fall.}
\vspace{10px}


\begin{wrapfigure}[10]{R}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{datawarehouse.PNG}
	\caption{Aufbau eines Datawarehouses}
	\label{fig:datawarehouse}
\end{wrapfigure}

Daten sind in der Praxis meist nicht optimal abgelegt. Daten existieren meist
\begin{itemize}
	\item in unterschiedlichen Formaten (Excel, Access, DB etc)
	\item in unterschiedlichen DB-Strukturen 
	\item in unterschiedlichen IT-Architekturen und -Systemen. Meist auch uralt Legacy-Systeme (Wie z.B. Cobol)
	\item zeit-aktuell und dynamisch
	\item zu detailliert und feingranular für wirksame Management-Abfragen
	\item in einem Format, das für Änderungstransaktionen optimiert wurde (z.B. 3. Normalform)
	\item mit begrenzten Zugriffsrechten (z.B. aus Security-Gründen)
	\item in einem schlecht verfügbaren Zustand (Legacy-System, proprietäres Format, Security-Gründe)
	\item in einem Format, welches komplexe SQL-Queries verlangt, um an Informationen oder Wissen zu gelangen.
\end{itemize}

$\rightarrow$ Lösung: \textbf{Data-Warehouse}

\newpage

\subsection{Definition Data-Warehouse (Skript S 37)}
	
\blockquote[Oracle corp: Data warehousing Guide 11g (2007)]{A data warehouse is a relational database that is designed for query and analysis rather than for transaction processing. It usually contains historic data derived from transaction data, but can incude data from other sources. Data warehouses separate analysis workload from transactin workload and enable an organisation to consoldiate data from several sources.} 
 

\blockquote[IBM Corp: Enterprise Data Warehousing with DB2.9 - Redbook (2008)]{A data warehouse is a organisation's data with a corporate wide scope for use in decision support and informational applications.}
\vspace*{10px}

Zusammengefasst kann man also sagen, ein Data Warehouse ist eine Datenbank, welche nicht (ausschliesslich) zur Speicherung von Informationen genutzt wird, sondern hauptsächlich als Hilfsmittel bei Entscheidungen eingesetzt wird ($\rightarrow$ Experten-Systeme)



\subsection{Bestandteile eines Data-Warehouses (Skript S43)}

\begin{description}
	\item [SSRS: ] SQL Server Reporting Services
	\item [SSAS: ] SQL Server Analysis Services
	\item [SSIS: ] SQL Server Integration Services
\end{description}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{bestandteiledatawarehouse.PNG}
	\caption{Bestandteile eines Data-Warehouses}
	\label{fig:bstdw}
\end{figure}

\newpage

\subsection{Welche Datenbank für welche Tasks? (Buch S40ff)}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{bestandteileDW.png}
	\caption{Welche Bestandteile eines Data-Warehouses werden für was benutzt?}
	\label{fig:bestDW}
\end{figure}


Verschiedene Datenbanken können (und sollten) für verschiedene Tasks verwendet werden.

\newpage

\section{Referenzarchitekturen}
\begin{wrapfigure}[15]{L}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{refModell.png}
	\caption{Referenzmodell eines Data Warehouses nach Bauer \& Günzel}
	\label{fig:refModel}
\end{wrapfigure}


Eine \textbf{Referenzarchitektur} ist ein Referenzmodell für eine Überklasse von Architekturen, in unserem Falle von Date Warehouses.

Ein \textbf{Referenzmodell} ist ein allgemeines Modell für eine Klasse von Dingen. Ein Referenzmodell sollte folgende zwei Eigenschaften haben:

\begin{itemize}
	\item Es können bestimmte Sachverhalte oder Modelle auf dessen Basis erstellt werden.
	\item Das allgemeine Modell kann als Vergleichsobjekt dienen.
\end{itemize}

In diesem Modul wird ausschliesslich mit dem hier dargestellten Referenzmodell von Bauer \& Günzel gearbeitet.

\vspace{3cm}

\subsection{Bestandteile des Referenzmodells (Skript S45ff)}
\subsubsection{Datenquelle (Skript S46)}
Als \textbf{Datenquelle} eines Data-Warehouses werden oft beliebige\textit{ Bezugs-Datenbestände} benutzt (auch \textit{effektive Daten} oder \textit{Primärdaten} genannt)

Das können zum Beispiel:
\begin{itemize}
	\item Daten aus \textbf{Legacy-Systemen}
	\item Daten aus \textbf{Anwendungsprogrammen}
	\item Daten aus \textbf{zentralen/dezentralen Arbeitsplatz-DBs}
	\item \textbf{Textdateien} (z.B. im ASCII oder UTF-8 Format)
	\item Tabellen aus z.B Excel
\end{itemize}

sein.

\newpage

Es gibt weiterhin einige Probleme mit Datenquellen aus verschiedenen Systemen, wie zum Beispiel:
\begin{itemize}
	\item Unterschiedliche Zeichencodierung (ASCII, ANSI, UTF-8)
	\item Unterschiedliche Trennzeichen zwischen Datenfeldern (Komma, Semikolon)
	\item Unterschiedliche Zeilenwechsel (CR/LF, LF)
	\item Unterschiedlichen Sortierungen (alphanumerisch, numerisch)
\end{itemize}

Deshalb wird immer häufiger auf einheitliche XML-Inputs zurückgegriffen.

\subsubsection{Arbeitsbereich / Staging Area (Skript S46 / Buch S55)}
Der Arbeitsbereich integriert die Daten aus den verschiedenen vorhin genannten Datenquellen. Diese Integration passiert nach der Extraktion aus diesen Datenquellen.

Die Daten werden mit Hilfe des Metadaten-Repository anhand ihrer Metadaten zusammengefügt und abgelegt.

Das \textbf{Metadaten-Repository} besteht normalerweise aus verschiedenen Datenbank-Tabellen zur Verwaltung von Metadaten. Diese Metadaten stammen aus sehr unterschiedlichen Systemen und enthalten alle notwendigen Beschreibungen zu ihrem System und der Umwelt. Somit können die heterogenen Daten fast ohne Programmieraufwand zu einer homogenen Masse zusammengefügt werden.

\vspace{10px}

\noindent Die Staging Area ist ein \textbf{flüchtiges Zwischendepot} für die Daten. Hier werden die notwendigen Transformationen durchgeführt werden, welche in weiteren Schritten notwendig sind.

\subsubsection{Basis-DB / Operational Data Store (ODS) (Skript S47 / Buch S58)}
Ein ODS ist eine Datenbank, welche aktuelle/operative Daten hält, meist in kleinen Teilmengen unterteilt. Diese Datenbank ist eine \textbf{temporäre} Zwischenstation zwischen der Staging Area und dem Data Warehouse. 

\vspace{10px}

\noindent Das Datenmodell des OBS entspricht meist demjenigen der Datenquelle. Dies ist vor allem dann der Fall, wenn der OBS aus Leistungsgründen vom Quellsystem getrennt wurde und benutzt wird um gelegentliche Einzelfall-Analysen durchzuführen.

\vspace{10px}

\noindent Das Datenmodell des OBS kann andererseits auch demjenigen des Data Warehouses entsprechen. Dies ist dann der FAll, wenn der ODS als temporärer Zwischenspeicher zwischen den Quelldaten und dem Data Warehouse genutzt wird, oder aber wenn der ODS vom Data Warehouse Daten erhält.

Eine Basis-DB/ein ODS muss nach Gauer \& Günzel folgende Eigenschaften haben:
\begin{itemize}
	\item Die Daten sind \textbf{integriert} von den jeweiligen Datenquellen. Das heisst die verschiedenen Datenformate und Schematas wurden vereinheitlicht.
	\item Sie enthält nebst aktuellen Daten auch \textbf{historische Daten}, jedoch in geringerer Feingranularität wie das Data Warehouse.
	\item Sie ist \textbf{Anwendungsneutral}, d.h. sie ist nicht für eine spezielle Anwendung optimiert bzw. fokussiert.
	\item Nach einer definierten Zeitspanne werden die Daten in die \textbf{Ableitungsdatenbank} übertragen, wo sie je nach Auswertungsbedarf in einem anderen Detaillierungsgrad abgelegt werden.
	\item Die \textbf{Aktualisierung} der Daten erfolgt zu \textbf{beliebigen Zeitpunkten}. Dieser Zeitpunkt wird durch den Aktualisierungsbedarf gesteuert.
	\item Die Daten wurden bereits in der Staging Area bereinigt.
\end{itemize}

\vspace{10px}

\noindent Der ODS wird entweder laufend oder periodisch mit Daten aus der Staging Area gefüttert. Vor allem in kleineren Data Warehouse Systemen fällt der ODS sogar manchmal ganz weg und die Staging Area übernimmt auch diese Arbeiten. Im professionellen Umfeld sind jedoch sowohl der ODS wie auch die Stagin-Area ein integraler Bestandteil der Data Warehouse Architektur. 

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{ODS.png}
	\caption{Eingliederung des ODS in die Data Warehouse Architektur}
	\label{fig:ODS}
\end{figure}

\subsubsection{Ableitungsdatenbank (=(Enterprise-)Data Warehouse) (Skript S47/Buch S64)}
Wenn von einem Data Warehouse gesprochen wird, ist in der Regel ein \textit{Enterprise-Data-Warehouse} gemeint (= ein Data-Warehouse mit unternehmensweitem Datenmodell und unternehmensweiten, universellen Datenbestand). 

\vspace{10px}

\noindent Die Datenbestände eines Data Warehouses werden zwar periodisch ergänzt, jedoch werden praktisch nie Daten gelöscht oder verändert, wenn sie mal im Data Warehouse sind. Aufgrund dessen beinhalten solche Systeme eine enorme Menge von Daten (auch VLDBs für Very Large DataBases).

Es ist jedoch selten nötig auf den gesamte§n Datenbestand dieser riesigen Datenbanken zuzugreifen. Aufgrund dessen werden Daten, die sehr selten benutzt werden in grossen Unternehmungen meist in Teilkopien des Data Warehouses, sogenannten \textit{Data Marts} gespeichert.

\subsubsection{Auswertungsdatenbank (Data-Mart) (Skript S50/Buch S67)}
Ein Data-Mart ist eine \textbf{Teilkopie} eines Data-Warehouses, die aber auf demselben Datenmodell basieren. Solche Data-Marts werden meist für Abteilungen einer Unternehmung wie z.B. das Marketing erstellt. Der Vorteil dieser Data-Marts ist, dass diese unabhngig sind vom zentralen DW.

Vorteile eines Data-Marts sind z.B. 

\begin{itemize}
	\item Bessere Leistung, da nicht alle Analysen/Auswertungen auf dem zentralen DW gemacht werden müssen
	\item Entlastung des DW
	\item Im Falle von lokalen Data-Marts weniger Netzwerkbelastung
\end{itemize}

Die Pflege und (Weiter)Entwicklung des Data-Marts liegt jeweils in der Verantwortung der einzelnen Abteilungen.

\vspace{10px}

\noindent Dass ein Data-Mart existieren kann, \textbf{muss} ein zentrales Data-Warehouse existieren. Deshalb werden solche Data-Marts meist als \textbf{abhängige Data-Marts} bezeichnet.

 Diese abhängigen Data-Marts stehen im Gegensatz zu den (historischen) \textbf{unabhängigen Data-Marts}. Diese Data Mars erhalten ihre Daten direkt von verschiedenen Quellsystemen (gehen also nicht über ein zentrales Data-WArehouse). Jedoch widerspricht das dem Gedanken von einem zentralen "Datenhort" des Data-Warehousing und \textit{sollten} heute nicht mehr verwendet werden (oder man sollte sie zumindest nicht mehr als Data-Marts bezeichnen...)

Ein "Problem" von Data-Marts ist, dass sie unabhängig voneinander sind. Somit werden meist dieselben Aggregationen mehrmals auf verschiedenen Data-Marts durchgeführt, was auf längere Zeit nicht sehr kosteneffizient ist. Stattdessen sollten diese Aggregationen eher auf dem zentralen DW durchgeführt werden (was aber nicht immer geht, da die verschiedenen Abteilungen nicht wissen, dass diese Aggregation bereits in einem anderen Data-Mart durchgeführt wurde)

\newpage

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{DWH_Bereiche.png}
	\caption{Bereiche eines Data-Warehouses nach Bauer \& Günzel}
	\label{fig:bereiche}
\end{figure}

\subsubsection{Integrationsbereich (Skript S54)}

Der Integrationsbereich bereitet die Daten, die aus verschiedenen Datenquellen kommen via staging area und ODS so auf, dass sie ins zentrale DW überführt werden können. 

Im Zusammenhang mit dem Integrationsbereich wird oft von \textbf{ETL} gesprochen. ETL steht für \textit{Extract - Transfer - Load} und entspricht den wesentlichen Arbeitsschritten, die durchgeführt werden müssen, bis die Daten vom Quellsystem im DW landen.



\subsubsection{Auswertungsbereich (Buch S72)}
Der Auswertungsbereich des Data Warehouses umfasst die \textbf{Ableitungsdatenbank/Data Warehouse} und die \textbf{Auswertungsdatenbank}. Genaueres über diese beiden Bestandteile des Auswertungsbereiches können in Kapitel 4.1.4 - Ableitungsdatenbank und 4.1.5 - Auswertungsdatenbank nachgelesen werden.

\vspace{10px}

\noindent Wie der Name schon vermuten lässt, werden die Daten, die von den Datenquellen über ETL-Prozesse in die Basisdatenbank geladen wurden, hier ausgewertet. 

Auswerten kann vieles sein. So können einfache Aggergationsfunktionen zur Auswertung benutzt werden, aber auch komplexe statistische Methoden, die z.B. beim Data Mining zum Einsatz kommen.

Zusammengefasst kann man sagen: Eine \textbf{Auswertung} ist die \textit{Anwendung von Auswertefunktionen auf ausgewählte Daten zur Generierung von neuer Information} .

Zudem werden die ausgewerteten Daten so aufbereitet und bereitgestellt, dass sie auch in anderen Systemen weiterverarbeitet werden können und/oder an andere Personen/Unternehmen weitergegeben werden.

Last but not least werden die Ergebnisse der Auswertung wieder in die Basisdatenbank zurückgegspeichert, so dass sie die Qualität der Datenbasis erhöht und zukünftige Auswertungen somit verbessert. 


\subsubsection{Verwaltungsbereich (Skript S272)}
Der Verwaltungsbereich eines Data Warehouses enthält den \textbf{Data Warehouse Manager}, den \textbf{Metadaten Manager} und das \textbf{Repositorium} (Abb. \ref{fig:bereiche})

\paragraph{Data Warehouse Manager (Buch S43)}\mbox{}\\
Der Data-Warehouse Manager (eine Person) initiiert, steuert und überwacht die einzelnen Data Warehouse Prozesse. Wie in Abb. \ref{fig:bereiche} zu sehen ist, übernimmt er unter anderem die zentrale Steuerung von: 

\begin{itemize}
	\item Monitor
	\item Extraktion
	\item Transformation
	\item Laden
	\item Auswertung
\end{itemize}

Zudem kann er, wenn nötig auch den Datenbeschaffungsprozess triggern. Das kann manuell geschehen oder anhand des Monitors (Kap. 5.4)

\paragraph{Metadaten Manager (Buch S. 82)}\mbox{}\\
Ähnlich wie beim Data Warehouse Manager kann beim Metadaten Manager recht einfach auf dessen Tätigkeit geschlossen werden: Es ist eine \textbf{Software}, die die Metadaten des Data Warehouses überwacht, steuert und initialisiert.

\vspace{10px}

\noindent Genauer gesagt heisst das, der Metadaten Manager verwaltet das \textbf{Metadaten Repositorium}, in welchem alle Metadaten (also Operative, Struktur-, Prozess- und Begriffsmetadaten) gespeichert werden.

Es können ausserdem Metadaten aus Entwurfs- und Modellierungswerkzeugen (wie z.B. SSMS oder MySQL Workbench) extrahiert werden

\vspace{10px}

\noindent Der Metadaten Manager übergibt ausserdem die Metadaten, die er aus dem Repositorium zieht dem Data Warehouse Manager, der diese wiederum an die metadatengesteuerten Werkzeuge, die diese zur Laufzeit lesen, interpretieren und ausführen. Diese Werkzeuge erzeugen selbst wieder Metadaten (z.B. Log- oder Reportdateien), die schlussendlich wieder im Repositorium landen und später wieder diesen Tools gefüttert werden. 

Das Ziel ist es, eine vollständig automatisierte Aktualisierung der Metadaten zu erreichen.

\paragraph{Repositorium (Buch S79)}\mbox{}\\
Das Repositorium ist der zentrale Storage für die Metadaten im Data Warehouse.

\newpage

\subsection{Referenzmodelle in die Praxis umgesetzt (Skript S63)}
\subsubsection{"Hub and Spoke"-Architektur}
\begin{wrapfigure}[14]{L}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=16\baselineskip]{HubAndSpoke.PNG}
	\caption{Hub and Spoke / Nabe und Speiche Architektur}
	\label{fig:hubnspoke}
\end{wrapfigure}

Bei der Hub and Spoke (auch Nabe und Speiche Arhictektur) werden aus mehreren Quellsystemen (Speichen/Spokes) Daten in eine Basisdatenbank geladen, dort ETL-ed und anschliessend in eine Ableitungsdatenbank überführt (Hub/Nabe). Von dort werden sie anschliessend wieder in verschiedene Auswertung-Datenbanken verteilt (Speiche/Spokes).

\vspace{10px}

\noindent Das Core-Data-Warehouse ist also eine der zentralen Naben oder Hubs und ist verantwortlich für die Integration, QA und die Verteilung von Daten an die verschiedenen Data-Marts (Auswertungs-Datenbanken). 

\vspace{80px}


\subsubsection{Star-Architektur}

\begin{wrapfigure}[16]{R}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=16\baselineskip]{starschema.jpg}
	\caption{Starschema mit einer Fakttabelle in der Mitte und fünf Dimensionstabellen}
	\label{fig:star}
\end{wrapfigure}

Die Star- oder Stern-Architektur ist eine der drei am weitesten verbreiteten Datenmodelle. Sie besteht aus einer sog. \textbf{Faktentabelle} in der Mitte, die von mehreren \textbf{Dimensionstabellen} sternförmig umgeben ist. 

Durch eine solche Anordnung sind die Datenbanken normalerweise \textit{denormalisiert}, können also redundante Daten enthalten. Jedoch wird dieser eventuelle höhere Speicherbedarf in Kauf genommen, da ein solches Sternschema eine wesentliche höhere Performance bietet im ETL-Bereich wie eine normalisierte Datenbank.

\vspace{10px}

\noindent Eine weitere Verbesserung der Performance könnte man mit dem, aus der Stern-Architektur abgeleiteten \textbf{Schneeflockenarchitektur} erzielen.

\newpage

\subsubsection{Snowflake-Architektur}
\begin{wrapfigure}[21]{L}{0.7\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{snowflake.PNG}
	\caption{Beispiel einer Schneeflocken-Architektur}
	\label{fig:snowflake}
\end{wrapfigure}

Die Snowflake- oder Schneeflocken-Architektur ist eng mit der Stenr-Architektur verbunden. Der einzige Unterschied zwischen der Stern- und der Schneeflocken-Architektur ist, dass die Dimensionstabellen gleichzeitig auch Faktentabellen sind, die wiederum von anderen Tabellen umgeben sind. Diese Dimensionstabellen werden über Joins verknüpft. 

\subsubsection{Galaxy-Architektur}

\begin{wrapfigure}[19]{R}{0.7\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{galaxy.PNG}
	\caption{Galaxy-Architektur}
	\label{fig:galaxy}
\end{wrapfigure}

Die Galaxy- oder Galaxie-Architektur ist ebenfalls stark mit dem Stern-Schema verwandt. Allerdings können sich hier mehrere Faktentabelle eine Dimensionstabelle teilen.

\vspace{20px}

\noindent Bei den drei oben genannten Architekturen werden Daten als \textbf{Fakten} bezeichnet und somit in der \textbf{Faktentabelle} gespeichert. Fakten werden, je nach Literatur auch \textbf{Metriken} oder \textbf{Kennzahlen} genannt. Solche Faktentabellen können über die Zeit sehr gross werden und enthalten Kennzahlen, die sich aus dem laufenden Geschäft ableiten lassen, wie z.B. Profitabilität, Kosten oder Ausgaben.

Wie in Abbildung \ref{fig:star} zu sehen ist, enthalten Faktentabellen Fremdschlüssel zu den Einträgen in den Dimensionstabellen. Das impliziert, dass es jeden Eintrag zu einer Kombination von Dimensionen nur einmal geben kann.

Im Gegensatz zu den riesigen und volatlen Faktentabellen sind Dimenstionstabellen recht statisch und vergleichsweise klein. 

\subsubsection{Beispiel zur Erstellung eines Datenbankschemas}
Es soll das DB-Design für eine Kursverwaltung erstellt werden. Die DB soll in einem Sternschema aufgebaut sein (Sternschema: Siehe Kap. 4.2.2)

\vspace{10px}

\noindent \textbf{Ablauf}

\begin{enumerate}
	\item Identifikation der Fakten
	\item Identifikation der Dimensionen
	\item Festlegen der Dimensionshierarchie
	\item Herstellung der Beziehungen
\end{enumerate}

\paragraph{Identifizierung der Fakten}\mbox{}\\
Welches sind die numerischen Werte, die die Entwicklung eines Geschäfts am Besten ausdrücken? 

\vspace{10px}

\noindent In unserem Fall wäre das sicher einmal die \textit{Anzahl der Kursbesucher}.

\paragraph{Identifikation der Dimensionen}\mbox{}\\
Was sind Dinge, die das wann, wo, was wer, etc. der Fakten beschreiben?  

\vspace{10px}

\noindent In unserem Fall wären das z.B. der Standort (wo?), die Zeit(wann?) und der Kurs (was?)

Momentan sieht unser Schema also so aus:

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{db-schema_empty.jpg}
	\caption{Leeres Datenbank-Schema mit Dimensions- und Faktentabellen.}
	\label{fig:empty_schema}
\end{figure}

\newpage

\paragraph{Festlegen der Dimensionshierarchie}\mbox{}\\
Nun wird geschaut, welche Elemente gehören zu den Dimension bzw. Fakten?

Das leere Datenbankschema wird nun aufgefüllt:

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=8\baselineskip]{db-schema.jpg}
	\caption{Datenbank-Schema mit Dimensions- und Faktentabellen.}
	\label{fig:db-schema}
\end{figure}

\paragraph{Herstellen der Beziehungen}\mbox{}\\
Ein DB-Schema, in welchem die Tabellen keine Beziehungen zueinander haben ist recht nutzlos. Nun muss geschaut werden, wie diese Tabellen oder Entitäten miteinander in Beziehung stehen. So hat z.B. ein Standort n Kurse. Diese Kurse werden zu n Zeiten durchgeführt. 

Grafisch dargestellt sieht unser DB-Schema mit Beziehungen nun so aus:

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=9\baselineskip]{db-schema_bez.jpg}
	\caption{Datenbank-Schema mit Dimensions- und Faktentabellen, die miteinander in Beziehung stehen.}
	\label{fig:db-schema_bez}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=13\baselineskip]{star_schema.jpg}
	\caption{Fertiges, ausnormalisiertes Sternschema der Kursverwaltung.}
	\label{fig:star_schema}
\end{figure}

\newpage

\noindent Werfen wir nun einen genaueren Blick auf die Tabellen, die hier dargestellt sind. 

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{dim_tables.jpg}
	\caption{Dimensionstabellen für die Zeit, den Kurs und das Kurszentrum}
	\label{fig:dim_tables}
\end{figure}

\begin{wrapfigure}[13]{R}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{fact_table.jpg}
	\caption{Faktentabelle der Einschreibungen}
	\label{fig:fact_table}
\end{wrapfigure}

\noindent In Abb. \ref{fig:fact_table} sieht man zudem noch die korrespondierende Faktentabelle. Es fällt auf, dass Dimensionstabellen weniger Einträge haben wie Faktentabellen und auch eher statisch sind. Nachdem man die gewünschten Attribute der Dimension erfasst hat, wird die Dimensionstabelle normalerweise nicht mehr gross geändert. Die Faktentabelle hingegen wächst stetig und kann in grösseren Data Warehouses problemlos mehrer 10 Millionen Einträge umfassen.

\vspace{10px}

\noindent Ebenfalls ist auffällig, dass die Dimensionstabelle erstaunlich viele Redundanzen hat. Jedoch ist dies gewollt. Man nimmt Redundanzen in Kauf, um dafür eine bessere Performance zu erhalten ($\rightarrow$ Denormalisierung).

Faktentabellen, im Gegensatz dazu, besitzen keine Redundanzen. Diese doppelten Einträge, die zu sehen sind werden \textit{notwendige Redundanzen} genannt. Denn wenn man sie löschen würde, entnähme das dem System Informationen.

\vspace{10px}

\noindent Eine weiter auffällige Tatsache ist, dass die Primärschlüssel der Dimensionstabelle offenbar eine Semantik besitzen (1101 für den 01. Monat des Jahres 2011). Es ist zu empfehlen, jegliche Semantik wegzulassen bei Primärschlüsseln. Deshalb wird im produktiven Umfeld meist auf das \verb|IDENTITY| oder \verb|AUTO_INCREMENT| zurückgegriffen.

\newpage

\section{ETL}
\subsection{Extraktion (Skript S 55/Buch S56)}

Unter Extraktion versteht man die \textbf{Übertragung der Daten vom Quellsystem in die Staging Area}

\vspace{10px}

\noindent Je nach dem, welche Extraktions-Strategie gewählt wurde, werden die Daten zu unterschiedlichen Zeitpunkten extrahiert:

\begin{description}
	\item[Sofort] Der Extraktionsprozess wird bei jeder Änderung sofort gestartet
	\item[Trigger-/Ereignisbasiert] z.B. Beim Erreichen einer festgelegten Anzahl von Änderungen
	\item[Periodisch] Die Daten haben eine geforderte Mindestaktualität. Nach Ablauf dieser festgelegten Periode werden die Änderungen in eine sog. Snapshot-Datei überführt, die anschliessend in die Datenbank importiert wird.
	\item[Anfrage] Eine Anfrage auf die Datenbank erfordert aktualisierte Daten.
\end{description}

\vspace{10px}

\noindent Es gibt zwei grundsätzlich zwei \textbf{Extraktionstechniken}
\begin{description}
	\item[Replikationsbasiert] Geänderte Tupel werden in spezielle Tabellen geschrieben
	\item[Logbasiert] Jede Änderung wird in einer Logdatei protokolliert. Durch die Auswertung dieser Logdatei wird entschieden, welche Daten geändert und somit extrahiert werden müssen
\end{description}

\noindent Die Extraktion geschieht meist über SQL und es werden meist Schnittstellen zu den Datenbanken verwendet wie z.B. ODBC, OLE DB oder JDBC.

\vspace{10px}

\noindent \textbf{Open Database Connectivity (ODBC)} ist eine standardisierte Datenbankschnittstelle, mit welcher über SQL auf die Datenbank zugegriffen werden kann. Der Vorteil von ODBC ist, dass es eine API gibt, die es einem erlaubt, eine DB-Anwendung zu schreiben, die unabhängig vom Datenbankmanagement-System (DBMS) (mySQL, postgreSQL, Oracle etc) funktioniert, solange ODBC einen Treiber dafür hat.

\vspace{10px}

\noindent \textbf{Java Database Connectivity (JDBC)} ist eine universelle Java-basierte Datenbankschnittstelle für Datenbanken diverser Hersteller. JDBC ist vor allem auf relationale Datenbanken ausgelegt.


\subsection{Tranformation (Skript S56/Buch S57)}

Nach der Extraktion müssen diese rohen Daten transformiert werden. Das heisst, sie müssen so aufbereitet werden, dass sie auch tatsächlich Sinn ergeben. Die Transformation lässt sich auf folgende Arbeitsschritte herunterbrechen:

\begin{description}
	\item[Data migration / Data wrangling] Daten werden \textbf{vereinheitlicht} also dass sie z.B. dieselben Masseinheiten oder Datenformate verwenden.
	\item[Data clean(s)ing] Daten werden \textbf{bereinigt}, also doppelte Daten gelöscht, falsche Daten korrigiert oder veraltete Daten aktualisiert.
\end{description}

\vspace{10px}

\noindent Dabei nimmt das Data Wrangling mit Abstand am meisten Zeit in Anspruch (bis zu 80\%)


\subsection{Load (Skript S58/Buch S58)}

Die extrahierten und bereinigten Daten müssen nun noch in die \textbf{Basis- oder Ableitungsdatenbank übertragen} werden. Daür werden meist Ladewerkzeuge des Datenbankherstellers verwendet (SSIS bei SQL oder SQL*Loader bei Oracle)

\vspace{10px}

\noindent 
Weitere Möglichkeiten sind z.B. die SSIS Integration von Visual Studio, SSIS direkt oder DataWriter von IBM

\vspace{10px}

\textbf{SQL-Codebeispiele für den Load-Prozess}

\vspace{10px}

\noindent SQL-INSERT

\begin{lstlisting}[language=SQL]
INSERT INTO table_name
	VALUES (
		value1,
		value2,
		value3,
		...
	)
; 
\end{lstlisting}

\noindent SQL BULK INSERT

\begin{lstlisting}[language=SQL]
//FileType=1 (TxtFile1.txt)

"Kelly","Reynold","kelly@reynold.com" 
"John","Smith","bill@smith.com" 
"Sara","Parker", "sara@parker.com"

//FileType=2 (TxtFile2.txt) 

Kelly,Reynold,kelly@reynold.com 
John,Smith,bill@smith.com 
Sara,Parker,sara@parker.com 

BULK INSERT TmpStList FROM 'c:\TxtFile1.txt' WITH (FIELDTERMINATOR = '","') 
BULK INSERT tmpStList FROM 'c:\TxtFile2.txt' WITH (FIELDTERMINATOR = ',')
\end{lstlisting}


\vspace{10px}

\noindent Aufgrund dessen, dass während des Ladens die betroffenen Datenbanken gar nicht oder nur eingeschränkt nutzbar sind, ist die Effizienz des Ladevorgang essentiell. Aus diesem Grund werden Ladevorgänge meist in der Nacht, am frühen Morgen oder übers Wochenende durchgeführt.

Es wird dabei zwischen \textbf{Online- und Offline-Ladevorgängen} unterschieden. Beim Online-Laden kann die Datenbank weiterhin verwendet werden. Beim Offline-Ladevorgang ist die Datenbank während des Vorgangs offline, kann also nicht verwendet werden. Ob eine Datenbank einen Online-Ladevorgang unterstützt, hängt davon ab, ob die verwendete Anwendung oder Datenbank dies zur Verfügung stellt. So könnten z.B. während des Ladevorgang die Datenänderungen in ein Logfile geschrieben werden und dieses Logfile wird nach erfolgreichem Ladevorgang in die aktualisierte Datenbank übertragen.

Ebenfalls unterscheidet man zwischen dem \textbf{Initialisierungsladen} (alle Daten, DB wird zum ersten Mal gefüllt) und dem \textbf{Aktualisierungsladen} (nur geänderte Daten werden in die DB geladen)

Daten werden entweder in eine Basis- Ableitungs- oder Auswertungsdatenbank geladen. Aufgrund der grossen Datenmengen können nicht die normalen Datenmanipulationswerkzeuge eingesetzt werden. Stattdessen werden meist sog. \textbf{Bulk-Loader},die auf genau diese Tasks spezialisiert sind, verwendet. Diese Bulk-Loader verwenden aus Effizienzgründen keine Schnittstelle zur DB, sondern greifen direkt darauf zu. Deshalb können bestimmte Bulk-Loader auch nur für bestimmte Datenbanken verwendet werden. 

\subsection{Monitor (Skript S60/Buch S54)}
Der Monitor spielt je nach Extraktionsstrategie eine wichtige Rolle. Er überwacht die Datenquellen und bemerkt somit Datenänderungen sofort. Je nach Extraktionsstrategie wird somit ein ETL-Prozess getriggert, ein Logeintrag oder eine Snapshot-Datei gemacht o.ä. 

Monitore haben vor allem in Data Warehouses eine sehr grosse Bedeutung. Dank ihnen müssen weniger DB-Zugriffe gemacht werden und gleichzeitig werden Redundanzen vermieden.

\vspace{10px}

\noindent Es gibt einige Sachen, die zu bedenken sind beim Einführen einer Monitor-Lösung:
\begin{itemize}
	\item Wird die gesamte Änderung festgehalten oder nur das neue/geänderte Tupel? ($\rightarrow$ Je nach dem Platzproblem)
	\item Benachricht das Quellsystem den Monitor sobald es eine Änderung gegeben hat ($\rightarrow$ Triggering) oder fragt der Monitor regelmässig beim Quellsystem nach ($\rightarrow$ Polling)? ($\rightarrow$ je nach dem, Überbelastung von Quell- oder Zielsystem)
	\item Überwacht das Quellsystem selbst, welche Änderungen erfolgen, oder wird dies von einem externen Dienst übernommen? ($\rightarrow$ Überbelastung Quellsystem vs. Kosten eines externen Dienstes)
	\item Sollen Änderungen sofort zum Data Warehouse gepusht werden oder nur zu bestimmten Zeiten (z.B. über Nacht)? ($\rightarrow$ Belastung des Data Warehouses und Quellsystems vs. Synchrone Daten.)
	
\end{itemize}

\newpage

\section{Unstrukturierte Daten (Skript S72)}
Ein Grossteil der Daten, der uns heute vorliegt, ist \textbf{unstrukturiert}. Das heisst, es ist mit viel Aufwand verbunden, aus grossen Datenmengen Informationen zu extrahieren (man denke z.B. daran, wie viel Aufwand es benötigen würde, um 10'000 E-Mails nach Thema zu sortieren vs. wie viel Aufwand es benötigt, 10'000 Datensätze nach ID zu sortieren).

Mithilfe von \textbf{strukturierten Daten} kann man \textit{automatisiert} Informationen aus Datenquellen extrahieren und aufbereiten.

\subsection{Nutzen}

\noindent Wenn man heute unstrukturierte Daten auswerten würde, könnte man z.B. automatisch Kundenbewertungen und -beschwerden aus Foren und Blogs auslesen, Twitter-Posts analysieren u.v.m. 

Zusammengefasst kann man sagen: Unstrukturierte Daten ermöglichen \textbf{Analysen, die zu schnellen Reaktionen und Entscheidungen führen}.5

\subsection{Herausforderungen}

\begin{itemize}
	\item Kein vorgegebenes/einheitliches Schema
	\item Kein vorgegebenes/einheitliches Datenformat
	\item Semantikabhängig (z.B. JSON vs. XML)
	\item Grosse Mengen $\rightarrow$ können nicht manuell ausgewertet werden.
	\item Priorität kann nicht automatisiert gemessen werden.
	\item Datenqualität kann nicht automatisiert gemessen werden
\end{itemize}

\subsection{Analyse (Skript S74ff/Buch S131ff)}

\subsubsection{NLP - Natural Language Processing}
 NLP beschreibt die Techniken und Methoden zur maschinellen Verarbeitung natürlicher Sprache. Ziel ist eine direkte Kommunikation zwische nMensch und coputer auf Basis der natürlichen (menschlichen) Sprache.

NLP ist eine sehr AI-lastige Technologie und mittels welcher Text in strukturierte Informationen gebündelt werden kann. NLP setzt sich mittels der Verarbeitung der natürlichen Sprache, dem Verstehen und der Semantik von 
Wörtern und Sätzen der Klassifizierung von Texten auseinander.

\subsubsection{Data Mining}
 Data Mining ist die systematische Anwendung statistischer Methoden auf grosse Datenbestände mit dem Ziel, neue Querverbindungen und Trends zu erkennen.

\subsubsection{Text Mining}
 Text Mining ist eine Untermethode von Data Mining und konzentriert sich vor allem auf Data Mining von grossen Texten wie z.B. die Plagiaterkennung, die gesamte Doktorarbeiten durchliest und analysiert.
 
 \subsubsection{Klassifikation}
 Klassifikation ist eine Unterart des Data-Minings und versucht, den Datenbestand in vorgegebene Klassen zuzuordnen.
 
 \subsubsection{Regression}
 Die Regression ist eine Unterart des Data-Minings und zielt darauf ab, einen Ursache-Wirkung-Zusammenhang zwischen einzelnen Merkmalen der zugrunde liegenden Datenbasis zu ermitteln. 
 
 Am Beispiel von Abbildung \ref{fig:regclass} kann Klassifikation vs. Regression so unterschieden werden: Mittels \textbf{Klassifikation} kann ermittelt werden, dass ca. die Hälfte der untersuchten Gene krank sind, während mit der \textbf{Regression} gezeigt werden kann, dass, je höher die Konzentration von Gen 1 ist, desto länger leben die Patienten (Trend)
 
 \begin{figure}[htb]
 	\centering
 	\includegraphics[keepaspectratio=true,height=15\baselineskip]{regressionvsclassification.png}
 	\caption{Klassifikation (links) vs. Regression (rechts)}
 	\label{fig:regclass}
 \end{figure}

\newpage

\subsubsection{Abhängigkeitsentdeckung / Assoziationsanalyse}
Die Abhängigkeitsentdeckung oder auch Assoziationsanalyse wird verwendet, um Zusammenhänge von Merkmalen und Muster und/oder Abhängigkeiten in grossen Datenbeständen zu finden. 

Ein Beispiel davon ist die \textbf{Warenkorbanalyse}. Es soll gezeigt werden, dass Kunden, die Zahnbürsten kaufen, mit einer grossen Wahrscheinlichkeit auch Zahnpasta kaufen.
\vspace{10px}

\noindent Die Analyste geschieht auf Basis von zwei Werten. Dem \textbf{Support} und der \textbf{Confidence}. Der Support ist die Wahrscheinlichkeit, dass der Kunde sowohl X, wie auch Y kauft. Die Confidence ist die Wahrscheinlichkeit, mit welcher der Kunde noch Y kauft, nachdem er bereits X im Warenkorb hat.
\vspace{10px}

\noindent Mathematisch kann das so ausgedrückt werden:

\begin{equation}
confidence({X} \rightarrow {Y}) = \dfrac{support({X, Y})}{support({X})}
\end{equation}

Ein ebenfalls wichtiger Begriff ist der \textbf{Lift}. Er beschreibt die Abhängkeit zwischen der Zahnbürste und der Zahnpasta im Warenkorb.

\begin{description}
	\item[Lift = 1: ] X und Y werden unabhängig voneinander gekauft.
	\item[Lift $>$ 1: ] Es besteht eine positive Abhängigkeit. Wenn jemand X kauft, kauft er wahrscheinlich auch Y.
	\item[Lift $<$ 1: ] Es besteht eine negative Abhängigkeit. Wenn jemand X kauft, ist es unwahrscheinlich, dass er auch Y kauft.
\end{description}

\paragraph{Beispiel}
Es soll die Abhängigkeit zwischen dem Verkauf von \textbf{Zahnbürsten} und \textbf{Zahnpasta} überprüft werden.

\begin{itemize}
	\item 20\% aller Kunden kaufen sowohl eine Zahnbürste, wie auch Zahnpasta.
	\item 10\% aller Kunden kaufen nur eine Zahnbürste.
	\item 40\% aller Kunden kaufen nur Zahnpasta
\end{itemize}

\begin{equation}
lift(Zahnbuerste \rightarrow Zahnpasta) = \dfrac{support({Zahnbuerste \rightarrow Zahnpasta})}{support(Zahnbuerste) * support(Zahnpasta)}
\end{equation}

Mit den oben genannten Zahlen eingesetzt ergibt dies:

\begin{equation}
	lift(Zahnbuerste \rightarrow Zahnpasta) = \dfrac{0.2}{0.1 * 0.4} = 5
\end{equation}

Der Lift ist also wesentlich höher wie 1, woraus geschlossen werden kann, dass eine \textbf{starke Abhängigkeit} besteht zwischen dem Kauf von Zahnbürsten und Zahnpasta.
 
 \newpage
 
 \subsubsection{Clustering}
 
  \begin{figure}[htb]
 	\centering
 	\includegraphics[keepaspectratio=true,height=12\baselineskip]{clustering.png}
 	\caption{Partitionierendes vs. Hierarchisches Clustering}
 	\label{fig:Clustering}
 \end{figure}
 
 Clustering ist ähnlich wie die Klassifikation, allerdings werden beim Clustering-Verfahren automatisch passende Klassen erstellt. Das Clustering-Verfahren zielt darauf ab, Gruppen oder Klassen zu erstellen, innerhalb von welchen sich die Objekte sehr ähnlich sind. Gleichzeitig sollten sich die erstellten Gruppen aber möglichst stark voneinander unterscheiden.
 
 Clustering wird normalerweise nur zu Beginn ausgeführt, um Klassen zu erstellen, mit welchen anschliessend eine Klassifikation durchgeführt werden kann.
 
 \vspace{10px}
 
 \noindent Es wird generell zwischen dem \textbf{partitionierenden} Clustering und dem \textbf{hierarchischen} Clustering unterschieden (Abbildung \ref{fig:Clustering}):
 
 \paragraph{Partitionierendes Clustering} unterteilt die gegebenen Daten in eine \textbf{vorgegebene Anzahl} von Klassen. Dabei werden zuerst die vorgebene Anzahl Clusterzentren/Klassen aus den Datensätzen ausgewählt. Anschliessend werden die restlichen Datensätze diesen Clusterzentren zugeordnet. 
 
 Nach dem ersten Durchgang werden erneut Clusterzentren/Klassen gewählt und die Daten werden wiederum diesen, neuen, Clusterzentren zugeordnet. Dieses ganze Spiel wird solange wiederholt, bis die Daten 'optimal' geclustert sind (möglichst ähnlich innerhalb der Klasse/des Clusters, möglichst verschieden zwischen den Klassen/Cluster).
 
   \begin{figure}[htb]
 	\centering
 	\includegraphics[keepaspectratio=true,height=12\baselineskip]{hierarchical_clustering.png}
 	\caption{Beispiel von agglomerativem (oben) und diversiven (unten) clustering}
 	\label{fig:hierCluster}
 \end{figure}
 
 \paragraph{Hierarchisches Clusteriong} kann wiederum in \textit{agglomeratives} und \textit{diversives} Clustering unterteilt werden. (Siehe Abbildung \ref{fig:hierCluster})
 
 Das \textit{agglomerative} Clustering funktioniert so, dass jeder Datensatz eine eigene Klasse/ein eigenes Cluster ist. Anschliessend werden die \textbf{zwei ähnlichsten Klassen zu einer zusammengefasst}. Dieses Zusammenfassen zweier ähnlicher Klassen wird solange durchgeführt wird, bis alle Daten in einem einzigen Cluster zusammengeführt sind.
 
 \vspace{10px}
 
 \noindent Beim \textit{diversiven} Clustering geht es in die andere Richtung. Die Datensätze werden bei jeder Iteration so halbiert, dass die Elemente innerhalb eines Clusters sich so ähnlich wie möglich sind, die Cluster untereinander aber möglichst unterschiedlich zueinander sind ('optimale' Cluster also). Diese Unterteilung wird solange fortgesetzt, bis jeder Datensatz seinen eigenen Cluster hat. Dieses Verfahren wird in der Praxis aber eher weniger angewandt.
 
 \subsubsection{Visualisierungstechnik}
 \begin{wrapfigure}[10]{L}{0.6\textwidth}
 	\centering
 	\includegraphics[keepaspectratio=true,height=13\baselineskip]{streudiagramm.PNG}
 	\caption{Daten visuell aufbereitet mittels eines Streudiagrammes}
 	\label{fig:streu}
 \end{wrapfigure}
 Bei der Visualisierungstechnik überlässt man die tatsächliche Auswertung dem Menschen. Die Daten werden mittels \textbf{zwei bis drei Merkmalen} im \textbf{zwei- oder dreidimensionalen} Raum dargestellt (z.B. Streudiagramm, siehe Abbildung \ref{fig:streu}). Es ist nun dem Menschen überlassen, aus dieser Grafik etwas nützliches herauszulesen.
 
 \vspace{90px}

\subsubsection{Fallbasierte Systeme}

\begin{wrapfigure}[12]{R}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=12\baselineskip]{fallbasiert.png}
	\caption{Funktionsweise eines Fallbasierten Systemes}
	\label{fig:fallbasiert}
\end{wrapfigure}

Solche Systeme greifen auf Daten von ähnlichen, bereits gelösten Problemen zurück. Es gibt eine \textbf{Falldatenbank} gefüllt mit bereits gelösten, getaggeden oder anderswie codierten Problemen, auf welche zugegriffen werden kann. (Siehe Abbildung \ref{fig:fallbasiert})

Dieses System beruht auf der Annahme, dass geringe Änderungen an einer Problemstellung auch nur geringe Änderungen am Lösungsweg zur Konsequenz habenn (also \textbf{lineare Probleme}). Somit sind fallbasierte Systeme \textit{nicht} geeignet, um komplexe Probleme zu lösen.

\newpage

\subsubsection{Entscheidungsbaumverfahren (Buch S136)}
Ein Entscheidungsbaum ist die \textbf{grafische Darstellung einer Datenteilung} (z.B. mittels Clustering). Dabei stellen die nicht-Blatt Knoten des Baumes die Merkmale der Daten dar, nach denen der Datensatz geteilt wird. Über die Kanten werden diese Merkmale verbunden. Die Teildatenbestände (z.B. Cluster oder Klassen) werden schlussendlich in den Blättern dargestellt.

\vspace{10px}

\noindent Ein solcher Baum wird mittels des \textbf{Entscheidu9ngsbaumverfahrens}. Zuerst wird ein Merkmal bestimmt, nach welchem sortiert werden soll (in Abbildung \ref{fig:tree} wäre dies z.B. ob \\ 
$Schalengewicht <= 0.168$ und  $MSE = 10.273$ ist) und es wird danach sortiert. Dieser Vorgang wird nun wiederholt (dieses mal ist z.B. das Merkmal links $Schalengewicht <=0.059$ und $MSE = 4.635$ ist).

Der Vorgang wird nun solange wiederholt, bis alle Datensätze zu einem Cluster gehören, oder sich kein Merkmal mehr finden lässt, durch welches geteilt werden kann.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=13\baselineskip]{tree.png}
	\caption{Beispiel eines Entscheidungsbaumes}
	\label{fig:tree}
\end{figure}

Um Bäume zu verbessern, die eine hohe Fehlerquote aufweisen, kann \textbf{Pruning} verwendet werden. Dabei werden einzelne Knoten und Kanten des Baumes entfernt, um den Baum zu verkleinern und somit die Teilgenauigkeit des Baumes insgesamt zu erhöhen.

\vspace{10px}

\noindent Entscheidungsbäume werden zum Beispiel zur \textbf{Klassifizierung} von Daten hinzugezogen werden, um die Klassen einfacher und übersichtlicher darstellen zu können.

\newpage

\subsection{Architekturen (Skript S84/Buch S163ff)}
\subsubsection{Keine physische Integration}
Strukturierte und unstrukturierte Daten existieren \textbf{parallel} in verschiedenen Speicherungsmöglichkeiten. Diese Daten werden nur verknüpft, falls beide für eine Auswertung benötigt werden.

\subsubsection{Pysische aber nicht logische Integration}
Strukturierte und unstrukturierte Daten werden zwar in der gleichen Speicherungsmöglichkeit abgelegt, sind jedoch nicht verknüpft.

Unstrukturierte Daten werden oft als BLOB (Binary Large Object) gespeichert. BLOBS sind prinzipiell einfach riesige Binär-Dateien, in welche alle unstrukturierten Daten (Blogposts, Bewertungen, YouTube Videos etc.) reingeschmissen werden.

In relationalen Datenbanken werden BLOBS oft extern gespeichert. Das heisst, die Datenbank legt nur eine Referenz ab, wo das BLOB gefunden werden kann, sollte es gebraucht werden.

Je nach dem welcher DB-Engine verwendet wird, muss eine BLOB-Spalte in einer DB-Tabelle als \textit{BLOB, LONGBLOB, LONGRAW} oder auch \textit{BYTEA} definiert werden.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{blob}
	\caption{Beispiel von BLOBs und strukturierten Daten in der selben DB}
	\label{fig:blob}
\end{figure}

\newpage

\subsubsection{Physische und logische Integration}
\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{unstruktdata.PNG}
	\caption{Die drei verschiedenen Methoden zur Integration von unstrukturierten Daten.}
	\label{fig:logInt}
\end{figure}

\noindent {\color{cyan}\textbf{Extraktion von unstrukturierten Daten}} geschieht meist mittles NPL und UIMA (Unstructured Information Management Architecture). Dabei muss darauf geachtet werden, dass zwar alle unnützen Formatierungen entfernt werden, so dass die Daten gesäubert und integriert werden können, jedoch die wichtigen Formatierngen beibehalten werden (wie z.B. HTML-tags für title o.ä)

\vspace{10px}

\noindent {\color{cyan}\textbf{Transformation von unstrukturierten Daten}} hat fast noch eine grössere Bedeutung als die Bereinigung von strukturierten Daten. Der Prozess besteht hauptsächlich daraus, Synonyme, Homonyme und Ähnlichkeit zu erkennen, irrelevante Daten wie Füllwörter oder Formatierungszeichen zu entfernen und/oder Daten in andere Sprachen zu übersetzen.

Für diesen Task werden Werkzeuge wie z.b. der DataWrangler, Open Refine oder DataCleaner verwendet, oder aber auch NLP-Werkzeuge wie GATE, UIMA oder Stanford's Core NLP Suite.
\vspace{10px}

\noindent {\color{cyan}\textbf{Laden von unstrukturierten Daten}}
\begin{description}
	\item[a) / Basisdatenbank] Die Daten werden bereits im Datenbeschaffungsprozess analysiert, aufbereitet und strukturiert bevor sie in der DB abgelegt werden. 
	
	Eine Verwendung der unstrukturierten Daten ist nach Ablegung nicht mehr möglich.
	\item[b) / Data Warehouse] Die Daten werden möglichst ohne Informationsverlust in der Basis-DB/ODS abgelegt und erst anschliessend im Data-Warehouse strukturiert.
	\item[c) / Hybrid-Architektur: ] Die strukturierten und unstrukturierten Daten werden parallel in der Basis-DB abgelegt.
\end{description}

\newpage

\subsubsection{Beispiel}
\textbf{Aufgabenstellung: } Folgende E-Mail soll in ein Data-Warehouse integriert werden.

\begin{lstlisting}
Subject: Beschwerde
Von: peer.haber@bluewin.ch
Gesendet: Di. 18.10.2016 15:27
An: Customer Service
______________________________________________________
Geehrtes Support Team
leider bin ich mit dem Vertrag vom 21.10.2016 nicht zufrieden. sie haben mir 
am Telefon erklaert, dass im Vertrag 5GB Daten inbegriffen sind. 
Leider ist dem nicht so. Bitte kontaktieren Sie mich unter folgender Nummer:

+41 79 678 23 63

mit freundlichen Gruessen
Peer Haber
peer.haber@bluewin.ch

\end{lstlisting}

Die Mail soll in die Architektur 3a) integriert werden. Dies geschieht in vier Schritten:

\begin{enumerate}
	\item Extraktion der Daten aus den jeweiligen Ursprungsformaten $\rightarrow$ Extraktion
	\item Analyse der Daten $\rightarrow$ Transformation
	\item Datenbereinigung $\rightarrow$ Transformation
	\item Laden der Daten in ein Datenbanksystem des Wata-Warehosues $\rightarrow$ Laden
\end{enumerate}

\paragraph{Schritt 1 - Extraktion} \mbox{}\\
Der Text des E-Mails wird aus dem Ursprungsformat extrahiert:

\begin{lstlisting}
Geehrtes Support Team
leider bin ich mit dem Vertrag vom 21.10.2016 nicht zufrieden. sie haben mir 
am Telefon erklaert, dass im Vertrag 5GB Daten inbegriffen sind. 
Leider ist dem nicht so. Bitte kontaktieren Sie mich unter folgender Nummer:

+41 79 678 23 63

mit freundlichen Gruessen
Peer Haber
peer.haber@bluewin.ch
\end{lstlisting}

\newpage

\paragraph{Schritt 2 - Analyse der Daten} \mbox{} \\
Der extrahierte Text wird mittels Data Mining oder NLP analysiert:
\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{NLP_email.PNG}
	\caption{E-Mail-Analyse mit dem Text-Mining Tool UIMA}
	\label{fig:nlpemail}
\end{figure}

Die \textit{semantische Auswertung} der oben gezeigten E-Mail zeigt folgendes:

\begin{description}
	\item[Kategorisierung: ] Kategorie: Beschwerde
	\item[Auswertung Freitexteingabe: ] Unzufriedener Kunde.
	\item[Auswertung Dokument: ] Analyse des Vertrages, ob Kunde recht hat.
\end{description}

\paragraph{Schritt 3 - Datenbereinigung}
\begin{itemize}
	\item Erkennen von Synonymen und Homonymen
	\item Beseitigung von irrelevanten Daten (z.B. Füllwörtern)
	\item Evtl. Übersetzung in andere Sprachen.
\end{itemize}

\noindent \sout{Geehrtes} \textbf{Support Team},

\noindent \sout{leider bin ich mit dem} \textbf{Vertrag vom 21.10.2016 nicht zufrieden}. \sout{Sie
haben mir am} \textbf{Telefon}\sout{ erklärt, dass im} \textbf{ Vertrag 5 GB Data} \sout{inbegriffen
sind. Leider ist dem nicht so. Bitte kontaktieren Sie mich unter folgender Nummer:}

\noindent \textbf{+41 79 678 23 63}

\noindent \sout{mit freundlichen Grüssen}

\noindent \textbf{Peer Haber}

\noindent \textbf{peer.haber@bluewin.ch}

\paragraph{Schritt 4 - laden der Daten in ein Datenbanksystem des Data-Warehouses}\mbox{} \\

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=3.5\baselineskip]{haber_peer.PNG}
\end{figure}

\section{Informations- und Datenqualität (Skript S95)}
\subsection{Definition}
\blockquote[DIN 55350/1995]{Qualität ist die Gesamtheit von Eigenschaften und Merkmalen eines Produktes oder einer Tätigkeit, die sich auf deren Eignung zur Erfüllung festgelegter oder vorausgesetzter Erfordernisse beziehen.}
\vspace{10px}

\noindent Eine weitere Definition nach Cordts 2009 lautet: \textit{Die Datenqualität ist der Grad der Eignung von Daten zur Erfüllung eines bestimmten Verwendungszweck.}

Solche Erfordernisse sind zum Beispiel:
\begin{itemize}
	\item Korrektheit
	\item Vollständigkeit
	\item Relevanz
	\item Konsistenz
	\item Aktualität
\end{itemize}

\subsection{Ursachen und Orte von Qualitätsmängel}
Schätzungsweise 10-20\% aller Daten in einem operativen System sind fehlerhaft. Diese Daten zu verbessern nimmt ca. 80\% des Aufwands im ETL-Prozess in Anspruch.

Schlechte Datenqualität kann folgende Ursachen haben:

\begin{description}
	\item[Schlechte Datenerfassung: ] Durch menschliches Versagen in der Dateneingabe/Datenerfassung; Falsch (oder gar nicht) geschulter Mitarbeiter.
	\item[Schlechte Prozesse: ] Durch mangelhaft definierte Abläufe und Zuständigkeiten
	\item[Schlechte Architektur: ] Durch unsaubere/überhastete Systemwechsel/Migrationen oder infolge von Sparmassnahmen am falschen Ort.
	\item[Schlechte Definitionen: ] Durch mangelhafte/fehlende Planung oder Sorgfalt. Durch mangelhafte/fehlende Doku.
\end{description}

\begin{figure}[htb!]
	\centering
	\includegraphics[keepaspectratio=true,height=13\baselineskip]{data_quality.PNG}
	\caption{Beispiel von schlechter Datenqualität}
\end{figure}

\newpage

\subsection{Messbarkeit von Datenqualität}
Momentan gibt es noch keine akzeptierte Metrik für die Beurteilung der Datenqualität. Allerdings gibt es Grafiken zur Visualisierung des "sweet spot" zwischen überhöhten Kosten durch schlechte Datenquaität und überhöhten Kosten durch zu hohe Investitionen in die Verbesserung der Datenqualität (Abb. \ref{fig:costQuality}). 

\begin{figure}[htb!]
\centering
\begin{minipage}{.45\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{messungDatenQualitaet.PNG}
	\captionof{figure}{"Messung" der Datenqualität}
	\label{fig:measureQuality}
\end{minipage}
\begin{minipage}{.45\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{kostenNutzung.PNG}
	\captionof{figure}{Kosten-Qualitäts Gegenüberstellung}
	\label{fig:costQuality}
\end{minipage}
\end{figure}

\begin{wrapfigure}[17]{R}{0.6\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{dgiq.PNG}
\end{wrapfigure}

\noindent Zudem hat die DGIQ (Deutsche Gesellschaft für Informations- und Datenqualität) 15 Dimensionen der Datenqualität definiert. Diese sind in vier Kategorien eingeteilt. Diese Einteilung basiert auf einer Studie des MITs.

\color{system}{
\begin{description}
	\item[Zugänglichkeit: ]  Informationen sind  \\ zugänglich, wenn sie anhand einfacher Verfahren auf direktem Weg für den Anwender abrufbar sind. 
	\item[Bearbeitbar: ]  Informationen sind leicht bearbeitbar, wenn sie leicht zu ändern/für unterschiedliche Zwecke zu verwenden sind. 
\end{description}
}

\color{inhalt}{
\begin{description}
	\item[Hohes Ansehen: ]  Informationen sind hoch angesehen, wenn die Informationsquelle, das Transportmedium und das verarbeitenden System im Ruf einer hohen Vertrauenswürdigkeit und Kompetenz stehen.
	\item[Fehlerfrei: ] Informationen sind fehlerfrei, wenn sie mit der Realität übereinstimmen 
	\item[Objektiv: ] Informationen sind objektiv, wenn sie streng sachlich und wertfrei sind.
	\item[Glaubwürdig: ] Informationen sind glaubwürdig, wenn Zertifikate einen hohen Qualitätsstandard ausweisen oder die Informationsgewinnung und -verbreitung mit hohem Aufwand betrieben werden.
\end{description}
}

\color{darstellung}{
\begin{description}
	\item[Eindeutig auslegbar: ]  Informationen sind eindeutig auslegbar, wenn sie in gleicher, fachlich korrekter Art und Weise begriffen werden.
	\item[Einheitlich dargestellt: ] Informationen sind einheitlich dargestellt, wenn die Informationen fortlaufend auf dieselbe Art und Weise abgebildet werden.
	\item[Übersichtlich]  Informationen sind übersichtlich, wenn genau die benötigten Informationen in einem passenden und leicht fassbaren Format dargestellt sind.
	\item[Verständlich] Informationen sind verständlich, wenn sie unmittelbar von den Anwendern verstanden und für deren Zwecke eingesetzt werden können
\end{description}
}

\color{nutzung}{
\begin{description}
	\item[Relevant: ]  Informationen sind relevant, wenn sie für den Anwender notwendige Informationen liefern.
	\item[Angemessener Umfang: ] Informationen sind von angemessenem Umfang, wenn die Menge der verfügbaren Information den gestellten Anforderungen genügt.
	\item[Vollständig: ]  Informationen sind vollständig, wenn sie nicht fehlen und zu den festgelegten Zeitpunkten in den jeweiligen Prozess-Schritten zur Verfügung stehen.
	\item[Wertschöpfend: ] Informationen sind wertschöpfend, wenn ihre Nutzung zu einer quantifizierbaren Steigerung einer monetären Zielfunktion führen kann.
	\item[Aktuell: ] Informationen sind aktuell, wenn sie die tatsächliche Eigenschaft des beschriebenen Objektes zeitnah abbilden
\end{description}
}

\newpage

\color{black}
\subsection{Verbesserung der Datenqualität}

\begin{wrapfigure}[17]{R}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{dataImprovement.PNG}
	\caption{Vorgehensmodell zur Datenqualitätsverbesserung}
\end{wrapfigure}

Die Verbesserung der Datenqualität kann in drei Phasen aufgeteilt werden, die wiederum in mehrere Tasks unterteilt sind.

\begin{enumerate}
	\item Daten verstehen
		\subitem Data Identification
		\subitem Data Profiling
	\item Daten verbessern
		\subitem Data Standardisation
		\subitem Data Enrichment
		\subitem Data Deduplication
	\item Daten Steuern
		\subitem Data Monitoring
\end{enumerate}

\subsubsection{Daten verstehen}
Was für Daten liegen vor?, Welche Daten kann man benutzen? Wo liegen diese Daten? Wer hat die Rechte auf diese Daten zuzugreifen? Wer kann die Daten für die weitere Verwendung "freigeben"? Was bedeuten diese Daten? In welchem Kontext stehen diese Daten?

\paragraph{Data Identification}\mbox{}\\
\textit{Woher kommen unsere Daten?} Durch die Auswertung der Metadaten kann einiges ermittelt werden, wie zum Beispiel die Art des Quellsystems.

\begin{itemize}
	\item Wie und von wem wurden die Daten erstellt?
	\item Welche Benutzer greifen am häufigsten zu?
	\item Für welche Zwecke werden die Daten vor allem eingesetzt?
	\item In welchem Zustand befinden sich die Daten?	
\end{itemize}

Bei der Data Identification werden die benötigten Daten identifiziert (wie das der Name schon sagt), ihr Ist-Zustand beschrieben und spezifiziert. 

Das Ziel der Data Identification ist es, schlussendlich eine Metadatensammlung vorliegen zu haben und anhand dessen erste Angaben zur Datenqualität machen zu können.

\newpage

\paragraph{Data Profiling} \mbox{}\\
Nachdem im vorherigen Schritt der Ist-Zustand der Daten beschrieben wurde, wird im Data-Profiing der Soll-Zustand beschrieben und spezifiziert. Das heisst, es werden Kriterien definiert wie zum Beispiel

\begin{itemize}
	\item Benennungen
	\item Datentypen
	\item Domänen
	\item Minima und Maxima
	\item Kardinalitäten (Beziehungen zwischen Tabellen)
	\item Muster
	\item Fehlertoleranz
	\item Geschäftsregeln
\end{itemize}

Ausserdem werden Beziehungen von Daten untereinander festgehalten. 

Jede Abweichung des Ist-Zustands vom definierten Soll-Zustand ist ein Qualitätsmangel. Somit kann genauer eingeschätzt werden, in welchem Gesamtzustand sich die Daten befinden. Dies wiederum ermöglicht es, Fehler zu ermitteln, priorisieren und anschliessend die Ursache zu beheben.

\vspace{10px}

\noindent Es gibt grundsätzlich drei verschiedene Arten des Data-Profiling:

\begin{description}
\item[deskriptiv/beschreibend: ] Analyse von Häufigkeiten, Abhängigkeitsanalysen, Ausreisser-Tests etc.
\item[kognitiv/lernend: ] Regelinduktionen, Segmentierungen oder Klassifizierungen
\item[deduktuv/ableitend: ] (Geschäfts-)Regelanalyse
\end{description}

Eine oder mehrere dieser Data-Profiling Methoden wird auf sämtliche Datenobjekte des Systems sowie deren Beziehungen untereinander angewandt. Man untersucht die Daten einer Spalte auf die oben genannten Kriterien und kann somit Rückschlüsse auf die Datenqualität ziehen ($\rightarrow$ Column Profiling)

Ebenfalls untersucht man die Abhängigkeiten von mehreren Spalten in einer Tabelle und kann somit z.B. überprüfen, in welcher Normalform sie sich befinden oder wie viele Fremdschlüssel in der Tabelle vorhanden sind ($\rightarrow$ Multi Column Profiling) 

\vspace{10px}

\noindent Das Ziel des Data-Profilings ist es, ein sogenanntes Soll-Metadatenreqpositorium vorliegen zu haben. Das heisst soviel wie, es soll eine Spezifikation vorliegen, wie die Metadaten der Daten aussehen müssen, bevor sie ins Zielsystem integriert werden können. Ausserdem liegt nach dem Data Profiling normalerweise ein detaillierteres Verständnis der vorliegenden Daten vor, da man sich eingehend mit ihnen beschäftigt hat.

\newpage

\subsubsection{Daten verbessern}
Je gründlicher das Data Profiling durchgeführt wird, desto zuverlässiger und besser ist die Bereinigung. je besser de Bereinigung, desto höher die Datenqualität.

\paragraph{Data Standardisation}\mbox{}\\
Daten müssen vereinheitlicht werden. Vereinheitlichung heisst meist auch Homogenisierung. Daten müssen in ein einheitliches Format und eine einheitliche Semantik gebracht werden.

Data Standardisation beinhaltet vor allem folgende Themen:

\begin{description}
	\item[Vervollständigen: ] z.B. fehlende Vornamen nachtragen
	\item[Synonyme entfernen: ] Mehrere Schreibweisen eines Wortes mit der gleichen Bedeutung 
		\subitem Fa. vs. Firma
		\subitem unverheiratet vs. ledig
		\subitem Herr vs. Hr. vs. Mr.
		\subitem etc.
	\item[Homonyme entfernen: ] Eine Schreibweise eines Wortes mit mehreren Bedeutungen
		\subitem Peter (Vor- oder Nachname?)
		\subitem Kim (männlich oder weiblich?)
		\subitem Lieferung (Von hier oder zu hier?)
		\subitem etc...
	\item[Felder zusammennehmen: ] Strasse und Hausnummer gehören zusammen
	\item[Felder aufteilen: ] Vor- und Nachname erhalten je ein eigenes Feld
	\item[Rauschdaten eliminieren: ] Entweder Jahrgang \textbf{oder} Alter. Beides ist überflüssig
	\item[Rechtschreibung: ]  Straße / Strasse oder Brun-Brücke / Brunbrücke 
	\item[Einheitliche Darstellung: ] +41 79 vs. 0079 / 03.02.2018 vs. 2018-02-03
	\item[Eliminatin von Stoppwörtern: ] z.B. der, die, das, und, oder, doch
	\item[Einheitliche Abwandlungsformen: ]  Konjugation und Deklination
		\subitem \textbf{Konjugation: }z.B. Welche Personenform wird genommen? (Er hat am 03.02 Geburtstag vs. Ich habe am 03.02 Geburtstag)
		\subitem \textbf{Deklination: }Welche Fälle werden verwendet? (die schönen Häuser vs. der schönen Häuser)
	\item[Einheitliche Einheiten: ] Nur metrisch oder nur imperial
\end{description}\footnote{Stoppwörter nennt man Wörter, die bei einer Volltextindizierung nicht beachtet werden, da sie sehr häufig auftreten und keine Relevanz für die Erfassung der Daten besitzen.}

Data Standardisation wird häufig mittels linguistischen und algorithmischen Verfahren durchgeführt. Dabei werden Referenzdaten gesammelt, die anschliessend gepflegt werden müssen, dass sie später wieder angewendet werden können.

Das Zeil der Data Standardisation ist es, Daten in einer strukturell und inhaltlich direkt vergleichbaren Einheit vorliegen zu haben.

\paragraph{Data Enrichment}\mbox{}\\
Nachdem in der Data Standardisation unnötige Informationen entfernt worden sind, werden im Schritt des Data Enrichment zusätzliche, nützliche Informationen hinzugefügt, die zukünftige Analysen vereinfachen sollen. Dies können z.B. sein:

\begin{description}
	\item[Demographische Daten: ] Nationalität, Herkunft, Schulbildung, Einkommen etc.
	\item[Geographische Daten: ] Land, Provinz, Bevölkerungsidchte etc. (Geodaten helfen oft bei der Deduplizierung von Daten)
	\item[Produktcode: ] EAN / EPC / ISBN etc. von Produkten
	\item[Kundeninformationen: ] Bestell- und Zahlungsverhalten, Interessen etc.
	\item[Tags: ] z.B. "Spätzahler", "Tierliebhaber", "Technologiebegeistert"
	\item[Umsatz: ] Umsatz, der der Kunde generiert.
	\item[Sumindex: ] Summe aller Bestellungen, die der Kunde im Laufe einer Zeitspanne aufgegeben hat.
\end{description}

Das Ziel des Data Enrichments ist es, Daten in so einem Format vorliegen zu haben, die die Suche nach Dubletten vereinfacht. Ebenfalls können mit diesen angereicherten Daten bessere Analysen durchgeführt werden. Data Enrichment hat überhaupt keinen Einfluss auf die Datenqualität, jedoch kann die Datenmenge erheblich ansteigen.

\paragraph{Data Deduplication}\mbox{}\\
Dubletten oder auch Duplikate sind Datensätze, die zwar syntaktisch unterschiedlich sind, semantisch jedoch dasselbe meinen. Es wird zwischen zwei Arten von Dubletten unterschieden: Den \textit{vollständig identischen} und den \textit{sich bis zu einem gewissen Grad ähnlichen} Dubletten.

Dubletten sind redundant und verstossen somit gegen die Information Quality Dimensionen. Ursachen für Dubletten können z.B. ein Fehler bei der Data Standardisation sein (Grösse sowohl in inch wie auch in cm angegeben), oder auch nicht aktuelle Daten (Kunde zweimal erfasst nachdem er umgezogen ist, einmal mit der neuen und einmal mit der alten Adresse).

Es gibt jedoch auch Fälle, da sind Dubletten gewollt. Das kann z.B. sein bei einer Mehrfachablage für eine Verlosung, oder aber auch bei Gratis Trials von Online-Diensten (ich bin einmal Peter Müller und nach dem Ablauf des Gratis Trials bin ich plötzlich Sandro Hüsler, aber eigentlich immer noch die selbe Person)

Dubletten können Anomalien und Fehlern in Analysen und Prognosen auslösen oder können in grosser Anzahl auch zu einem Ressourcenproblem führen. Deshalb sollten sie so gut wie möglich beseitigt werden. Diese Beseitigung findet in zwei Teilschritten statt:

\begin{enumerate}
	\item Dublettenerkennung Dies wird meist mittels Clustering durchgeführt. Die gefundenen möglichen Dubletten werden aufgelistet und auf Redundanz untersucht.
	\item Eliminierung und/oder Merging von Dubletten (Siehe Abb. \ref{fig:dubl})
\end{enumerate}

\begin{figure}[htb!]
	\centering
	\includegraphics[keepaspectratio=true,height=14\baselineskip]{DublettenBeseitigung.jpg}
	\caption{Prozess der Dublettenbeseitigung als iterativer Prozess}
	\label{fig:dubl}
\end{figure}

\noindent Mehrere Arten der Dublettenerkennung und -entfernung finden sich im Skript auf S120ff.

\newpage

\subsubsection{Daten steuern}
Da sich Daten immer wieder ändern, können Sie sich auch ungewollt verschlechtern. Ein System- und/oder Datenqualitätsowner muss da dauernd ein Auge darauf haben.

\paragraph{Data Monitoring}\mbox{}\\
Wie bei den letzten paar Themen klar geworden sein sollte, ist die Datenqualitätspflege ein iterativer Prozess. Das Monitoring begleitet diesen Prozess und dokumentiert und misst diesen.

Aufgaben des Data Monitorings sind z.B. 
\begin{itemize}
	\item Messen und Steuern der Datenqualitätskennzahlen ($\rightarrow$ Data Auditing)
	\item Alarmierung bei Regelverletzung
	\item Erkennen von (Daten-)Trends
\end{itemize}

Das Ziel des Data Monitorings ist es, ein Höchstmass von Wissen und Bewusstsein für den momentanen Stand der Datenqualität zu haben.

\newpage

\section{Historisierung (Skript S144/Buch S195)}
Unter der Historisierung versteht man, dass man nicht nur aktuelle Datenbestände verwaltet, sondern auch veraltete Daten. Wenn also z.B. ein Mitarbeiter eine Lohnerhöhung erhält, so wird der alte Lohn nicht überschrieben, sondern beide Einträge existieren parallel.

Es existieren verschiedene Methoden zur Historisierung. In diesem Modul wird hauptsächlich die SCD-Versionierung behandelt. SCD steht für Slowly Changing Dimensions. Es gibt die SCD-Typen 0, 1, 2 und 3. Wir werden uns ebenfalls kurz mit der Tupelversionierung auseinandersetzen.

\subsection{SCD Typ 0 - Keine Historisierung, keine Aktualisierung}
Es wird \textbf{keine Historisierung} durchgeführt. Es wird überprüft, ob unter diesem Primärschlüssel bereits ein Datensatz existiert. Falls dem so ist, \textbf{wird nichts gemacht}. Ansonsten wird der neue Datensatz in die Datenbank eingefügt.

\vspace{10px}

\noindent Es werden also nur neue Daten hinzugefügt, bereits existierende Datensätze werden jedoch \textbf{nicht} verändert.

\subsection{SCD Typ I - Keine Historisierung, Aktualisierung}
Es wird ebenfalls\textbf{ keine Historisierung} durchgeführt. Im Gegensatz zu SCD Typ 0 sind Aktualisierungen jedoch möglich. Erhält ein Mitarbeiter also eine Lohnerhöhung von CHF 5'000 und sein Lohn steigt von CHF 10'000 auf CHF 15'000, dann wird der alte Lohn in der Datenbank mit dem neuen Lohn überschrieben. Es gibt kein Zeichen mehr davon, dass der Mitarbeiter jemals nicht CHF 15'000 verdient hat.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=4\baselineskip]{scd_type1.png}
	\caption{Beispiel einer Aktualisierung.}
	\label{fig:scd1}
\end{figure}

Es muss jedoch darauf geachtet werden, dass z.B. persistente Datenaggregate (z.B. Views), die mit dem alten Datensatz zusammenhängen nun evtl. nicht mehr korrekt sind und somit neu berechnet werden müssen.

\subsection{SCD Typ II - Vollhistorisierung}
Diese Methode findet am meisten Anwendung. Es wird \textbf{nichts überschrieben}. Stattdessen werden zwei Extra-Spalten \code{dat-von} und \code{dat-bis} unterhalten, in denen festgehalten wird, in welchem Zeitraum dieser Datensatz gültig ist.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=5\baselineskip]{scd_type2.png}
	\caption{Beispiel einer Vollhistorisierung.}
	\label{fig:scd2}
\end{figure}

\noindent Im Gegensatz zu SCD Typ 1 sind persistente Datenaggregate immer noch korrekt, da der alte Datensatz ja noch existiert. Sofern sie mit den korrekten Versionen verbunden sind zumindest.


\subsection{SCD Typ III - Historisierung mit neuem Attribut (Spalte)}
Bei jeder Aktualisierung eines Datensatzes wird der gesamten Tabelle eine neue Spalte angehängt, in welche der alte Wert geschrieben wird.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=4.5\baselineskip]{scd_type3.png}
	\caption{Beispiel einer Historisierung mit neuer Spalte.}
	\label{fig:scd3}
\end{figure}

Bei dieser Art der Historisierung wird jedoch meistens nur der letzte alte Wert gespeichert. Sollte die Acme Aupply Co nun also wieder in einen neuen Staat ziehen, so wird CA mit IL überschrieben und IL wird mit dem aktuellen Staat überschrieben. Somit ist diese Art von Historisierung eigentlich nur bei solchen Daten zu empfehlen, bei welchen eine einmalige Änderung zu erwarten ist.

\subsection{Tupelversionierung}
Bei der Tupelversionierung werden die Tupel nicht mit einem Zeitstempel, wie in SCD Typ II, sondern mit einer Versionierungsnummer versehen. Es werden keine bestehenden Tupel aktualisiert, sondern nur neue Tupel erzeugt. Das Tupel mit der höchsten Versionierungsnummer ist jeweils das aktuelle gültige.

\begin{figure}[htb!]
	\centering
	\includegraphics[keepaspectratio=true,height=17\baselineskip]{tupelversionierung.PNG}
	\caption{Tupelversionierung}
	\label{fig:tupel}
\end{figure}

\newpage

\section{Multidimensionale Datenmodellierung (Script S153 / Buch S201)}
\subsection{Nutzen von MDDB}
\begin{wrapfigure}[18]{R}{0.65\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{erd.JPG}
	\caption{Ein solches ERD ist nicht mehr übersichtlich und somit nutzlos}
	\label{fig:erd}
\end{wrapfigure}
Ab einer gewissen Komplexität sind Entity-Relationship Diagramme schlicht und einfach nicht mehr brauchbar (z.B. in Abb. \ref{fig:erd}).

\vspace{10px}

\noindent Ebenfalls benötigt man ab einer gewissen Komplexität eine unschön grosse Anzahl von Joins, um anständige Datenaggregate zu kriegen, was bald auch zu Performanceproblemen führt. 

\vspace{10px}

\noindent Die Lösung der beiden oben genannten Probleme heisst \textbf{multidimensionale Datenbanken}. Anstelle von einer komplizierten zweidimensionalen Darstellung der Daten nehmen wir einfach eine Dimension hinzu .

\vspace{40px}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=12.5\baselineskip]{mddbs.png}
	\caption{Vergleich verschiedener Datenbank-Dimensionen}
	\label{fig:mddbs}
\end{figure}

In Abbildung \ref{fig:mddbs} kann eine Gegenüberstellung der verschiedenen Datenbank-Dimensionen gesehen werden. Jede Dimension fügt ein zusätzlicher Wert hinzu. So wird in der 1-dimensionalen Datenbank nur das Kurszentrum gespeichert, während in der 2-dimensionalen Datenbank zusätzlich noch die Anzahl Kursteilnehmer pro Kurs hinterlegt wird. Die 3-dimensionale Datenbank schichtet nun mehrere 2-dimensionale Datenbank-"scheiben" aufeinander in eine Würfelform. Jede dieser "Scheiben" symbolisiert ein Monat.

Anstelle, dass man nun ein ellenlanges Join-Statement von allen möglichen Tabellen machen muss, wenn man herausfinden will, wie viele Teilnehmer der Kurs K2 im Center C4 im Februar hatte, muss man nur die korrekten Koordinaten wissen (3/3/1).

\newpage

\noindent Eine MDDB (Multi-Dimensional Data Base) hat viele Vorteile. Einige davon sind:

\begin{itemize}
	\item Intuitive Benutzeroberfläche
	\item Kurze und stabile Antwortzeiten
	\item Hohe Zugriffs- Analyse- und Daten Präsentationsflexibilität
	\item Interaktives Navigieren in den Datenbeständen
	\item Schnelle Erstellung von geeigneten Ad-hoc-Berichten und Grafiken
	\item Kein technisches Datenbank-Wissen vorausgesetzt
	\item Vereinfachte Datenmustererkennung
	\item Vereinfachte Erkennung von schlecht bereinigten Daten aufgrund von unplausiblen Analysedaten
\end{itemize}

\subsection{Kennzahlen, Fakten und Dimensionen}

\noindent \textbf{OLAP-Systeme} werden verwendet, um anhand von Kennzahlen Entscheidungen zu treffen (Siehe Kap. 1.3-1.5). Das heisst, in den Datenbanken werden \textbf{Kennzahlen} gespeichert. 

\vspace{10px}

\noindent Eine solche \textbf{Kennzahl} symbolisiert eine \textbf{Eigenschaft}. Das mag z.B. ein Umsatz, ein Preis oder auch ein monatlicher Stromverbrauch sein.

Ein anderer Begriff für Kennzahlen sind \textbf{Fakten}. Mehrere Fakten können zu einem neuen Fakt aggregiert werden (z.B. Kennzahl 330 und Kennzahl 54 können aggregiert werden zum Fakt, dass das Produkt mit der PID 54 CHF330.- kostet.)

Zusätzlich wird der \textbf{Geschäftsaspekt}, auch \textbf{Dimension} genannt hinzugenommen. Er beschreibt den \textbf{Beobachtungspunkt oder -raum} der Kennzahl/des Fakts. So ein Dimension kann z.B. eine Filiale, eine Abteilung oder auch ein Quartal sein. Dimensionen sind sogenannte \textbf{deskriptiven Werte}, das heisst sie enthalten \textbf{keine messbaren Werte}, sondern geben den vorhandenen Kennzahlen lediglich einen \textbf{Kontext}.

Dabei liefern gröbere Dimensionen eine gröbere Granularität des Faktes (Dimension HSLU vs. Dimension Student $\rightarrow$ Fakt Verwaltungskosten Universität vs. Verwaltungskosten Student).

Es ist nicht immer einfach, Fakten und Dimensionen auseinanderzuhalten. So kann ein Preis sowohl Fakt, wie auch Dimension sein. Wenn man die Entwicklung des Preises im Verlaufe der Zeit beobachtet, dann ist der Preis ein Fakt. Wenn man nun aber die Anzahl von verkauften Produkten in verschiedenen Preissegmenten beobachtet, dann ist der Preis die Dimension.

\vspace{10px}

\noindent Grundsätzlich kann man sagen \textit{Eine Analyse basiert primär auf der Interpretation der Ausprägungen eines Fakts in mehreren Dimensionen.}

\newpage

\subsection{OLAP Würfel-Paradigma}
Je mehr Dimensionen man hat, desto feiner wird die Granularität der Analyse. 

\vspace{10px}

\noindent Um zurückzukommen auf Abb. \ref{fig:mddbs}: In der 1-dimensionalen Datenbank kann eine 1-dimensionale Analyse durchgeführt werden: \\

\centerline{\textit{Wie viele Kursteilnehmer hat das Kurszentrum C3}.} 

\vspace{10px}

\noindent \textbf{Dimension: } Kurszentren \\
\noindent \textbf{Fakten: } Kursteilnehmen

\vspace{10px}

\noindent Nehmen wir eine Dimension hinzu, so kann in der 2-dimensionalen Datenbank bereits eine 2-dimensionale Analyse durchgeführt werden. Zusätzlich kommt nun noch die Kurs-Dimension hinzu: \\

\centerline{\textit{Wie viele Kursteilnehmer hat der Kurs K2 im Kurszentrum C3}}

\vspace{10px}

\noindent \textbf{Dimension: } Kurszentren, Kurs \\
\noindent \textbf{Fakten: } Kursteilnehmen

\vspace{10px}

\noindent Nehmen wir nun zusätzlich noch die dritte Dimension hinzu, so kann nun eine 3-dimensionale Analyse durchgeführt werden, die wiederum feingranularer ist wie die 2-dimensionale Analyse: \\

\centerline{\textit{Wie viele Kursteilnehmer hat der Februar-Kurs K2 im Kurszentrum C3}}

\vspace{10px}

\noindent \textbf{Dimension: } Kurszentren, Kurs, Monat \\
\noindent \textbf{Fakten: } Kursteilnehmen

\vspace{10px}

\noindent Man kann nun noch beliebig viele Dimensionen hinzufügen, bis man die gewünschte Granularität erreicht hat (Wie viele weibliche Kursteilnehmer in der Altersgruppe 35-40, die im Bereich Gesundheitspflege tätig sind, hatte der Februar-Kurs K2 2016 im Kurszentrum C3?). Man ist nicht an den 'phyischen' 3-Dimensionalen Raum gebunden, sondern kann so viele Dimensionen erstellen, wie man wünscht, denn \textbf{der Würfel ist nur ein Modell und nirgends so materialisiert.} Das einzige Problem, das auftauchen könnte ist, dass es ein bisschen schwierig ist, ein 10-dimensionaler Würfel auf Papier darzustellen.

\subsubsection{Die 10 OLAP Würfel-Paradigmen}
\begin{enumerate}
	\item Sechs bis max. acht Dimensionen bleiben analystisch überschaubar und praktikabel
	\item Die verwendeten Dimensionen müssen unabhängig voneinander sein
	\item Die verwendeten Dimensionen können durchaus einer parallelen Dimensionshierarchie entnommen sein wie z.B. Print oder Broadcast
	\item Es muss nicht an jeder Dimensionskante gleich viele Elemente haben. In Abb. \ref{fig:mddbs} hat es z.B. 5 Kurse und 6 Monate.
	\item Die Elemente einer Dimension müssen immer die gleiche Granularität besitzen. Ihre Fakten sind also unmittelbar vergleichbar. Im Beispiel von Abb. \ref{fig:mddbs} sind es Monate als Zeitangabe und nicht z.B. Monate und Quartale gemischt.
	\item Unter Kombination mehrerer Dimensionen lassen sich mit dem gleichen Zahlenmaterial unterschiedliche Würfel herstellen. Jeder Würfel ist auf ein Analysemuster hin optimiert. Jede Analyse braucht ihren eigenen Würfel.
	\item Das Produkt (Projektion) aller Elemente aller Dimensionen ergibt die Anzahl der Fakten (oder NULL falls keine Daten vorhanden sind)
	\item Eine Zeitdimension hat es fast immer. Eine solche lässt zeitbezogene Entwicklungsanalysen zu.
	\item Die Fakten eines Würfels dokumentieren immer je nur eine relevante Kenngrösse wie z.B. den Umsatz oder den Gewinn
	\item Eine Dimension bringt Fakten immer in die gleiche Klasse von Beobachtungsräumen: Pro Monat, pro Quartal, pro Jahr, pro Abteilung etc.
\end{enumerate}

\subsubsection{Grundoperationen am Würfel}

\begin{description}
	\item[Slicing: ] Ausschneiden von Scheiben aus dem Würfel. Ändert nichts an der Granularität, macht aus einer 3-D Analyse eine 2-D Analyse
	\item[Dicing: ] Durch Einschränken von einer (oder mehreren) Dimensionen wird aus dem Würfel ein kleinerer erzeugt. (Z.B. aus dem Würfel in Abb. \ref{fig:mddbs} den Würfel der Kursteilnehmer von Kurs K2/K3 im Zentrum C2/C3 während Jan/Feb ausschneiden.)
	\item[Pivoting/Rotateion: ] Drehen des Würfels, so dass min. eine andere Dimension sichtbar ist
	\item[Drill-Down: ] Detaillierung eines Datenfeldes auf einzelne Werte (z.B. von Monaten auf einzelne Tage)
	\item[Drill-Up / Roll-Up: ] Gegenteil von Drill-Down; Verallgemeinerung eines Datenfeldes (z.B. von Monaten auf Quartalssicht)
	\item[Drill-Across: ] Verschiebung des Fokus auf derselben Dimensionsstufe (z.B. andere Region oder anderes Produkt)
	\item[Drill-Through: ] Ähnlich wie Drill-Across; Auswertung der gleichen Ansicht mit anderen Würfeln
	\item[Drill-In / Merge: ] Verringerung der Granularität durch Entfernen von eizelnen Dimensionen
	\item[Split: ] Gegenteil von Merge; Erhöhung der Granularität eines einzelnen Wertes durch Hinzufügen zusätzlicher Dimensionen (z.B. den Umsatz dieses Produktes in dieser exakten Filiale berechnen)
\end{description}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=8\baselineskip]{drill_across.jpg}
	\caption{Beispiel einer Drill-Across Operation}
	\label{fig:rel}
\end{figure}


\newpage

\subsection{Dimensionstruktur, -klassifizierung und -hierarchie}

Jede Dimension sollte eine Zusammenfassung mehrerer Elemente zu einem neuen, übergeordneten Dimensionswert ermöglichen ($\rightarrow$ roll-up). Das Gegenteil von roll-up ist drill-down. Diese beiden Funktionen ergeben einen hierarchischen 1:n Klassifikationspfad, wie zu sehen in Abb. \ref{fig:dimhier}.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{dimension_hierarchy.jpg}
	\caption{Klassfikiationshierarchie}
	\label{fig:dimhier}
\end{figure}

\subsubsection{Die 9 Regeln der Klassifikationshierarchie}

\begin{enumerate}
	\item Die Klassifikationsstruktur ist strukturiert
	\item Der Klassifikationsbaum muss vollständig sein. (Gehen wir davon aus, dass der Baum nach Radio und TV noch weitergeht)
	\item Die Bestimmung der Elemente und Zuordnung der Unterelemente ist schwer aber unerlässlich für künftige Analysen (Gehören Plakat-Aktionen zu Print oder Broadcast?)
	\item Die Bestimmung der Hierarchiestufen ist schwer.
	\item Höchstens 7 Hierarchiestufen mit je max. 15 - 20 Elementen um die Übersicht zu bewahren
	\item Es ist durchaus möglich, dass die gleichen Strukturelemente bis hin zur Wurzel parallel nach einer weiteren Sachlogik zusammengefasst werden. Im abgebildeten Beispiel analysieren wir nach dem Medium. Es könnte aber auch nach Zielgruppe zerlegt werden. So entsteht eine neue, parallele Klassifikationshierarchie.
	\item Dimensionen aus parallelen Klassifiationshierarchien gelten als unabhängig und können deshalb im Würfel eigene Kanten bilden.
	\item Die Hierarchie stellt uns oft vor die Frage wonach man detaillieren soll. Straftaten nach Alter oder nach Geschlecht oder nach Nationalität? Dies wird durch die geplanten Analysen bestimmt. Die Managementfragen sind zentral, bevor man mit dem Design der Dimensionsstruktur beginnt. Es kann dann halt sein, dass in mehreren Ästen eines Baumes gleiche Elemente vorkommen.
	\item Eine zusammengefasste Dimension ist eine neue Dimension. Aber die beiden Dimensionen sind nicht unabhängig, können also keine eigenen Würfel-Kanten sein. Parallele Dimensionen sind unabhängig.
\end{enumerate}

\subsection{Dünnbesetztheit}
In der Praxis sind längst nicht alle Würfel 'voll besetzt'. Diese fehlenden Daten werden auch als 'missing Data' bezeichnet. Es wird prinzipiell zwischen drei Arten von missing Data unterschieden:

\begin{description}
	\item[unmöglich] Es ist unmöglich, solche Daten zu erheben
	\item[unbekannt:] Die Daten sind einfach nicht bekannt.
	\item[(noch) nicht eingetreten: ] Das Event für diese Daten ist (noch) nicht eingetreten.
\end{description}

\noindent Zusätzlich wird zwischen der natürlichen und der logischen Dünnbesetztheit unterschieden.

\paragraph{Natürliche Dünnbesetztheit} beschreibt die Dünnbesetztheit aufgrund von nicht eingetretenen Daten (non-event) (Im Juli werden keine Skis verkauft). Jedoch hat man nicht NULL Skis verkauft, sondern 0. Mathematisch gesehen ist der Würfel also voll besetzt und es kann problemlos gerechnet werden.

\paragraph{Logische Dünnbesetztheit} beschreibt ebenfalls die Dünnbesetztheit von nicht eingetretenen Daten, jedoch nicht non-event nicht-Daten sondern event-not-applicable nicht-Daten. Nehmen wir an, ein Produkt ist im Feb.2018 auf den Markt gekommen. Es ist recht schwer, Verkaufsdaten für dieses Produkt im Nov. 16 zu finden. Diese Verkaufszahlen wären undefined oder NULL. Im Gegensatz zur natürlichen Dünnbesetztheit kann mit der logischen Dünnbesetztheit nicht korrekt gerechnet werden, da gewisse Anfragen aufgrund einer Unmöglichkeit zurückgewiesen werden und somit zu fehlenden oder verfälschten Analysen führen können.

Deshalb müssen undefinierte Felder in einem Würfel dementsprechend gekennzeichnet werden (entweder mit NULL oder n/a (not applicable)). Welcher Wert genau in undefinierte Datenfelder geschrieben werden soll, müsste in den Metadaten hinterlegt werden.

\vspace{10px}

\noindent Bei einem logisch dünnbesetzten Würfel kann die Anzahl der undefinierten und diejenige der definierten (auch 0) Zellen verglichen werden. Daraus erhält man den Füllgrad des Würfels, d.h. wie viel Prozent des Würfels enthält tatsächlich Daten. Bei einem sehr hohen Füllgrad spricht von von einem dicht besetzt (engl. dense) Würfel im Gegensatz zu einem dünn besetzten (sparse) Würfel.

\newpage

\subsection{Modelle in multidimensionallen Datenbanken}

In den bisher bekannten Datenbanken benötigt man zwei Modelle bzw. Schemata: Das \textbf{logische Modell} und das \textbf{physiche Modell}. Bei multidimensionalen Datenbanken kommt nun zusätzlich noch das \textbf{semantische Modell} dazu.

Die folgenden Modelle sind nach absteigendem Abstraktionslevel geordnet.

\subsubsection{semantisches Modell}
Der Datenwürfel mit seinen Dimensionen, Hierarchien und Fakten ist ein gutes semantisches Modell und hilft uns bei der Vorstellung von mehrdimensionalen Analysen. Der Datenwürfel ist eine möglichst wahrheitsgetreue Abbildung der Realität.

\subsubsection{logisches Modell}
Das logische Modell wird benötigt, um die Business-Logik (Requirements, interne Organisation, Prozesse etc.) darzustellen. Das Endprodukt eines logischen Modells ist normalerweise das \textbf{ERD} (Entity Relation Diagram). Das ERD zeigt die Beziehungen zwischen verschiedenen Kategorien von Daten und ist essenziell für das Datenbank-Design. 

\subsubsection{physisches Modell}
Das physische Modell baut anschliessend auf dem vorher erstellten logischen Modell auf und zeigt das tatsächliche Layout der Datenbank.

Nun werden Tabellen und Spalten mit Primär- und Fremdschlüssel erstellt nach den Vorgaben des logischen Modells. Im Gegensatz zum logischen Model ist das physische Model abhängig von der verwendeten Datenbank-Software.


\subsection{OLAP-Modelle in multidimensionalen Datenbanken}
Wie bereits in Kap. 1.5 besprochen wurde, wird OLAP (Online Analytical Processing) für die Analyse von Daten verwendet. Es wird zwischen vier Varianten des OLAP unterschieden:

\begin{description}
	\item[MOLAP:] Datenanalysen in multidimensionalen Systemen. MOLAP speichert Zahlen in Form von \textbf{Datenpunkten} und hat deshalb einen Performance-Vorteil gegenüber anderen OLAP-Systemen, die Daten als Datensätze speichern.
	\item[ROLAP: ] Datenanalysen in relationalen Systemen. Das System zieht Daten direkt aus bestehenden Datenbanken und benötigt so wesentlich weniger Speicherplatz.
	\item[HOLAP: ] Datenanalysen in hybriden Systemen. Hybrid für sowohl relational wie auch multidimensional.
	\item[DOLAP: ] Datenanlaysen in Desktop Systemen. Bei diesen Systemen nimmt nicht die tatsächliche Analyse die meiste Zeit in Anspruch, sondern das Erstellen und Auffrischen der dargestellten Daten-Würfel auf dem Desktop.
\end{description}

\begin{figure}[htb!]
	\centering
	\includegraphics[keepaspectratio=true,height=20\baselineskip]{OLAP}
	\caption{Vor- und Nachteile der verschiedenen OLAP-Systemen}
	\label{fig:OLAP}
\end{figure}

\begin{figure}[htb!]
	\centering
	\includegraphics[keepaspectratio=true,height=11\baselineskip]{comp_OLAP.jpg}
	\caption{Vergleich der verschiedenen OLAP-Systeme}
	\label{fig:comp_OLAP}
\end{figure}

\begin{figure}[htb!]
	\centering
	\includegraphics[keepaspectratio=true,height=11\baselineskip]{performance_vs_functionality.jpg}
	\caption{Abwägung der Performance vs. Funktionalität für die verschiedenen OLAP-Systeme}
	\label{fig:performance_functionality}
\end{figure}

\newpage

\subsection{CUBE, ROLLUP und GROUPING SETS (Skript S177)}
\code{GROUP BY [...] WITH CUBE} oder auch direkt \code{GROUP BY CUBE} liefern dasselbe Ergebnis wie \code{UNION ALL}, ist aber vor allem wenn \code{UNION ALL} in Kombination mit \code{GROUP BY} oder \code{ORDER BY} verwendet wird einfacher zu verstehen.

\paragraph{ROLLUP}\mbox{}\\
\code{ROLLUP} ist eine Erweiterung der \code{GROUP BY} 'Methode'. Es ermöglicht es, zusätzliche Zeilen einzufügen, die das Subtotal einer Query anzeigen.

\begin{lstlisting}[language=sql, captionpos = b, caption={Beispiel der Nutzung von ROLLUP}]
SELECT warehouse, product, SUM(quantity)
FROM Inventory
GROUP BY ROLLUP(warehouse, product)
\end{lstlisting}

\begin{figure}[htb]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[keepaspectratio=true, width=.8\linewidth]{warehouse_table.jpg}
		\caption{Die Tabelle 'warehose'}
		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{rollup.jpg}
		\caption{Die Summe der einzelnen Produkte und in allen Warehouses}
		\label{fig:sub2}
	\end{subfigure}
	\caption{Beispiel der Anwendung von ROLLUP}
	\label{fig:rollup}
\end{figure}

\paragraph{CUBE}\mbox{}\\
Ähnlich wie \code{ROLLUP} berechnet auch \code{CUBE} Subtotale von Queries. Allerdings generiert \code{CUBE} ein Subtotal von allen möglichen Kombinationen der spezifizierten \code{GROUP BY} Spalten

\begin{lstlisting}[language=sql, captionpos=b, caption={Beispiel der Nutzung von CUBE}]
SELECT warehouse, product, SUM(quantity)
FROM Inventory
GROUP BY CUBE(warehouse, product)
ORDER BY warehouse, product
\end{lstlisting}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=7\baselineskip]{cube.jpg}
	\caption{Beispiel der Anwendung von CUBE}
	\label{fig:cube}
\end{figure}

\paragraph{GROUPING SETS}\mbox{}\\
\code{GROUPING SETS} erlauben es einem, Daten gleichzeitig nach verschiedenen Gruppen zu gruppieren. Im untenstehenden Beispiel werden die gleichen Daten sowohl nach \code{Color}, wie auch nach \code{Item} gruppiert.

\begin{lstlisting}[language=SQL, captionpos=b, caption={Beispiel der Nutzung von GROUPING SETS}]
SELECT Item, Color, SUM(Quantity) AS QtySum
FROM Inventory
GROUP BY GROUPOING SETS(Item, Color)
\end{lstlisting}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=7\baselineskip]{grouping_sets.jpg}
	\caption{Beispiel der Anwendung von GROUPING SETS}
	\label{fig:group_sets}
\end{figure}

\section{MDX (Multidimensional Expressions) (Skript S196)}
MDX steht für multidimensional expressions. Es ist eine von Microsoft entwickelter Standard und ist eine Query-Sprache, die auf SQL aufbaut und äusserst komplex aber auch extrem mächtig ist.

\vspace{10px}

\noindent Das Ziel von MDX ist es folgende Dinge zu definieren oder weiterzuentwickeln:
\begin{itemize}
	\item Eine standardisierte Query-Sprache mit eindeutiger Semantik
	\item Standardisierte Zugriffsprotokolle
	\item Abfrageoptimierungen
	\item Zugriffsstrukturen für die Datenverwaltung
	\item Standardisierte Datenaustauschformate
\end{itemize}

\vspace{10px}

In MDX werden sehr viele Klammern gebraucht. Deren Bedeutung:

\begin{description}
	\item[Geschweifte Klammern {}: ] Eine Ansammlung von Tupel derselben Dimension als solche zu kennzeichnen
	\item[Runde Klammern (): ] Definition der Tupel in der \code{WHERE} Bedingung
	\item[Eckige Klammern []: ] Definiton der Namen von Dimensionen, Leveln und deren Membern, sowie auch Namen von Cubes.
\end{description}

\subsection{MDX SELECT}
Der MDX \code{SELECT} Befehl ist grundsätzlich dasselbe wie in SQL: Er ermöglicht das Lesen und Verbinden von Daten aus Daten-Würfeln. 

\vspace{10px}

\noindent Der \code{SELECT} Befehl benötigt mindestens die Anzahl der Achsen, die das Resultat enthalten soll (\code{AXIS}). Es können maximal 128 Achsen angegeben werden, wobei jedoch maximal zwei Achsen angezeigt werden können. Dabei handelt es sich bei Achse 0 um Spalten und bei Achse 1 um Zeilen. Ebenfalls muss die Anzahl Elemente auf jeder Achse angegeben werden (\code{MEMBERS}). Schlussendlich muss noch der Datenwürfel mittels \code{CUBE} angegeben werden.
\begin{lstlisting}[language=sql, captionpos=b, caption={Allgemeiner Syntax von MDX' SELECT}]
SELECT Ausdruck1 ON COLUMNS, Ausdruck2 ON ROWS, Ausdruck3 ON AXIS(2), Ausdruck4 ON AXIS(3)
FROMN CubeName
\end{lstlisting}

\begin{lstlisting}[language=sql, captionpos=b, caption={Beispiel MDX' SELECT}, label={fig:mdx_select}]
SELECT 	{[Mitarbeiter].[Mitarbeiter].MEMBERS} ON AXIS(0) --kann alternativ auch als '0' oder 'COLUMNS' geschrieben werden.
		{[Produkt].[Artikel]}.MEMBERS ON AXIS(1) --kann alternativ auch als '1' oder 'ROWS' geschrieben werden.
FROM	[DW1fach]
\end{lstlisting}

Das Beispiel von Listing \ref{fig:mdx_select} beschreibt eine Abfrage, bei der alle Mitarbeiter in den Spalten stehen und alle Artikel in den Zeilen.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=17\baselineskip]{mdx.jpg}
	\caption{Visualisierung des Codebeispiels von Listing \ref{fig:mdx_select}}
	\label{fig:mdx}
\end{figure}


\subsection{DAX (Data Analysis Expression)}
DAX ist die Sprache hinter PowerPivot, Power BI Desktop und SQL Server Analysis Services. Sie verwendet einige der typischen Excel-Funktionen, besitzt jedoch noch einige weitere Funktionen und Features, die mit relationalen Daten arbeiten und dynamische Aggregationen durchführen können.Es wird als Evolution der MDX mit einer Prise von Excel-Funktionen angeschaut, die einfach zu lernen ist und gleichzeitig die Flexibilität und Macht von PowerPivot bietet. 

\section{Planung der Systemtechnischen Architektur (Skript S205)}

\subsection{Realtime Data-Warehouse Systeme (Skript S206)}
Realtime Daa-Warehouse Systeme sind Data-Warehouses, die Daten in Realtime zur Verfügung stellen können. Jedoch muss man sich jeweils genau überlegen, ob eine solche "Data Freshness" überhaupt Sinn macht für die zu unterstützenden Geschäftsprozesse.

Ein klassisches Data-Warehouse durchläuft in regelmässigen Zyklen von einem Tag bis zu einem Monat den Datenbeschaffungsprozess (Kap. 4.1.8). Es kann jedoch durchaus Prozesse geben, die täglich oder sogar im Minutentakt aktualisierte Daten benötigen. 

Der Datenbeschaffunsprozess ist meist sehr rechenintensiv und wird deshalb in Off-Peak Zeiten (z.B. über Nacht) durchgeführt.

\subsection{In-Memory Computing (Skript S207/Buch S174)}
Realtime Data-Warehouses benötigen In-Memory Computing für die schnellere Verarbeitung von Daten. In-Memory Computing bedeutet, dass die Daten direkt im RAM gespeichert und verarbeitet werden. Aufgrund der wesentlich tieferen Zugriffszeiten und schnelleren Lese/Schreibe-Geschwindigkeit im Gegensatz zu normalen Festplatten kann die Datenverwaltung und -analyse so erheblich verschnellert werden.

RAM ist per Definition volatil. Da man logischerweise nicht will, dass die gesamten Daten des Data-Warehouses nach einem allfälligen Neustart des Servers plötzlich weg sind, werden normale Festplatten trotzdem noch verwendet, jedoch nur zu Backup-Zwecken (Snapshots, Mirroring oder auch Transaction-Logs).

\subsubsection{RAM-Management}
In In-Memory DAta Warehouses wird die sogenannte NUMA-Architektur eingesetzt (Abb. \ref{fig:numa}).

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{numa.png}
	\caption{Vergleich der NUMA und UMA-Architektur}
	\label{fig:numa}
\end{figure}

Im Gegensatz zu UMA (Uniform Memory Access) können hier nicht alle CPUs auf alle RAM-Bänke zugreifen, sondern jede CPU hat "ihre" RAM. Jedoch wissen alle CPUs auf welchen Bänken, bzw. bei welcher CPU sich die benötige Information befindet. So findet eine rege Kommunikation zwischen den einzelnen CPUs statt. Die Verwaltung, welche Informationen bei welcher CPU abgespeichert werden, wird vom Betriebssystem übernommen.

\newpage

\begin{wrapfigure}[17]{R}{0.55\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{SAP-HANA.jpg}
	\caption{Speicherverwaltung von SAP-HANA}
	\label{fig:sap_hana}
\end{wrapfigure}

Da das Betriebssystem aber meist keine Ahnung des Datenbankschemas und der verschiedenen Abhängigkeiten unter den jeweiligen Tabellen hat, kann es die Informationen gar nicht am effizientesten verteilen. Dafür hat die SAP eigens ein Tool entwickelt: SAP HANA. Beim Starten der Datenbank legt HANA seinen eigenen Hauptspeicher an (Abb. \ref{fig:sap_hana}). Dort wird der Programmcode für die Datenbanktabellen, die temporären Berechnungen und der Speicher für interne Prozesse gehandhabt. Wird in diesem Hauptspeicher wieder Platz frei, gibt HANA diesen Speicher nicht mehr ans Memory zurück, sondern behält es für allfällige spätere Nutzen noch bei sich. So muss es das Betriebssystem nicht mehr nach mehr Memory fragen, sondern kann diesen freien Speicher direkt wieder einem der in Abb. \ref{fig:sap_hana} dargestellten Bereiche zuordnen.

\subsubsection{Zeilenorientiertes vs. Spaltenorientiertes Speichern}

\begin{wrapfigure}[19]{L}{0.50\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{zeilenVsSpalten.jpg}
	\caption{Klassisch- vs. Zeilen- vs. Spaltenbasierte Datenspeicherung}
	\label{fig:zeilenVsSpalten}
\end{wrapfigure}

Im Gegensatz zu klassischen, relationalen Datenbanken (Abb. \ref{fig:zeilenVsSpalten} oben) abspeichern, werden Daten in In-Memory Datenbanken zeilen- oder spaltenbasiert (Abb. \ref{fig:zeilenVsSpalten} links, bzw. rechts) abgespeichert. Die Speicherungsart der Daten hängt davon ab, ob die Datenbank für OLAP oder OLTP verwendet wird.

\vspace{10px}

\noindent Bei OLTP-Systemen werden die Daten meist zeilenbasiert abgespeichert, da dies schnellen Schreibzugriff ermöglicht. Der Nachteil dabei ist der vergleichsweise langsame Lesezugriff, was bei OLTP-Systemen jedoch zu verkraften ist, da solche Systeme vor allem schreibe-heavy Operationen durchführen.

Bei OLAP-Systemen ist es genau umgekehrt. Da man auf schnellen Lesezugriff angewiesen ist, werden die Daten meist spaltenorientiert abgespeichert. OLAP-Systeme müssen nicht so viele Daten schreiben, von da her kann die vergleichsweise langsame Schreibgeschwindigkeit vernachlässigt werden.

\newpage

\paragraph{Vergleich Zeilen- und Spaltenorientierte Abfragen}\mbox{}\\
Welche Speicherart effizienter ist, hängt von der Art der Abfrage ab.

\vspace{10px}

\noindent Nehmen wir ein Beispiel, bei dem die spaltenorientierte Abspeicherung im Vorteil ist: 

\begin{lstlisting}
SELECT SUM(SALES)
FROM SALES
WHERE DATA > 2012-01-01
\end{lstlisting}

Wie in Abbildung \ref{fig:col_based} zu sehen ist, muss hier nur auf zwei Spalten zugegriffen werden: Man checkt in der \code{Date} Spalte, ob das Datum > 2012-01-01 ist und wenn ja, dann addiert man die korrespondierende Summe zum Resultat.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{column_based.jpg}
	\caption{Beispiel einer spaltenorientierten Abfrage}
	\label{fig:col_based}
\end{figure}

\noindent Nehmen wir nun ein anderes Beispiel wie folgendes:

\begin{lstlisting}
SELECT *
FROM SALES
WHERE COUNTRY = 'INDIA'
\end{lstlisting}

\noindent so ist die zeilenbasierte Speicherart eindeutig effizienter, wie Abbildung \ref{fig:row_based} zeigt. Während eine spaltenbasierte Datenbank hier bei jeder Spalte überprüfen müsste, ob die korrespondierende \code{Country}-Spalte 'India' enthält, muss die zeilenbasierte Datenbank nur in einer Spalte 'India' suchen und dann dieser Zeile 'entlangfahren'

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{row_based.jpg}
	\caption{Beispiel einer zeilenorientierten Abfrage}
	\label{fig:row_based}
\end{figure}

Zusammengefasst kann man sagen: Wenn die Abfrage wenige Spalten aber viele Zeilen beinhaltet, ist die spaltenbasierte Speicherart effizienter; bei vielen Spalten und wenigen Zeilen hat die zeilenbasierte Speicherung die Oberhand.

\newpage

\subsubsection{Chancen und Risiken des In-Memory Computing}
\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{in_memory.jpg}
\end{figure}

\subsection{Cloud-basiertes Data-Warehouse (Skript S218)}

Heutzutage ist alles mögliche und unmögliche in der Cloud. Wieso nicht auch das Data-Warehouse? Wieso mühselig und kostenintensiv ein eigenes Data-Warehouse aufbauen, wenn Grossanbieter wie Oracle und SAP dies bereits als SAAS anbieten? Es hat einige Vorteile, wie

\begin{itemize}
	\item Keine Wartungs- und Upgrade-Kosten
	\item Kein Zeit- und Ressourcenaufwand für die Verwaltung der riesigen Datenmengen
	\item Keine Kosten für den internen Betrieb
	\item Dynamisch skalierbar
\end{itemize}

\newpage

\subsubsection{Dictionary Daten-Komprimierung}

Wie am Beispiel der Abbildung \ref{fig:dictComp} zu sehen ist, funktioniert die Dictionary-Compression so, dass es ein Dictionary gibt, in welchem alle Daten abgespeichert sind. Die Spalten halten schlussendlich keine Daten mehr, sondern nur noch Pointer zum Ort der Daten im Dictionary. So können Redundanzen (wie z.B. das Doppelte Speichern des Namen "John") vermieden werden.
 
\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=15\baselineskip]{dictionary_compression.JPG}
	\caption{Beispiel der Dictionary Compression}
	\label{fig:dictComp}
\end{figure}

\newpage

\section{Hadoop (Skript S229)}
Apache Hadoop ist ein in Java geschriebenes Framework für skalierbare, verteilt arbeitende Software. Es basiert auf dem MapReduce-Algorithmus von Google sowie auf Vorschlägen des Google-Dateisystems und ermöglicht es, intensive Rechenoperationen mit grossen Datenmengen im Petabyte-Bereich auf Computerclustern durchzuführen.

\vspace{10px}

\noindent Hadoop wurde von Apache entwickelt und ist open source. 

\vspace{10px}

\noindent Die bekanntesten Anwender von Hadoop sind wohl Facebook IBM, AOL und Yahoo

\subsection{Anwendungen}
Hadoop kann auf drei Arten verwendet werden.

\begin{enumerate}
	\item Als neuer Datenspeicher
	\begin{figure}[htb!]
		\centering
		\includegraphics[keepaspectratio=true,height=9\baselineskip]{neuer_Datenspeicher.PNG}
	\end{figure}
	\item Als Datenplattform und zusätzlicher Input für das Enterprise Data Warehouse
	\begin{figure}[htb!]
		\centering
		\includegraphics[keepaspectratio=true,height=9\baselineskip]{datei_plattform.PNG}
	\end{figure}
	\item Als Daten Plattform und Basis für BI und Analytics
	\begin{figure}[htb!]
		\centering
		\includegraphics[keepaspectratio=true,height=9\baselineskip]{datei_plattform_2.PNG}
	\end{figure}
\end{enumerate}

\newpage

\subsection{Motivation}
\begin{itemize}
	\item Automatische Generierung von Daten führt zu wird riesigen Datenmengen
	\item Kosten pro GB sind in den letzten paar Jahren extrem gesunken
	\item Währenddessen haben sich die Transferzeiten nicht gross verändert
		\subitem $\rightarrow$ Die Zeit, um eine komplette Festplatte auszulesen wird somit immer länger
	\item Mehrere Server mit eigenen Festplatten werden zu Cluster verbunden
	\item Durch die verteilte Speicherung ist parallele Verarbeitung möglich.
\end{itemize}

\subsection{Möglichkeiten}
Hadoop kann intensive Rechenprozesse parallel in Clustern durchführen. Somit können grosse Datenmengen auf mehrere tausend Server verteilt werden.

Für die parallele Verarbeitung mehrerer Tasks wird Nebenläufigkeit verwendet. Man verwendet bei schwächerer Hardware scheinbare Nebenläufigkeit handelt (präemptives Multitasking $\rightarrow$ Scheduler weist jedem Task gewisse CPU-Zeit zu) oder wenn mehrere Cores oder CPUs zur Verfügung stehen, kann echte Nebenläufigkeit durchgeführt werden.

\vspace{10px}

\noindent Hadoop wird als Schnittstelle zwischen Real Time Data und Data Warehouses verwendet.

\subsection{Aufbau}
Hadoop verwendet das HDFS (Hadoop Distributed File System) um Daten zu speichern und den MapReduce Algorithmus und diese zu verarbeiten. Weitere Bausteine,auf welche hier nicht weiter eingegangen wird, können im Skript auf S232 nachgesehen werden.

\subsection{Architektur}
\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=19\baselineskip]{hadoop_architektur.png}
	\caption{Vergleich einer klassischen RDBMS-Architektur im Vergleich zu Hadoop}
	\label{fig:hadoop_arch}
\end{figure}

\begin{minipage}[L]{0.45\textwidth}
	\begin{description}
		\item[Master: ] Verwaltet die Anfragen vom Client. Der Master enthält zusätzlich zu den Slaves eine NameNode sowie einen JobTracker
		\item[Client: ] Der Client schickt Anfragen zum Speichern und Verarbeiten von Daten. Der Client kann ein Software-UI oder auch ein CMD-Tool sein
		\item[Cluster/Rechnerverband: ] Ansammlung mehrerer Computer/Server (Nodes). Durch Cluster wird erstens die Rechenpower und zweitens die Redundanz und somit Verfügbarkeit erhöht.
		\item[Node: ] Rechner in einem Cluster, jede Node enthält ein Task Tracker und eine Data Node
	\end{description}
\end{minipage}
\begin{minipage}[R]{0.45\textwidth}
	\begin{center}
		\includegraphics[height=15\baselineskip, keepaspectratio=true]{hadoop_architektur_2.png}
	\end{center}
\end{minipage}

\begin{description}
	\item[HDFS: ] Hadoop Distributed File System. Hadoops Dateisystem
	\item[Name Node: ] Managed das Dateisystem, die Indexierung und die Metadaten. Es gibt eine Active und eine Standby Name-Node, die im Falle des Ausfalls der Active NN einspringen könnte
	\item[Data Node. ] Verwaltet die Daten und liefert sie den Clients. Die Name Node kommuniziert mit den Data Nodes
	\item[Job Tracker Node: ] Erhält Anfragen vom Client und plant und überwacht das MapReducing. Nur 1 Job Tracker pro Cluster
	\item[Task Tracker: ] Führt alle MapReduce Transaktionen aus, Mehere Task Tracker pro Cluster 
\end{description}

\subsection{MapReduce}
Wie bereits erwähnt, verwendet Hadoop den MapReduce Algorithmus zur Verarbeitung von grossen Datenmengen. Das Ziel dieses Algorithmus ist es, ungeordnete Datenmengen in Key-Value Paare umzuwandeln:

\begin{description}
	\item[Map: ] Generiert für jeden Datensatz ein Key-Value Paar
		\begin{multicols}{2}
			\code{[Store, SalesAmount]}
		\columnbreak
		
			\code{[New York, \$450]}	
		\end{multicols}
	\item[Reduce: ] Summiert dieselben Key-Value Paare
		\begin{multicols}{2}
			\code{[New York, \$450]}\\
			\code{[New York, \$500]}\\
			\code{[San Jose, \$444]}\\
			\code{[San Jose, \$600]}
		\columnbreak
		
			\code{[New York,\$950]}\\
			\code{[San Jose, \$1044]}
		\end{multicols}
\end{description}

\newpage

\noindent Der MapReduce Algorithmus läuft folgendermassen ab:

\begin{enumerate}
	\item Verteile die Inputdaten (wenn möglich fair) auf die verfügbaren Nodes auf
	\item Jede Node führt nun ein Map-Durchgang mit den ihr zugewiesenen Daten durch
	\item Die entstandenen Key-Value Paare werden so gemischt, dass alle gleiche Keys bei derselben Node zu liegen kommen
	\item Jede Node führt ein Reduce auf die ihr zugewiesenen Daten und Keys aus.
\end{enumerate}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=14\baselineskip]{mapreduce.png}
	\caption{MapReduce verwandelt ungeordnete Daten in Key-Value Paare}
	\label{fig:mapreduce}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=20\baselineskip]{map_reduce_call.png}
	\caption{Ablauf eines Map-Reduce Aufrufes}
	\label{fig:mapreducecall}
\end{figure}

\vspace{10px}


\subsection{Speichern mit HDFS}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=20\baselineskip]{hdfs_speichern.png}
	\caption{Speichervorgang unter HDFS}
\end{figure}

\subsection{Anwendungen}
Mehr Anwendungen von Hadoop können auf Seite 255 im Skript nachgelesen werden.

\newpage

\section{Speicherarten (Skript S256)}

\begin{description}
	\item[Klassiche Datenhaltung: ] Räumlich konzentriert halten und zentral/dezentral zur Verfügung stellen
	\item[Daten-Prtitionierung: ] Räumlich verteilt halten und zentral/dezentral zur Verfügung stellen
	\item[RAID] steht für \textit{Redundant Array of Independent Disks} und erlaubt die redundante Abspeicherung von Daten in einem Festplatten-Verbund, so dass keine Daten verloren gehen im Falle eines Festplatten-Defekt
	\item[Dateidienste ] verwalten die Datenbestände mit einem eigenen Dienst-Betriebssystem, welches auch für Zugrifssicherung und ggf. Verschlüsselung zuständig ist.
	\item[NAS-Systeme: ] \textit{Network Attached Storage} sind übers Netzwerk zugreifbar und sind auf Firmware basierte Dateidienste, die vorwiegend in kleinen Umgebungen zum Einsatz kommen
	\item[SAN-Systeme: ] \textit{Storage Area Network} verteilen die Datenbestände über ein eigenes Storage-Netzwerk
	\item[Speichervirtualisierung: ] Ist eine Speicherart, bei der direkt adressierbare Daten-Volumen auf mehrere Festplatten oder SAN-Nodes verteilt werden (z.B. LVM bei Linux). Speichervirtualisierung wird von NAS und SAN unterstützt.
\end{description}

\noindent In Data Warehouses benötigt man extreme Mengen von temporärem Speicherplatz zum Zwischenspeichern von Datensätzen beim Sortieren, Joinen oder sonstiges Aggregieren und es werden viele Daten ausgelagert. Deshalb wird in solchen Umgebungen meist mit Speichervirtualisierung mittels SAN gearbeitet.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=9\baselineskip]{storage.png}
	\caption{Vergleich zwischen klassischer (links) und partitionierter (rechts) Datenhaltung}
	\label{fig:storage}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=9\baselineskip]{raid.png}
	\caption{Beispiel von RAID 6. Hier können 2 Festplatten defekt sein ohne dass ein Datenverlust auftritt}
	\label{fig:raid}
\end{figure}

\newpage

\subsection{Objektspeicher (Skript S258)}

\subsubsection{Definiton}
\begin{center}
\blockquote{Objektspeicherung ist ein allgemeiner Begriff für einen Ansatz, wie man eigenständige Einheiten an Storage manipuliert und adressiert, die man Objekte nennt.}
\end{center}

Objekte enthalten Daten. Jedoch werden Objekte nicht hierarchisch gespeichert (wie z.B. Dateien, die normalerweise in einem Baum abgespeichert werden (/home/user/Documents/foo.txt)). Objekte sind flach organisiert. Jedes neue Objekt wird einfach in den Storage-Pool geschmissen. Ebenfalls sind Objekte atomar, es können also keine Objekte innerhalb von anderen Objekten abgelegt werden.

Ebenso wie herkömmliche Dateien haben Objekte Metadaten. Das Objekt wird jedoch mithilfe dieser Metadaten charakterisiert. Jedes Objekt erhält eine unique ID, die in den erweiterten Metadaten abgelegt wird.

Objektspeicherung wird meist mit dem Valet-Service in Restaurants verglichen: Der Besucher gibt sein Auto ab und erhält dafür einen Beleg. Der Valet parkt das Auto irgendwo, wo genau ist dem Besucher prinzipiell egal. Wenn der Besucher sein Auto wieder will, so kann er nur den Beleg abgeben und erhält so sein Auto wieder. 

Bei Objektspeicher ist das sehr ähnlich. Der Besucher ist der Server oder der Enduser und das Auto sind Daten. Der Valet ist der Objektspeicher und der Beleg ist eine eindeutige ID des gespeicherten Objekts. Der Server übergibt dem Objektspeicher ein Objekt, das er speichern will. Er erhält dafür nun eine eindeutige ID zurück. Wenn er das Objekt wieder lesen will, muss er dem Server nur diese eindeutige ID mitgeben und erhält das abgespeicherte Objekt zurück.

\subsubsection{Motivation}
Es gibt mehrere Gründe, wieso en Objektspeicher einem klassischen Dateisystem vorzuziehen ist.

\begin{itemize}
	\item Objekte sind nicht hierarchisch organisiert. Der Storage Pool erlaubt die relativ einfache Speicherung von enormen Mengen von unstrukturierten Daten.
	\item Ein hierarchisches Dateisystem ist ungeeignet wenn eine Datei aus Milliarden von Dateien gelesen werden soll.
	\item Objektspeicher bestehen aus vergleichsweise günstigen Clustern
	\item Storage Pools sind praktisch grenzenlos skalierbar 
	\item Storage Pools erlauben einen schnellen Lese- und Schreibzugriff, auch bei enormen Datenmengen
	\item Objektspeicherung erlaubt ein schnelles Indexieren, eine schnelle Suche und Langzeit-Archivierung
	\item Storage Pools sind Cloud-fähig
\end{itemize}

Aus diesen und anderen Gründen verwenden viele grosse Player wie Spotify, Facebook und Google Objektspeicher.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{objekt_speicher.png}
	\caption{Vergleich Dateisystem (links) und Objektspeicherung (rechts)}
	\label{fig:objStor}
\end{figure}

\newpage

\subsubsection{Arhitektur}
Objektspeicherungs-Software besteht aus vielen lose gekoppelten Services und ist Hardwareunabhängig. Ausserdem wird solche Software in drei Layer aufgeteilt:

\begin{description}
	\item[Storage-Layer: ] Schnittstelle zu den Data-Nodes
	\item[Metadatenmanagement-Layer: ] Bestimmt wo die Objekte gespeichert werden, wie sie auf die Nodes verteilt werden und wie sie geschützt werden.
	\item[Präsentation-Layer: ] Verwaltet die Schnittstelle zu den Clients sowohl über HTTP- wie auch über Dateisystem-Protokolle
\end{description}

Da Objektspeicher-Software meist über HTTP mit dem Client interagiert, wird auch hier die normale HTTP-API verwendet:

\begin{description}
	\item[\code{GET}] Objekt holen
	\item[\code{PUT}] Objekt erstellen
	\item[\code{COPY}] Objekt kopieren
	\item[\code{DELETE}] Objekt löschen
	\item[\code{HEAD}] Metadaten eines Objekt abrufen
	\item[\code{POST}] Objekt-Metadaten erstellen oder bearbeiten
\end{description}

\newpage

\newgeometry{margin=0.75in}

\section{XMLA (Skript S268)}
XMLA steht für \textit{XML for Analysis} und ist eine API, die es Programmen erlaubt, mit OLAP- oder MDDB-Systemen zu kommunizieren. Die Kommunikation läuft über HTTP, SOAP und XML.

HTTP und XML sollten bereits bekannt sein. SOAP (Simple Object Access Protocol) ist ein Netzwerkprotkoll, mit dessen Hilfe Daten zwischen Systemen ausgetauscht und Remote Procedure Calls durchgeführt werden können.

XMLA verwendet MDX (Siehe Kap. 10) als Abfragesprache. Die MDX-Statements werden in \code{<Statement>} Tags gehüllt. Zur Übertragung verwendet XMLA die SOAP-Methoden \code{Execute} und \code{Discover}

\subsection{Execute}
Das Rückgabewert einer \code{Execute} Methode ist ein Satz Tupel (entweder OLAP oder multidimensional). Die Methode verwendet das \code{<Command>} Tag, in welchem ein MDX, DMX oder auch SQL-Statement eingebettet werden kann, und das \code{<Properties>} Tag, in welchem gewissen Eigenschaften wie Timeout, Catalog name etc. mitgegeben werden können.

\begin{lstlisting}[language=xml, captionpos=b, caption={MDX-SELECT Statement mit XMLA übermittelt}]
<Execute xmlns="urn:schemas-microsoft-com:xml-analysis">
	<Command>
		<Statement>
			SELECT [Measures].MEMBERS ON COLUMNS FROM [Adventure Works]
		</Statement>
	</Command>
	<Properties>
		<PropertyList>
			<DataSourceInfo>
				Provider=MSOLAP;Data Source=local;
			</DataSourceInfo>
			<Catalog>
				Adventure Works DW Multidimensional 2012
			</Catalog>
			<Format>
				Multidimensional
			</Format>
			<AxisFormat>
				ClusterFormat
			</AxisFormat>
		</PropertyList>
	</Properties>
</Execute>
\end{lstlisting}

\newpage

\subsection{Discover}\mbox{}\\
Der Rückgabewert der \code{Discover} Methode ist ein Rowset voller Metadaten. Diese Methode macht Metadatenabfragen.

\begin{lstlisting}[language=xml, captionpos=b, caption={Anfrage für Cube-Listen aus der einem bestimmten Catalog mit XMLA übermittelt}]
<Discover xmlns="urn:schemas-microsoft-com:xml-analysis:rowset">
	<RequestType>MDSCHEMA_CUBES</RequestType>
	<Restrictions>
		<RestrictionList>
			<CATALOG_NAME>
				Adventure Works DW Multidimensional 2012
			</CATALOG_NAME>
		</RestrictionList>
	</Restrictions>
	<Properties>
		<PropertyList>
			<DataSourceInfo>
				Provider=MSOLAP;
				Data Source=local;
			</DataSourceInfo>
			<Catalog>
				Adventure Works DW Multidimensional 2012
			</Catalog>
			<Format>
				Tabular
			</Format>
		</PropertyList>
	</Properties>
</Discover>
\end{lstlisting}

\noindent Das Ganze wird nun über SOAP übermittelt, was schlussendlich so aussieht:

\begin{lstlisting}[language=xml, captionpos=b, caption={SELECT-Anfrage in XMLA verpackt, über SOAP versendet}]
<soap:Envelope>
	<soap:Body>
		<Execute xmlns="urn:schemas-microsoft-com:xml-analysis">
			<Command>
				<Statement>
					SELECT Measures.MEMBERS ON COLUMNS FROM Sales
				</Statement>
			</Command>
			<Properties>
				<PropertyList>
					<DataSourceInfo/>
						<Catalog>
							FoodMart
						</Catalog>
						<Format>
							Multidimensional
						</Format>
						<AxisFormat>
							TupleFormat
						</AxisFormat>
				</PropertyList>
			</Properties>
		</Execute>
	</soap:Body>
</soap:Envelope>
\end{lstlisting}
\restoregeometry

\section{Metadaten (Skript S271/Buch S339)}

\subsection{Definition}
\begin{center}
	\blockquote[Bauer/Günzel, 2009]{Als Metadaten bezeichnen wir jede Art von Informationen, die für den Entwurf, die Konstruktion und die Benutzung eines Informationssystems benötigt wird \\}
\end{center}

\noindent Auf Data-Warehousing angewendet heisst das soviel wie

\begin{center}
	\blockquote[Kimball, 2008]{We think about metadata as all the information that defines and describes the structures, operations and contents of the DW/BI system}
\end{center}

\noindent Man kann also sagen, Metadaten beschreiben Daten/Datenstrukturen sowie manuelle und maschinelle Prozesse

\subsection{Meta-Metamodell}

\begin{minipage}{0.7\textwidth}
	\includegraphics[keepaspectratio=true, height=15\baselineskip]{mof.png}
	\captionof{figure}{MOFs 4-Ebenen Architektur \\ am Beispiel von Casablanca}
	\label{fig:mof}
\end{minipage}
\begin{minipage}{0.3\textwidth}
	\includegraphics[keepaspectratio=true, height=12\baselineskip]{mof2.png}
	\captionof{figure}{Abstrahierte Version von \ref{fig:mof}}
	\label{fig:mof2}
\end{minipage}

\vspace{10px}


\noindent Die MOF (Meta Object Family) beschreibt eine 4-Schichten Metadaten-Architektur.

\begin{description}
	\item[M0 - Objekt] Konkrete, Real-Life Daten\\
		\code{"Casablanca"}
	\item[M1 - Modell] Modelle, wie UML- oder Objektmodelle, die die Objekte aus M0 beschreiben \\
		\code{"Casablanca ist ein Video mit dem Titel 'Casablanca'}
	\item[M2 - Meta-Modell] Meta-Modelle definieren, wie die Modelle von M1 aufgebaut und strukturiert sind \\
		\code{"Ein Video ist eine Klasse mit definierten Attributen"}
	\item[M3 - Meta-Meta-Modell] Das Meta-Meta-Modell definiert das Meta-Modell aus M2 und gleichzeitig sich selbst (M3). M3 stellt somit die oberste Ebene dar, die sich selbst beschreibt und somit ein unendlicher Meta-Loop generiert.\\
		\code{"Eine Klasse ist eine Klasse ist eine Klasse ist eine..."}
\end{description}

\subsection{Data Warehouse Manager}
Siehe Kap. 4.1.8 - Verwaltungsbereich

\subsection{Metadaten Manager}
Siehe Kap. 14.4

\subsection{Unterscheidung von Metadaten}
\begin{wrapfigure}[13]{R}{0.75\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{metadaten.png}
	\caption{Unterscheidung zwischen technischen und geschäftlichen Metadaten}
	\label{fig:metadaten}
\end{wrapfigure}
Optimal hätte ein Data Warehouse ein zentrales Metadaten-\\ Repository. In Realität ist es jedoch meist so, dass zwischen technischen und geschäftlichen Metadaten unterschieden wird.

Je nach dem, wen man fragt, kommen noch prozessuale Metadaten hinzu.

\subsubsection{Technische Metadaten}
Diese Metadaten beschreiben und definieren das Datenbank-System und werden meist automatisch angelegt. Sie sind Daten über das Data Warehouse Modell und sind meist im DBMS abgelegt

Solche Metadaten umfassen zum Beispiel:

\begin{multicols}{2}
	\begin{itemize}
		\item 	Server und Instanzen (Ort und Name)
		\item 	Datenbankname 
		\item 	Datenbankobjekte (Tabellen, Indizes, Sichten, etc.) 
		\item 	Datenmodelle der Quellsysteme 
		\item 	Datenextraktionsregeln 
		\item 	Datentransformationsregeln 
		\item 	Daten zur Versionierung 
		\item 	Data Cleansing Regeln 
		\item 	Daten Aggregations Regeln 
		\item 	Namen von Tabellenviews 
		\item 	Daten über die Steuerung des Ladeprozesses 
		\item 	Daten über den Datenupdatezyklus 
		\item 	Primär-/Fremdschlüssel Beziehungen 
		\item 	Datenstrukturen 
		\item 	Datenverteilung (Partitionierung) 
		\item 	Quell- und Zielsystem Beschreibung von ETL-Prozessen 
		\item 	etc.
		\item 	Transformationsvorschriften, -richtlinien (Policies) - \item 	\item 	Datenqualitätskriterien 
		\item 	Ausnahme- und Fehlerbehandlungen 
		\item 	Datentypen, Wertebereiche (Domänen) 
		\item 	Auskunft über Bezeichner (Namen) 
		\item 	Auskunft über offene Transaktionen 
		\item 	Rechtevergabe 
		\item 	Statistiken über Mengeninformationen und Datendichte 
		\item 	Serverstatistiken (Uptime, Anzahl Verbindungen, …) 
		\item 	Zugriffsstatistiken (Häufigkeiten von richtigen und falschen Anmeldungen, …) 
		\item 	System-, Versions- und Konfigurationsdaten 
	\end{itemize}
\end{multicols}

\subsubsection{Geschäftliche Metadaten}
Diese Metadaten beschreiben die Datenherkunft, ihre geschäftliche Bedutung, Rollen und Verwendung. Im Gegensatz zu den technischen Metadaten unterstützen sie die geschäftlichen Analysen.

\begin{multicols}{2}
	\begin{itemize}
		\item Datenlieferanten 
		\item Datennutzer 
		\item Datenflüsse 
		\item Bedeutung und Definition der Geschäftsdaten 
		\item Geschäftsregeln der Datentransformation (Minimum, Maximum, NULLBehandlung usw.) 
		\item Gruppierungen 
		\item Aggregationen 
		\item Dimensionshierarchien 
		\item Vordefinierte Berichte 
		\item Vordefinierte Queries 
		\item Daten für die Query und Reporting Tools 
		\item Report Verteilinformation 
		\item Sicherheits- und Zugriffsprivilegien
	\end{itemize}
\end{multicols}

\subsubsection{Prozessuale Metadaten}
Diese Metadaten beschreiben die Prozesse im Data Warehouse

\begin{multicols}{2}
	\begin{itemize}
		\item Regeln zur Datenextraktion, zur Transformation und zum Ladevorgang
		\item Auslösezeiten, Endzeiten von Vorgängen 
		\item Benutzte Ressourcen (CPU, Primär-, Sekundärspeicher usw.) 
		\item Betroffene Objekte 
		\item Mengen, z.B. Tupelmengen (Row Counts) 
		\item Vollständigkeits- und Qualitätsstatistiken für die Auditierung 
	\end{itemize}
\end{multicols}

\noindent Zudem wird unterschieden zwischen beschreibenden und vorgangsbezogenen Metadaten.

\begin{multicols}{2}
	\paragraph{Beschreibende Metadaten}\mbox{}\\
	Darunter fallen Daten, die das System \textit{beschreiben} (wie der Name das schon vermuten lässt), also z.B. über alle Schnittstellen, Komponenten und Daten
	\columnbreak
	\paragraph{Vorgangsbezogene Metadaten}\mbox{}\\
	Darunter fallen Daten, die wichtig sind in bestimmten Prozessen wie z.B. den ETL-Prozessen, also Daten über die Quell- und Zielsysteme, über die Quell- und Zieldaten etc.
\end{multicols}

\subsection{Metadaten-Strategie}
Die Strategie befasst sich mit den Fragen \textit{Wie und wer pflegt und integriert die Metadaten?}

\subsubsection{Wie}
Die \textit{Wie}-Frage hängt extrem von denverwendeten Produkten ab, aber das Ziel w
re eine totale Zentralisierung in einem einzigen Repository.

Prinzipiell können drei Ansätze unterschieden werden:

\begin{description}
	\item[Do-it Yourself: ] Die Firma kümmert sich selbst um die Metadaten
	\item[Kernprodukt: ] Die Firma verwendet ein Produkt, das das Repository und dessen Struktur vorgibt
	\item[Gesamtlösung: ] Die Firma hat einen Lieferanten, der bereits die gesamte Infrastruktur geliefert hat und somit auch eine zentrale Repository-Lösung bieten muss.
\end{description}

\subsubsection{Wer}
Die Person, die die Metadaten pflegt sollte optimalerweise alles wissen, sowohl im Geschäftlichen wie auch im Informatik-Aspekt. 

Ein bisschen genauer heisst das:

\begin{multicols}{2}
	\begin{itemize}
		\item Sehr gute Kenntnisse von DB/DBMS
		\item Sehr gute Kenntnisse von SQL
		\item Sehr gute Kenntnisse von XML
		\item Sehr gute Kenntnisse von den verwendeten Diensten und Produkten
		\item Gesunden Menschenverstand
	\end{itemize}
\end{multicols}

\subsection{Normierung der Metadaten}
Das Ziel ist nicht nur ein zentrales Repository, sondern auch eine Normierung aller Metadaten. Das heisst

\begin{multicols}{2}
	\begin{itemize}
		\item Enheitlichkeit
		\item Herstellerneutralität
		\item Maschinenlesbarkeit
		\item Einfach bearbeitbar mit Tools.
	\end{itemize}
\end{multicols}

\noindent Ein recht verbreiteter Standard ist das \textit{Common Warehouse Metamodel} oder CWM. Es umfasst die Modellierung, Beschreibung, Zugriff und Austausch von Metadaten und die Beschreibung anderer Objekte und Komponenten.

Das CWM wurde wie das MOF (Kap. 15.2) von der OMG entwickelt und greift bei verschiedenen Metadaten auch auf das MOF zurück, um eine gemeinsame Basis zu finden, mag das auf M2 oder auch erst M3 sein.

CWM ermöglicht die Beschreibung der Datenmodelle der Quell- und Zielsysteme sowie die Datenverschiebungen zwischen diesen beiden Systemen. Zudem können via CWM Abbildungsvorschriften zwischen dem physischen und dem logischen Modell des Data Warehouse definiert werden.
\vspace{10px}

\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[keepaspectratio=true,height=10\baselineskip]{CWM_schichten.PNG}
		\captionof{figure}{CWM-Schichtenmodell}
		\label{fig:cwm_schichten}
\end{minipage}
\begin{minipage}{0.5\textwidth}
		\includegraphics[keepaspectratio=true,height=10\baselineskip]{CWM_metamodell.PNG}
		\captionof{figure}{CWM-Metamodell}
		\label{fig:cwm_meta}
\end{minipage}

\vspace{10px}

\noindent Ein Ziel des CWM ist die Vereinfachung des Metadatenaustauschs. Es fungiert als gemeinsamer Nenner. Alle Metadaten laufen zuerst über das CWM bevor sie vom Quellsystem ins Zielsystem überführt werden.

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{CWM_schnittstellen.PNG}
	\caption{Vereinfachung des Umherschiebens von Metadaten durch das CWM}
	\label{fig:cwm_schittstellen}
\end{figure}


Zusammengefasst soll das CWM Dinge vereinfachen wie z.B.

\begin{multicols}{2}
	\begin{itemize}
		\item ETL-Applikationsentwicklung
		\item Tabellenerstellung in Data Warehouse und Data Mart
		\item Erstellens des Database Persistence Layer
		\item OLAP Persistenzschicht
		\item Datenmigration vom Data Warehouse zum Data Mart
		\item Aufsetzung des Reporting Tools
	\end{itemize}
\end{multicols}
\vspace{10px}

\noindent CWM-Definitionen werden in XMI verfasst. XMI steht für \textit{XML Metdata Interchange} und ist ein Standard der OMG für den Austausch von Metadaten

\begin{lstlisting}[language=xml, captionpos=b, caption={Ausschnitt einer Definition einer relationalen 'Administrations'-Tabelle}]
<?xml version="1.0" encoding="UTF-8"?> 
<XMI xmi.version="1.1" timestamp="Jun 20 2005 09:35:52" xmlns:CWM="org.omg.CWM1.0" xmlns:CWMRDB="org.omg.CWM1.0/ Relational" xmlns:CWMOLAP="Olap" xmlns:CWMTFM="Transformation">
	<XMI.header>
		<XMI.documentation>
			<XMI.exporter>Meta Integration Model Bridge</ XMI.
			<XMI.exporterVersion>4.1.0 - Aug 16 2004 12:21:11</XMI.
		</XMI.documentation>  <XMI.metamodel xmi.name="CWM" xmi.version="1.0"/>
	</XMI.header>
	<XMI.content>
		<CWMRDB:Catalog xmi.id="_4" name="Model" visibility="public">
			<CWM:Namespace.ownedElement>
				<CWMOLAP:Schema xmi.id="_5" name="Logical" visibility="public" namespace="_4">
					<CWMOLAP:Schema.dimension>    
						<CWMOLAP:Dimension xmi.id="_6" name="Assessment Summary" visibility="public" schema="_2">
							<CWM:Namespace.ownedElement>
								<CWMTFM:TransformationMap xmi.id="7" visibility="public" namespace="_6">
									<CWM:Namespace.ownedElement><CWMTFM:ClassifierMap xmi.id="_8" name="unnamed_8" visibility="public" namespace="7" transformationMap="7">
\end{lstlisting}

\section{Data Warehouse Strategie (Skripit S286)}
\begin{wrapfigure}[16]{R}{0.5\textwidth}
	\centering
	\includegraphics[keepaspectratio=true,height=18\baselineskip]{DWH_Strategie.PNG}
	\caption{Data Warehouse Strategie}
	\label{label}
\end{wrapfigure}
.
\vspace{40px}

Die Data Warehouse Strategie ist ein Teil der IT Strategie und sollte keinesfalls ein isolierter Prozess sein. Die IT-Strategie wiederum ist ein Teil der Unternehmensstrategie.

Bei der Strategiefindung spielen nebst den Daten auch der Zweck des Warehouses, dessen Organisation, die Technik, das Budget und auch der Mensch eine entscheidende Rolle. 

Wenn eine Strategie und Architektur des Data Warehouse gefunden ist, ist das Thema nicht abgeschlossen. Beide Dinge sollten ein permanenter Prozess sein.

\subsection{Reifegradmodelle}
Reifegradmodelle stufen den Zustand eines Systems oder eines Teilsystems innerhalb einer vordefinierten Abstufung ein. Diese Abstufungen zeigen den Ausprägungs- Entwicklungs und Leistungsstand des Systems.

Ein Reifegradmodell dient zur Standortbestimmung und dient unter anderem dazu, festzustellen, wie gross der Aufwand ist, um zur nächsthöheren Stufe zu gelangen.

Eine tiefere Stufe heisst nicht zwingend, dass das System eine niedrigere Qualität ht.

\vspace{10px}

\begin{tabular}{|p{7cm}|p{7cm}|}
	\hline
	\thead{Vorteile} & \thead{Nachteile}\\
	\hline
	\tabitem Standortbestimmung = Selbstreflexion \newline
	\tabitem Erzwingt Systembetrachtung von anderem Winkel \newline
	\tabitem Aufdeckung von Über- bzw. Unterentwicklung \newline
	\tabitem Man kann gleich noch aufräumen \newline
	\tabitem Bei gutem Reifegrad kann der in Kommunikation mit Partner verwendet werden & 
	\tabitem Wildwuchs an Reifegradmodellen \newline
	\tabitem Überaufwand für Erreichen bestimmter Stufen \newline
	\tabitem Reifegrad basiert nur auf qualitativ und nicht quantitativ \newline
	\tabitem Meist nur auf ein Teilsystem fokussiert. Keine Gesamtübersicht
	\\
	\hline
\end{tabular}

\begin{figure}[htb]
	\centering
	\includegraphics[keepaspectratio=true,height=10\baselineskip]{reifegrad_bimm.PNG}
	\caption{Beispiel eines Reifegradmodells (biMM)}
	\label{fig:bimm}
\end{figure}

\end{document}
