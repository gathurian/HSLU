\documentclass[a4paper, 11pt]{article}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage[utf8]{inputenc}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{makecell}
\usepackage{tabularx}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{lscape}
\usepackage{afterpage}
\usepackage{geometry}
\usepackage{listingsutf8}
\usepackage{chngcntr}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{outlines}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{amsmath}
\usepackage{breqn}
\usepackage{textcomp}



\counterwithin{figure}{section}

\AtBeginDocument{\counterwithin{lstlisting}{section}}

\geometry{a4paper, margin=1in}

\renewcommand*{\thead}[1]{\bfseries #1}
\newcommand{\code}[1]{\texttt{#1}}
\def\doubleunderline#1{\underline{\underline{#1}}}

\newcommand \tabitem{\makebox[1em][r]{\textbullet~}}

\DeclareMathSizes{12}{30}{16}{12}

\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\definecolor{darkgreen}{rgb}{0.05,0.56,0.06}


\lstset{frame=tlrb,
    language=python,
    captionpos=b,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=left,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{violet},
    stringstyle=\color{darkgreen},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3,
    literate=%
    {Ö}{{\"O}}1
    {Ä}{{\"A}}1
    {Ü}{{\"U}}1
    {ß}{{\ss}}1
    {ü}{{\"u}}1
    {ä}{{\"a}}1
    {ö}{{\"o}}1
}


\begin{document}

\newgeometry{top=0in, bottom=0in}
\title{Machine Learning FS2019}
\author{Alex Neher}
\maketitle

\tableofcontents

\newpage
\graphicspath{{./Pictures/}}

\restoregeometry

\section{Introduction}

There are two popular definitions of Machine Learning:

\begin{centering}
    \begin{quote}
        \blockquote[Arthur Samuel, IBM, 1959]{Field of study that gives computers the ability to earn without being explicitly programmed}
    \end{quote}

    \begin{quote}
        \blockquote[Tom Mitchell, 1998]{A computer program is said to learn from experience $E$ with respect to some task $T$ and some performance measure $P$, if its performance on $T$, as measured by $P$, improves with experience $E$}
    \end{quote}
\end{centering}

So summarizing these two quotes, it can be said, that machine learning is defined as \textbf{the process in which machines learn something (mostly) on their own}.

\subsection{Disciplines}

There are different disciplines in machine learning:

\begin{description}
    \item[Supervised Learning: ] The algorithm is given \textbf{labeled training data} and learns to \textbf{predict} the \textbf{labels} of yet unseen examples.
    \item[Unsupervised Learning: ] The algorithm is given \textbf{unlabeled data} and \textbf{creates labels by itself} based on the structure of the given data
    \item[Semi-Supervised Learning: ] A \textbf{mixture} of supervised and unsupervised learning. This approach is usually chosen if there is only \textbf{very little labeled test data}
    \item[Reinforcement Learning: ] No data is availabe, but the algorithm is \textbf{being rewarded}. The algorithm searches the ideal behaviour that maximizes its reward (Not subject of this lecture)
\end{description}

These classifications can be subdivided even more:

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=0.9\textwidth]{disciplines.jpg}
    \caption{Distinction between supervised and unsupervised learning}
    \label{fig:class}
\end{figure}

The main difference between \textbf{classification} and \textbf{regression} is that when using classification, the result is \textbf{categorical}, whereas regression returns \textbf{numerical} results. 

\textbf{Clustering} is similar to classificiation. However, while classification algorithms sort the given data into given groups, clustering algorithms determine these groups \textbf{by themself}. This means, you can give a clustering algorithm a seemingly random dataset and the algorithm finds some kind of structure in it.

\newpage

\section{Data Quality}

Data is categorized into \textbf{numerical} and \textbf{categorical} data. 

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=0.9\textwidth]{data_classification.png}
    \caption{Classification of Data}
    \label{fig:data_classification}
\end{figure}

Before any machine learning can take place, the quality of the given data has to be assessed and in some cases improved. Because every prediction made by machine learning algorithms is shit if the data quality is shit.

There are many reasons why the data quality could be poor:

\begin{itemize}
    \item Ill-designed, inadequate or inconsistent data formats
    \item Programming errors or technical issues (e.g. sensor outage)
    \item Data decay (e.g. outdated e-mail addresses)
    \item Poorly designed data entry forms (e.g. data fields without verification)
    \item Human errors in data export or data pre-processing
    \item Deliberate errors and false information (e.g. due to privacy concerns everybody is called Hans Muster and lives at Musterstrasse 123)
\end{itemize}

\subsection{Data Quality Assessment}

Before even starting to assess the data-quality, it is seldomly a bad idea to \textbf{clean} the data first.

\begin{enumerate}
    \item Identify and remove duplicates
    \item Replace null-values (do not delete them because that might falsify the mean and median of the data)
    \item Make data formats more machine-friendly (so-called \textit{data-wrangling} e.g. store the gender as boolean)
\end{enumerate}

\newpage

If you change anything from the original data set, you should always

\begin{itemize}
    \item Document all the changes
    \item Use a SVN (e.g. git)
    \item Let the data provider know that his data quality is shit (maybe they'll improve in the future)
    \item Investigate the origins of the poor data quality
\end{itemize}

\subsection{Approaches to Data Quality Assessment}

\begin{description}
    \item[Identify data sources and their trustworthiness]
    \item[Interpret statical key figures: ] See following sections
    \item[Visualize selected portions of the data: ] e.g. with Pair Plots (See Abb. \ref{fig:pair_plots} )
    \item[Manually check data ranges] Negative Salaries, People more than 200 years old...
    \item[Validate plausibility of attribute correlation: ] e.g. are mileage and number of seats in a core correlated? Can one of the columns be removed for redundancy?
    \item[Measure data redundancy: ] Can certain columns be removed due to not adding any real value to the data
    \item[Check for anomalies in syntax and semantics: ] Outliers can really distort a dataset and render the whole algorithm useless. Can be prevented by e.g. normalization of the data or removal of the outlier
    \item[Replace NULL Values and remove duplicate values]
\end{description}

There are different ways to cope with NULL variables, but they have to be addressed, as most machine learning algorithms do not play well with them. 

\begin{itemize}
    \item Delete all rows with NULL values \\
        Might be the easiest way if you have loads of data
    \item Fill in the missing values manually (e.g. from other sources) \\
        Might be the hardest way if you have loads of data
    \item Fill in a global constant like N/A, UNKNOWN
    \item Use a measure for central tendency \\
        e.g. take the mean if your data is symmetric or take the median if its skewed
    \item Use a measure for central tendency per class \\
        e.g. take different values for healthy and sick people
    \item Use e.g Regression to 'guess' the missing values
\end{itemize}

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=0.4\textheight]{pair_plots.png}
    \caption{Visualisation of Data with Pair Plots}
    \label{fig:pair_plots}
\end{figure}

\newpage

\subsection{Statistical Key Figures}

These figures can give you a rough overview about the whereabouts of your data-magnitude.

\subsubsection{Central Tendency}

\textbf{Mean} \\

This is the averge in a set of numeric data. You add all data and divide it by the number of data points

\[ \scalebox{1.5}{
        $\mu_{x}=\frac{1}{n} \sum^{n}_{i=1} x_{i}$
}\]

\vspace{10px}

\noindent \textbf{Mode} \\

This is the value that occurs the most in a given set of data

\vspace{10px}

\noindent \textbf{Median} \\

This is the middlemost value of a sorted set of data. In contrast to the Mean, the Median can give information concerning the distribution of the data.

Given a dataset of $1, 2, 3, 4, 5$, the median and mean are both $3$. However, if we have $1, 2, 3, 1000, 10000$, the mean is $2201.2$ whereas the mean is still $3$ 

\newpage

\subsubsection{Skewdness}

All of these values can give information concerning the datas \textbf{skewness}

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=0.9\textwidth]{skewness.png}
    \caption{Skewness of data}
    \label{fig:skewness}
\end{figure}

\noindent    
$Mean - Mode > 0 \rightarrow$ Negative skewness / Left-skewed data \\
$Mean - Mode = 0 \rightarrow$ Symmetric Data \\
$Mean - Mode > 0 \rightarrow$ Positive skewness / Right-skewed data

\vspace{10px}

\subsubsection{Quartile \& Interquartile Range (IQR)}

The three quartiles divide your data into four equal-sized, cnsecutive subsets.

To calculate $Q1$, take the median of your data and then again the madian of the left half of the data.

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=0.9\textwidth]{iqr.png}
    \caption{Quartiles of a dataset}
    \label{fig:iqr}
\end{figure}

\newpage

\subsubsection{Five Number Summary}

With this method, you can get a pretty good overview of your data. The \textbf{Five Number Summary} of a dataset consists of:

\begin{itemize}
    \item Median Q2
    \item Quartiles Q1 and Q2
    \item Smallest individual Value
    \item Largest individual Value
\end{itemize}

\begin{minipage}{0.45\textwidth}
    \begin{lstlisting}[caption={Five Number Summary in Python}]
    import numpy as np
    import panas as pd

    s = pd.Series(np.random.rand(100))
    s.describe()
    \end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
    \begin{lstlisting}[caption={Output}]
    mean       0.524559
    std        0.285565
    min        0.003933
    25%        0.298367
    50%        0.530632
    75%        0.765907
    max        0.993293
    dtype: float64
    \end{lstlisting}
\end{minipage}

\subsubsection{Boxplot}

\begin{wrapfigure}[14]{R}{0.5\textwidth}
    \centering
    \includegraphics[keepaspectratio=true,height=14\baselineskip]{boxplot.png}
    \caption{Boxplot}
    \label{fig:boxplot}
\end{wrapfigure}

This plot is a \textbf{visual representation of the five number summary} and can also give information on potential outliers. 

Values $1.5 \cdot IQR$ above the 3rd or below the 1st Quartile can be considered outliers and are displayed with small circles. 

\subsubsection{Variance}
The variance shows \textbf{how much the values are spread on average}. This is measured by squaring the sum of all deviations from the mean

\[ \scalebox{1.5}{
        $\dfrac{1}{1-n} \sum^{n}_{i=1}(x_{i} - \mu x)^2$
}\]

The standard deviation $\sigma$ is calculated as $\sqrt{variance}$

\newpage

\subsubsection{Covariance}
The covariance is used to determine whether two variables are \textbf{connected} to each other. 

If both variables are on the same side of the mean, the variance is \textbf{positive}, the variables are probably connected. Meaning if the value of one variable is rising, the other one will most likely rise as well. 

If one is above and one is below the mean, the variance is \textbf{negative}, the variables are most likely \textbf{inversely connected} to each other. Meaning if the value of one variable is rising, the other is most likely falling.

If the variables are \textbf{independent} from each other, the covariance is zero, as they both cancel each other out.

\[\scalebox{1.5}{
        $Cov(x,y) = \dfrac{1}{1-n} \sum^{n}_{i=1}(x_{i} - \mu x)(y_{i} - \mu y)$
}\]

The \textbf{covariance matrix} shows the covariance from all $X$ with all $Y$. As $Cov(x,x) = Var(x)$, the covariance matrix has the variance of $X$ in its diagonal


\subsubsection{Pearson Correlation} \label{sec:pearson}
Both the covariance and the variance are connected to the scale of the dataset, so the covariance of $X=[1,2,3,4,5] / Y=[6,7,8,9,10]$ is 2.5, whereas the covariance of $X=[1000,2000,3000,4000,5000] / Y=[6000,7000,8000,9000,10000]$ is 2'500'000'000. However, the Perason Correlation is 1 in both examples.

\[\scalebox{1.5}{
        $\rho(X,Y)=\dfrac{Cov(X,Y)}{\sigma_{x} \sigma_{y}} = $

}\] 
\[\scalebox{1.5}{
        $        \dfrac{\dfrac{1}{1-n} \sum^{n}_{i=1}(x_{i} - \mu x)(y_{i} - \mu y)}{\sqrt{\dfrac{1}{1-n} \sum^{n}_{i=1}(x_{i} - \mu x)^2} \cdot \sqrt{\dfrac{1}{1-n} \sum^{n}_{i=1}(y_{i} - \mu y)^2}}$
}\]
The Pearson Correlation is always between 1 and -1. 

\noindent 1 means the data is perfectly correlated, whereas -1 means that the data is perfectly incorrelated

\subsection{Normalization}
It is immensely important that all data is normalized before we run a machine learning algorithm over it. Considering the data in figure \ref{fig:vector_space_modefig}, 'Mileage' and 'Price' are in a completely different scale. If the mileage of the first shown car goes up 500 miles, it's not really a big deal. However, a price increase by 500 would double the car's price.

Such differently scaled data can (and will) falsify the result of every machine learning algorithm you could find. Therefore, \textbf{normalization is really important}.

\vspace{10px}

\noindent There are two popular normalization approaches: The Min-Max and the Z-Score normalization. 

\newpage

\noindent \textbf{Min-Max normalization}

\noindent All data is condensed to a value between 0 and 1. The smallest value becomes 0 and the largest one becomes 1.

\[\scalebox{1.5}{
        $x \rightarrow \dfrac{x - min_{x}}{max_{x}-min_{x}}$
}\]

\noindent \textbf{Z-Score Normalization}

\noindent The dataset is transformed in such a way, that the mean becomes 0 (so-called \textit{mean-centering}) and the standard deviation is 1

\[\scalebox{1.5}{
        $x \rightarrow \dfrac{x - \mu_{x}}{\sigma_{x}}$
}\]

\section{Geometry of Data}

\subsection{Feature Engineering}

Sometimes, data has to be modified to be better accessible/processable for machine learning algorithms. These algorithmus can work the best with simple numbers, so that's the data we should be striving for:

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=\linewidth]{feature_engineering.png}
    \caption{Turn 'complicated' data into easier data for better results}
    \label{fig:feature_engineering}
\end{figure}

\subsection{Vector Space Model}

As described before, machine learning algorithms work best with \textbf{numeric} data. However, the real world isn't that easy and mostly throws categorical data at you. Therefore, you have to convert categorical data to numerical data. 

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=\linewidth]{vector_space_model.png}
    \caption{Turn categorical data into numerical data with the vector space model}
    \label{fig:vector_space_modefig}
\end{figure}

This transformed data can also be visualized in a coordinate system and we can do math with it.

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=\linewidth]{geometric_interpretation.png}
    \caption{Transformed into vector space, data points can be interpreted as geometric points}
    \label{fig:geometric_intepretation}
\end{figure}

\newpage

\subsection{Similarity of Data}

The math we want to do is not even overly complicated: We just want to measure the distance between different points. Because \textbf{the smaller the distance between two points, the more similar they are}. 

\subsubsection{Euclidean Distance}

The distance between two points is most easily calculated using the \textbf{euclidean distance}:

\[\scalebox{1.5}{
        $dist(X,Y)= \sqrt{\sum^{n}_{i=1}(x_{i}-y{i})^2} $
}\]

So the distance between the points $(5/10)$ and $(8/6)$ can be calculated as

\begin{align*}
    \sqrt{(5-8)^2 + (10-6)^2} \\
    \sqrt{-3^2 + 4^2} \\
    \sqrt{9+16} \\ 
    \sqrt{25} = 5
\end{align*}

\newpage

\subsubsection{Cosine Similarity}

\begin{wrapfigure}[14]{R}{0.5\textwidth}
    \centering
    \includegraphics[keepaspectratio=true,height=14\baselineskip]{cosine_similarity.png}
    \caption{Cosine Similarity}
    \label{fig:cosine_similarity}
\end{wrapfigure}

If you want to to compare two points that appear to be on a line (Pearson Correlation close to 1), but the euclidean distance is high, then the cosine similarity is probably pretty low. 

The cosine similarity looks at the \textbf{angle} between point A and point B. However, it does also take the euclidean distance into consideration.

\vspace{10px}

The cosine similarity is essentially just the scalar product of the two points.

\vspace{10px}

\[\scalebox{1.5}{
        $sim(X,Y) = \dfrac{\sum^{n}_{i=i}{x_{i} y_{i}}}{\sqrt{\sum^{n}_{i=i}{x_{i}}^2\sqrt{\sum^{n}_{i=i}{y_{i}}^2}}}$
}\]
\[\scalebox{1.5}{
        $dist(X,Y) = 1 - sim(X,Y)$
}\]

\subsubsection{Levenshtein / Edit Distance for Strings}

Count the minimal number of changes necessary to turn one string into another:
\begin{itemize}
    \item count +1 when deleting a character [d]
    \item count +1 when adding a character [a]
    \item count +2 when changing a character [c]
\end{itemize}

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=\linewidth]{levenshtein.png}
    \caption{Examples for Levenshtein Distance}
    \label{fig:levenshtein}
\end{figure}

\newpage

\section{Supervised Machine Learning}

\subsection{Regression and classification algorithms}

\begin{multicols}{2}
    \textbf{Regression}
    \begin{itemize}
        \item Linear Regression
        \item Polynomial Regression
        \item k-NN Regression
        \item Support Vector Regression
        \item Neural Networks
        \item Regression Trees
    \end{itemize}
    \columnbreak
    \textbf{Classification}
    \begin{itemize}
        \item Logistic Regression
        \item Naïve Bayes
        \item k-NN
        \item Support Vector Machines
        \item Neural Networks
        \item Decision Trees
    \end{itemize}
\end{multicols}

\subsection{Decision Boundaries}

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=\linewidth]{decision_boundaries.png}
    \caption{Decision Boundaries for different classification approaches}
    \label{fig:decision_boundaries}
\end{figure}

Classifications usually end in somithing like Figure \ref{fig:decision_boundaries}. The example shows a classification whether a tumor is benign or cancerous. Brown means cancerous and blue means benign. Even though the data points are the same in all pictures, different approaches yield different results.

The goal of a 'good' classification-algorithm is to produce as few false-positive (algorithm says is cancer, but is actually not) and false-negatives (algorithm says its benign but is actually cancerous) as possible. 

\subsubsection{Kernel-Trick}
The data in Fig. \ref{fig:decision_boundaries} is still theoretically linearly separable. But in case it is not, you could use the so-called 'kernel-trick', where you simply add a dimension and change your point of view (see Fig. \ref{fig:kernel_trick})

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true,height=11\baselineskip]{kernel_trick.png}
    \caption{Kernel Trick to linearly separate data that is not linearly separable}
    \label{fig:kernel_trick}
\end{figure}

\subsection{k-Nearest-Neighbor}

\begin{minipage}{0.55\textwidth}
    \begin{lstlisting}
    from sklearn.neighbors import KNeighborsClassifier

    knn = KNeighborsClassifier(n_neighbors=3)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    acc = accuracy_score(y_test, y_pred)

    print("Test Set Accuracy for k=3" + ": (:.2f)".format(acc))
    \end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{0.40\textwidth}
    \centering
    \includegraphics[keepaspectratio=true, width=\textwidth]{knn.png}
\end{minipage}    

\vspace{10px}

\begin{wrapfigure}[15]{L}{0.5\textwidth}
    \centering
    \includegraphics[keepaspectratio=true,height=12\baselineskip]{k-nn-regression.jpg}
    \caption{Regression with k-NN}
    \label{fig:knn-regression}
\end{wrapfigure}

The k-neeares-Neighbor algorithm assigns the label of its nearest datapoint to the sample datapoint. If \code{n\_neighbors} is $>1$, it assigns the label of the most neighbours (via majority voting).

\vspace{10px}

Even though the K-NN algorithm is pretty slow compared to other algorithms, it is often a good choice if you only have a limited amount of data, because other, more performant algorithms usually require a lot more data to get decent predictions.

Usually, k-NN treats all neighbors as equals. However, you could assign a weight to each datapoint. This weight depends on the distance $d$ from the sample point. This is especially useful if you want to use k-NN to e.g. form a regression line.

You can use k-NN for regression by simply assigning the mean of all $k$ neighbors as label to the sample data.

\subsection{Training- and Testdata}
If you use the same data to train and test your algorithm, it might occur that the algorithm is 'memorizing' the data and gives you brilliant results. However, if you release it into the wild, where it encounters different data, it will perform really poorly. This is called \textbf{overfitting}

To counter overfitting, you usually split your data into \textbf{trainingdata} and \textbf{testdata}. You train the algorithm with the training data and test it with the testdata (who would've thought...). 

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true,width=0.9\textwidth]{training_testdata.png}
    \caption{Split your data into training- and testdata}
    \label{fig:training_testdata}
\end{figure}

\textbf{training} means that you tweak the parameters of the algorithm to minimize the cost function.

\textbf{testing} means that you test the performance of the algorithm with those tweaked parameters on \textit{yet unseen data}. If the algorithm has already seen the data, you might run into overfitting problems.

\vspace{10px}

If you need to tweak your hyperparameters a lot (e.g. the 'k' in k-NN), you should probably use a more complex evaluation workflow. Because if you keep tweaking the hyperparameters and then testing them with the same data, you'll end up with the very same overfitting problem that I explained earlier (and will therefore probably get fired and have to live on the street).

\vspace{10px}

Therefore, it is recommended that you add \textbf{validation data} to your workflow. You train your model with the training data, validate the results with the validatin data, and if the result is satisfactory, you can test it on entirely different test data.

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true,width=0.9\textwidth]{validation_data.png}
    \caption{Add validation data to the mix}
    \label{fig:validation_data}
\end{figure}

This method requires quite a lot of data. If you do not have the required amount of data, you could for example use \textbf{cross validation}. You still split your data into training- and testdata and then use a different 'slice' of your training data to validate the hyperparameter.

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true,width=0.9\textwidth]{cross_validation.png}
    \caption{10-fold cross validation}
    \label{fig:cross_validation}
\end{figure}

\subsection{Measuring the performance of classification}

To verify that your tweaked parameters are indeed within the margin that is acceptable, you need to do some quality assuranc first.

\subsubsection{Confusion Matrix}

\begin{minipage}{0.45\textwidth}
    \begin{tabular}{|p{1.5cm}|p{1.5cm}|p{1.5cm}|}
        \hline
        \thead{n=165} & Predicted YES & Predicted NO \\
        \hline
        Actual No & 50 & 10 \\
        \hline
        Actual YES & 5 & 100  \\
        \hline
    \end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.55\textwidth}
    \begin{description}
        \item[True Positive: ] Predicted Yes, Actual Yes
        \item[True Negative: ] Predicted No, Actual No
        \item[False Positive: ] Predicted Yes, Actual No
        \item[False Negative: ] Predicted No, Actual Yes
    \end{description}
\end{minipage}

\vspace{10px}

The confusion matrix shows, how many true/false positives and true/false negatives the algorithm produced. 

With these values, one can calculate the algorithms \textbf{Accuracy} and \textbf{Error Rate}

\subsubsection{Accuracy and Error Rate}

\[\scalebox{1.5}{
        $Accuracy = \dfrac{True\ Positive + True\ Negative}{Total}$ 
}\]
\vspace{10px}
\[\scalebox{1.5}{
        $Error\ Rate = \dfrac{False\ Positive + False\ Negative}{Total} = 1 - Accuracy$
}\]

\vspace{10px}

In our case the Accuracy would be $\dfrac{50+100}{50+100+5+10} = \dfrac{150}{165} = 0.91 = 91\%$ and the error rate therefore 9\%

\newpage

\subsubsection{Sensitivity}

Accuracy works great on balanced data, but it's not reliable on inbalanced data because Accuracy only checks how many times the classifier was right. 

If there were 5000 NO instances and 20 YES instances, a classifier that only returns NO would have an accuracy of over 99\%.

\vspace{10px}

\noindent The \textbf{Sensitivity} (also called 'Recall') counts how many true positives there are.

\[\scalebox{1.5}{
        $Sensitivity = \dfrac{True\ Positive}{Actual\ YES} = \dfrac{True\ Positive}{True\ Positive + False\ Negative} $
}\]

\vspace{10px}

In our confusion matrix from earlier, the Sensitivity would be $\dfrac{100}{100 + 5} = \dfrac{100}{105} = 0.95 = 95\%$

\subsubsection{Sepcificity}
This is the inverse of the Sensitivity. It counts how many NO the algorithm correctly predicted.o

\[\scalebox{1.5}{
        $Specificity= \dfrac{True\ Negative}{Actual\ NO} = \dfrac{True\ Negative}{True\ Negative + False\ Positive} $
}\]

\vspace{10px}

In our confusion matrix from earlier, the Specificity would be $\dfrac{50}{50+ 10} = \dfrac{50}{60} = 0.83 = 83\%$

\subsubsection{Precision}
Both of the preceding measures relied on the true negative. However, what would you do if you could not count the True Negatives? You use \textbf{Precision}. Precision shows how many times the algorithm is correct if it predicts YES.

\[\scalebox{1.5}{
        $Precision = \dfrac{True\ Positive}{Predicted\ YES} = \dfrac{True\ Positive}{True\ Positive + False\ Positive}$
}\]

\vspace{10px}

In our confusion matrix from earlier, the Precision would be $\dfrac{100}{100+ 10} = \dfrac{100}{110} = 0.91 = 91\%$

\subsubsection{F1 Score}

The F1 score is the harmonic mean between precision and recall/sensitivity

\[\scalebox{1.5}{
        $F1 = \dfrac{2 \cdot Precision \cdot Sensitivity}{Precision + Sensitivity} $
}\]

In our confusion matrix from earlier, the Precision would be $\dfrac{2 \cdot 0.91 \cdot 0.95}{0.91 + 0.95} = \dfrac{1.73}{1.86} = 0.93 = 93\%$

\vspace{10px}

Due to the fact that the F1 score does not take True Negatives into account, it tends to be strongly biased towards the worse score. 

However, it is still one of the best methods to solve a classification problem with skewed data.


\subsection{Measuring the performance of regression}

\begin{figure}[htb]
    \centering
    \includegraphics[keepaspectratio=true, width=\linewidth]{regression_error.png}
    \label{fig:regression_error}
\end{figure}

\subsubsection{Coefficient of Determination} \label{sec:r_squared}

\[\scalebox{1.5}{
        $R^2 = 1- \dfrac{\dfrac{1}{m} \sum^{m}_{i=1}(y_{i} - f_{i})^2}{\dfrac{1}{m} \sum^{m}_{i=1}(y_{i} - \bar{y})^2} =1 - \dfrac{\sum^{m}_{i=1}(y_{i} - f_{i})^2}{\sum^{m}_{i=1}(y_{i} - \bar{y})^2} $
}\]

$R^2$ is a staticstical measure of how well the predictions approximate the real data points. 
The top is the sum of squared errors (how much does the prediction deviate from the actual value) and the botom is the deviation of the mean.

$R^3 = 1$ is a perfect prediction-line

$R^3 = 0.53$ means that 53\% of the of the predictions are correct and can be explained by the model.

\newpage

\section{Linear Regression}

\begin{wrapfigure}[16]{R}{0.5\textwidth}
    \centering
    \includegraphics[keepaspectratio=true,height=14\baselineskip]{linear_regression.png}
    \caption{Linear Regression}
    \label{fig:linear_regression}
\end{wrapfigure}

Linear regression is a \textbf{supervised machine learning algorithm} to predict values based on previous variables.

It can also be used to verify whether two variablex X and Y are \textbf{dependant} on each other.

\vspace{10px}

Linear Regression produces a \textbf{straight line} (who would have thought) and each data point has a certain distance from that line. That distance is called \textbf{error} or \textbf{residual}. 

These errors should cancel each other out, as the regression-line should be placed in the middle of all the points.

\vspace{10px}

The equation for a 'normal' line is $g(x) = m \cdot x + q$, where $m$ symbolizes the slope of the line and $q$ tells you where the line crosses the $y$-axis.

The equation for the regression line is very much like the 'normal' line-equation, but we use greek symbols, because why not...

\[\scalebox{1.5}{
        $h_{\theta}(x) = \theta^{}_{0} + \theta^{}_{1} \cdot x$
}\]

\vspace{10px}

As mentioned before, deviation from that line are called errors. Therefore, the error of a given point $i$ can be calculated from the difference of the actual $y_{i}$ from the $h_{\theta}(x)$ (the $y$ on the line)

\[\scalebox{1.5}{
        $e_{i} = y_{i} - (\theta^{}_{0} + \theta^{}_{1} \cdot x_{i})$
}\]

\vspace{10px}

The goal of linear regression is to find the \textbf{optimal line}, ergo the line that \textbf{produces the smallest errors}.

However, as mentioned earlier, the errors usually cancel each other out, as sometimes the line lies above the data point (negative error) and sometimes it lies below it (positive error). To solve that problem, the parameter we will use to determine the quality of the produced line will be \textbf{the sum of squared error}. By squaring the errors, we can't have negative errors (because squared numbers cannot be negative). Therefore, the smaller the sum of squared errors, the better the line.

The sum of squared errors can be calculated as follows:

\[\scalebox{1.5}{
        $ J(\theta^{}_{0}, \theta^{}_{1}) = \dfrac{1}{2n} \sum^{n}_{i=1}(e_{i})^2 = \dfrac{1}{2n} \sum^{n}_{i=1}[y_{i} - (\theta^{}_{0} + \theta^{}_{1} \cdot x_{i})]^2$
}\]

\vspace{10px}

To minimize $J(\theta^{}_{0}, \theta^{}_{1})$, the \textbf{gradient} of $J$ must be \textbf{minimized} or, ideally, zeroed. 

This leads to the following formulas for $\theta^{}_{0}$ and $\theta^{}_{1}$

\begin{minipage}{0.45\textwidth}
    \[\scalebox{1.5}{
            $\theta^{}_{1} = \dfrac{S_{xy}}{S_{xx}}$
    }\]
\end{minipage} \hfill
\begin{minipage}{0.45\textwidth}
    \[\scalebox{1.5}{
            $\theta^{}_{0} = \bar y - \theta^{}_{1} \cdot \bar x$
    }\]
\end{minipage}

Where $\bar x$ and $\bar y$ are the \textbf{mean} of the $x$ and $y$ values respectively

\vspace{10px}

$S_{xx}$ and $S_{xy}$ are called \textbf{regression coefficients} and are calculated as follows:

\begin{minipage}{0.45\textwidth}
    \[\scalebox{1.5}{
            $S_{xy} = \sum^{n}_{i=1}(x_{i}-\bar x)(y_{i}-\bar y)$

    }\]
\end{minipage} \hfill
\begin{minipage}{0.45\textwidth}
    \[\scalebox{1.5}{
            $S_{xx} = \sum^{n}_{i=1}(x_{i}-\bar x)^2$

    }\]
\end{minipage}

\paragraph{Example} \mbox{}\\

\begin{flalign*}
    &X = [4, 6, 8, 10]& \\
    &Y = [2.3, 4.1, 5.7, 6.9]& \\
    &\bar X = \dfrac{1}{4} (4 \cdot 6 \cdot 8 \cdot 10) = 7& \\
    &\bar Y = \dfrac{1}{4} (2.3 \cdot 4.1 \cdot 5.7 \cdot 6.9) = 7.25& \\
    &S_{xx} = (4-7)^2+(6-7)^2+(8-7)^2+(10-7)^2 = 20& \\
    &S_{xy} = (4-7)(2.3-7.25)+(6-7)(4.1-7.25)+(8-7)(5.7-7.25)+(10-7)(6.9-7.25)=15.4& \\
    &\theta^{}_{1} = \dfrac{20}{15.4} = 0.77& \\
    &\theta^{}_{0} = 4.75-(0.77-7) = -0.64&
\end{flalign*}

From this follows that the regression line is:

\[\scalebox{1.5}{
        $h_{\theta}(x) = -0.64 \cdot 0.77 \cdot x$
}\]

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=\linewidth]{regression_line.png}
    \caption{Regression line of the example}
    \label{fig:regression_line}
\end{figure}

From figure \ref{fig:regression_line} we can now see that for $x_{i}=12$, $y$ would most likely be at $8.6$.

\subsection{Coefficient of Determination}

This coefficients was already mentioned in section \ref{sec:r_squared} as 'way to measure how good a regression is'. But how does it even do that?

As mentioned earlier, the goal of a good regression is to \textbf{minimize the sum of squared errors}. However, there are three kinds of sum of squared Errors:

\vspace{10px}

\begin{minipage}{0.4\textwidth}
    \begin{description}
        \item[$\bar y$:] Mean of all $Y$ 
        \item[$\bar x$:] Mean of all $X$ 
        \item[$\hat y_{i}$:] Expected value of $y$ based on the regression line 
        \item[$y_{i}$:] Actual value of $y$
        \item[$SST:$] \textit{Total sum of squares} \\
            Sum of SSE and SSR
        \item[$SSE:$ ] \textit{Sum of squared Errors} \\
            The sum of the deviations from the predicted points to the regression line
        \item[$SSR:$ ] \textit{Sum of squares explained by regression} \\
            The sum of all deviations from the mean ($\bar y)$ to the regression line
    \end{description}
\end{minipage} \hfill
\begin{minipage}{0.6\textwidth}
    \includegraphics[keepaspectratio=true,width=\textwidth]{SST.JPG}
\end{minipage}

\vspace{10px}

As illustrated in the figure above, $SST = SSR + SSE$.

\begin{align*}
    SST = \sum^{n}_{i=1}(y_{i}-\bar y)^2 && SSE=\sum^{n}_{i=1}(y_{i}-\hat y)^2 && SSR=\sum^{n}_{i=1}(\hat y_{i}-\bar y)^2
\end{align*}


But how does the aforementioned Coefficient of Determination relate to all of this? 

$R^2$ is the \textbf{fraction of the total error that can be explained by the regression}. If all errors can be explained by the regression ($R^2 = 1$), then the regression is perfect

\[\scalebox{1.5}{
        $ R^2 = \dfrac{SSR}{SST} = \dfrac{SST-SSE}{SST} = 1 - \dfrac{SSE}{SST}$
}\]

\vspace{10px}

There's yet another error, the \textit{Mean Squared Error} (MSE). MSE is basically the mean of $SSE$. It is calculated as

\[\scalebox{1.5}{

        $MSE = \frac{SSE}{n-2} = \frac{1}{n-2}\sum_{i=1}^{n}(y_{i}-\hat{y}^{}_{i})^2$
}\]

\newpage

\subsection{Correlation Analysis}

So now thath you have the regression parameters $\theta^{}_{0}$ and $\theta^{}_{1}$. which make out the regression line. However, that regression line is only as good as your parameters. So how do you test how well you calculated the parameters?

Easy, you calculate the \textbf{standard deviation of these parameters}.

\[\scalebox{1.5}{
        $s_{\theta_{0}} = \sqrt{MSE} \cdot \sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}{(x_i-\bar{x})^2}}}$
}\]

\[\scalebox{1.5}{
        $s_{\theta_{1}} = \sqrt{MSE}\cdot \sqrt{\frac{1}{\sum_{i=1}^{n}{(x_i-\bar{x})^2}}}$
}\]

\vspace{10px}

Based on these standard deviations, you can calculate a \textbf{confidence interval}. This interval is a \textbf{degree of uncertainty}. Say we conducted a study and we publish our results with a \textbf{confidence level} of 95\%.

This means that if we used the same sampling method to select different samples and computed an interval estimate for each sample, we would expect the true population parameter to fall within the interval estimates 95\% of the time.

\vspace{10px}

These intervals can be calculated with the aforementioned standard deviation of $\theta^{}_{0}$ and $\theta^{}_{1}$ and some \textit{critical values}. These critical values depend on how high your confidence level and degrees of freedom are. 

\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=\linewidth]{CriticalBordersTInterval.png}
    \caption{Critical Values for Confidence Intervals}
    \label{fig:critical_values}
\end{figure}


Degrees of freedom tell you how many regression parameters you already had to calculate. Take for example $SSE$,where both $\theta^{}_{0}$ and $\theta^{}_{1}$ are needed. Therefore, $SSE$ has a degree of freedom of $n-2$, given that you had to calculate 2 parameters.

\vspace{10px}

\noindent Confidence Intervals can be calculated with $[\theta_0 - (critical\ value \cdot \theta_0);\theta_0 + (critical\ value \cdot \theta_0)]$


\newpage

\subsection{Linear Regression Example}

\begin{wrapfigure}[14]{R}{0.4\textwidth}
    \centering
    \includegraphics[keepaspectratio=true,height=10\baselineskip]{linear_regression_example.png}
\end{wrapfigure}
\begin{flalign*}
    &X = [14, 16, 27, 42, 83, 50, 39]& \\
    &Y = [2, 5, 7, 9, 20, 13, 10]& \\
\end{flalign*}
\vspace{-40px}
\begin{flalign*}
    &\bar X = \dfrac{1}{7} \sum^{7}_{i=1}(x_{i}) =
    \dfrac{14+16+27+42+83+50+39}{7} \\
    &= \dfrac{271}{7} = \doubleunderline{38.7}& \\
    &\bar Y = \dfrac{1}{7} \sum^{7}_{i=1}(y_{i}) = 
    \dfrac{2+5+7+9+28+13+10}{7} = \dfrac{66}{7} = \doubleunderline{9.4}& \\
\end{flalign*}
\vspace{-40px}
\begin{flalign*}
    &S_{xy} = \sum^{7}_{i=1}(x_{i}-\bar x)(y_{i}-\bar y) = (14-38.7)(2-9.4) 
    +(16-38.7)(5-9.4) 
    +(27-38.7)(7-9.4) \\
    &+(42-38.7)(9-9.4) 
    +(83-38.7)(20-9.4)
    +(50-38.7)(13-9.4) \\
    &+(39-38.7)(10-9.4)=\doubleunderline{819} \\ 
\end{flalign*}
\vspace{-40px}
\begin{flalign*}
    &S_{xx} = \sum^{7}_{i=1}(x_{i}-\bar x)^2 = (14-38.7)^2
    +(16-38.7)^2
    +(27-38.7)^2
    +(42-38.7)^2
    +(83-38.7)^2 \\
    &+(50-38.7)^2
    +(39-38.7)^2 = \doubleunderline{3363.4}
\end{flalign*}
\vspace{-20px}
\begin{flalign*}
    &\theta_1 = \dfrac{S_{xy}}{S_{xx}} = \dfrac{819}{3363.4} = \doubleunderline{0.24}&
\end{flalign*} 
\vspace{-20px}
\begin{flalign*}
    &\theta_0 = \bar y - \theta_1 \cdot \bar x = 9.4 - 0.24 \cdot 38.7 = \doubleunderline{-0.008}&
\end{flalign*}
\vspace{-20px}
\begin{flalign*}
    &SSE = \sum_{i=1}^{7}(y_{i}-\hat{y}_{i})^2 = \sum_{i=1}^{7}(y_i - (\theta_0 + \theta_1 \cdot x))^2 = \sum_{i=1}^{7}(y_i - (-0.008 + 0.24 \cdot x))^2 \\ 
    &= \sum_{i=1}^{7}(y_i - (-0.232 \cdot x))^2 = \doubleunderline{5.87}&
\end{flalign*}
\vspace{-20px}
\begin{flalign*}
    &SSR = \sum_{i}^{7}(\hat{y}_{i}-\bar{y})^2 = \sum_{i}^{7}((\theta_0 + \theta_1 \cdot x)-38.7)^2 = \sum_{i}^{7}((-0.008 + 0.24 \cdot x)-38.7)^2 \\ 
    &= \sum_{i}^{7}((-0.232 \cdot x)-38.7)^2=\doubleunderline{199.84}&
\end{flalign*}
\vspace{-20px}
\begin{flalign*}
    &SST = SSE+SSR = 5.87 + 199.84 = \doubleunderline{205.71}&
\end{flalign*}
\vspace{-20px}
\begin{flalign*}
    &R^2 = \frac{SSR}{SST} = \dfrac{199.84}{205.71} = \doubleunderline{0.97}&
\end{flalign*}
\vspace{-10px}
\begin{flalign*}
    &MSE = \frac{SSE}{n-2} = \frac{5.87}{7-2}=\doubleunderline{1.17}&
\end{flalign*}
\vspace{-20px}
\begin{flalign*}
    &s_{\theta_{0}} = \sqrt{MSE} \cdot \sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}{(x_i-\bar{x})^2}}} = \sqrt{1.17} \cdot \sqrt{\frac{1}{7} + \frac{38.7^2}{\sum_{i=1}^{n}{(x_i-38.7)^2}}} = \doubleunderline{-0.008}& 
\end{flalign*}
\vspace{-10px}
\begin{flalign*}
    &s_{\theta_{1}} = \sqrt{MSE}\cdot \sqrt{\frac{1}{\sum_{i=1}^{n}{(x_i-\bar{x})^2}}} = \sqrt{1.17} \cdot \sqrt{\frac{1}{\sum_{i=1}^{n}{(x_i-38.7)^2}}} = \doubleunderline{0.019}& &
\end{flalign*}
\vspace{-10px}
\begin{flalign*}
    &Interval\ \theta_0 = [\theta_0 - 2.57 \cdot \theta_0; \theta_0 + 2.57 \cdot \theta_0] = \doubleunderline{[-2.14;2.12]} &
\end{flalign*}
\vspace{-20px}
\begin{flalign*}
    &Interval\ \theta_1 [\theta_1 - 2.57 \cdot \theta_1; \theta_1 + 2.57 \cdot \theta_1] = \doubleunderline{[0.20;0.29]}&
\end{flalign*}

\newpage
\restoregeometry


The results of the last page can also be achieved with the following python-code:

\begin{lstlisting}
import matplotlib.pyplot as plt
import scipy.stats as st
import seaborn as sns
import pandas as pd
import numpy as np

%precision 10
%matplotlib inline

x=np.array([14, 16, 27, 42, 83, 50, 39])
y=np.array([2, 5, 7, 9, 20, 13, 10])

mean_x = np.mean(x)
mean_y = np.mean(y)

Sxx = np.sum((x-mean_x)**2)
Syy = np.sum((y-mean_y)**2)
Sxy = np.sum((x-mean_x)*(y-mean_y))

thet1 = Sxy/Sxx
thet0 = mean_y-thet1*mean_x

# To show the regression line
plt.plot(x,y, 'bo')
plt.plot(x,thet0+thet1*x, 'r')
plt.show()

hat_y = thet0+thet1*x

SSE=np.sum(((y-hat_y))**2)

SSR=np.sum((hat_y-mean_y)**2)

SST = SSE + SSR

R_sq = SSR/SST

MSE = SSE/(len(x)-2)

Sthet0 = np.sqrt(MSE)*np.sqrt((1/len(x))+(mean_x**2)/(np.sum((x-mean_x)**2)))

Sthet1 = (np.sqrt(MSE))*np.sqrt(1/(np.sum((x-mean_x)**2)))

print("theta_1: " + str(thet1))

print("Interval theta0: [" + str(thet0 - (2.57*Sthet0)) + " ; " + str(thet0 + (2.57*Sthet0)) + "]")

print("Interval theta1: [" + str(thet1 - (2.57*Sthet1)) + " ; " + str(thet1 + (2.57*Sthet1)) + "]")

\end{lstlisting}



\subsection{Multple Linear Regression}

Linear Regeression is great and all, but what if you wanted to predict an independent value based on \textbf{multiple} other values? 

In that case we would need \textbf{multilinear regression}. 

\subsubsection{Example multilinear regression}

Suppose we want to predict the weight of a weightlifeter based on the training hours per week and the delivery of protein.

\vspace{10px}

\begin{minipage}{0.25\textwidth}
    \begin{tabular}{cccc}
        \toprule
        \thead{i} & \thead{y} & \thead{$x_1$} & \thead{$x_2$} \\
        \hline
        1 & 93 & 2 & 1.1 \\
        \hline
        2 & 106 & 2 & 1.9 \\
        \hline
        3 & 146 & 4 & 2 \\
        \hline
        4 & 140 & 5 & 1.5  \\
        \hline
        5 & 151 & 6 & 1.3 \\
        \hline
        6 & 158 & 7 & 2.1 \\
        \hline
        7 & 130 & 4 & 1.8 \\
        \hline
        8 & 159 & 5 & 2.5 \\
        \bottomrule
    \end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.75\textwidth}
    \begin{description}
        \item[$i$: ] No. of observation
        \item[$y$: ] Weight in kg
        \item[$x_1$: ] Training h/week
        \item[$x_2$: ] Protein intake g/kg/day
    \end{description}

    We assume that the function to calculate the weight from training-hours and protein intake is as follows:
\end{minipage}

\vspace{10px}

\[ \scalebox{1.5}{
    $y = h_\theta (x_1, x_2) = \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2$
}\]
    
\begin{figure}[htb!]
    \centering
    \includegraphics[keepaspectratio=true, width=\linewidth]{multilinear_regression.png}
    \caption{Model of the multilinear regression (\textcopyright J. Bürgler, 2019)}
    \label{multilinear_regression}
\end{figure}

The following solution  minimizes the sum of squared errors in this model (and therefore get a hella good regression).

\[ \scalebox{1.5}{
        $\theta = (X^T X)^{-1} X^T y = 
        \begin{bmatrix}
            \theta_0 \\
            \theta_1 \\
            \theta_2
        \end{bmatrix}  
        \rightarrow
        \begin{bmatrix}
            55.7 \\
            11.1 \\
            17.5 
        \end{bmatrix}  $
}\]

According to this, the regression plane would be:


\[ \scalebox{1.5}{
        $y = h_\theta (x_1, x_2) = 55.7 + 11.1 \cdot x_1 + 17.5 \cdot x_2$
}\]

\newpage

Of course, this can also be done rather easily in Python:

\begin{lstlisting}
import numpy as np

X = np.matrix([[2,1.1],[2,1.9],[4,2.0],[5,1.5],[6,1.3],[7,2.1],[4,1.8],[5,2.5]])

# Anzahl Reihen von X anhand der ersten Zeile (= Anzahl Beobachtungen)
n = X.shape[0]

y = np.matrix([93,106,146,140,151,158,130,159])

# Append a column of '1' infront of the X-matrix (to get the 'X'-matrix from the figure) 
X_ext = np.c_[np.ones((n,1)),X]

theta = np.linalg.inv(X_ext.T.dot(X_est)).dot(X_ext.T).dot(y)

print(theta)
\end{lstlisting}


\end{document}
